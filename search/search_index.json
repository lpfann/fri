{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Feature Relevance Intervals - FRI FRI is a Python 3 package for analytical feature selection purposes. It allows superior feature selection in the sense that all important features are conserved. At the moment we support multiple linear models for solving Classification, Regression and Ordinal Regression Problems. We also support LUPI paradigm where at learning time, privileged information is available. Documentation Check out our online documentation here . There you can find a quick start guide and more background information. You can also run the guide directly without setup online here . Installation FRI requires Python 3.6+ . For a stable version from PyPI use $ pip install fri Usage Please refer to the documentation for advice. For a quick start we provide a simple guide which leads through the main functions. References [1] G\u00f6pfert C, Pfannschmidt L, Hammer B. Feature Relevance Bounds for Linear Classification. In: Proceedings of the ESANN. 25th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/publication/2908201 [2] G\u00f6pfert C, Pfannschmidt L, G\u00f6pfert JP, Hammer B. Interpretation of Linear Classifiers by Means of Feature Relevance Bounds. Neurocomputing. https://pub.uni-bielefeld.de/publication/2915273 [3] Lukas Pfannschmidt, Jonathan Jakob, Michael Biehl, Peter Tino, Barbara Hammer: Feature Relevance Bounds for Ordinal Regression. Proceedings of the ESANN. 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; Accepted. https://pub.uni-bielefeld.de/record/2933893 [4] Pfannschmidt L, G\u00f6pfert C, Neumann U, Heider D, Hammer B: FRI - Feature Relevance Intervals for Interpretable and Interactive Data Exploration. Presented at the 16th IEEE International Conference on Computational Intelligence in Bioinformatics and Computational Biology, Certosa di Pontignano, Siena - Tuscany, Italy. https://ieeexplore.ieee.org/document/8791489","title":"Home"},{"location":"#feature-relevance-intervals-fri","text":"FRI is a Python 3 package for analytical feature selection purposes. It allows superior feature selection in the sense that all important features are conserved. At the moment we support multiple linear models for solving Classification, Regression and Ordinal Regression Problems. We also support LUPI paradigm where at learning time, privileged information is available.","title":"Feature Relevance Intervals - FRI"},{"location":"#documentation","text":"Check out our online documentation here . There you can find a quick start guide and more background information. You can also run the guide directly without setup online here .","title":"Documentation"},{"location":"#installation","text":"FRI requires Python 3.6+ . For a stable version from PyPI use $ pip install fri","title":"Installation"},{"location":"#usage","text":"Please refer to the documentation for advice. For a quick start we provide a simple guide which leads through the main functions.","title":"Usage"},{"location":"#references","text":"[1] G\u00f6pfert C, Pfannschmidt L, Hammer B. Feature Relevance Bounds for Linear Classification. In: Proceedings of the ESANN. 25th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/publication/2908201 [2] G\u00f6pfert C, Pfannschmidt L, G\u00f6pfert JP, Hammer B. Interpretation of Linear Classifiers by Means of Feature Relevance Bounds. Neurocomputing. https://pub.uni-bielefeld.de/publication/2915273 [3] Lukas Pfannschmidt, Jonathan Jakob, Michael Biehl, Peter Tino, Barbara Hammer: Feature Relevance Bounds for Ordinal Regression. Proceedings of the ESANN. 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; Accepted. https://pub.uni-bielefeld.de/record/2933893 [4] Pfannschmidt L, G\u00f6pfert C, Neumann U, Heider D, Hammer B: FRI - Feature Relevance Intervals for Interpretable and Interactive Data Exploration. Presented at the 16th IEEE International Conference on Computational Intelligence in Bioinformatics and Computational Biology, Certosa di Pontignano, Siena - Tuscany, Italy. https://ieeexplore.ieee.org/document/8791489","title":"References"},{"location":"docs/Guide/","text":"Quick start guide Installation Stable Fri can be installed via the Python Package Index (PyPI). If you have pip installed just execute the command pip install fri to get the newest stable version. The dependencies should be installed and checked automatically. If you have problems installing please open issue at our tracker . Development To install a bleeding edge dev version of FRI you can clone the GitHub repository using git clone git@github.com:lpfann/fri.git and then check out the dev branch: git checkout dev . We use poetry for dependency management. Run poetry install in the cloned repository to install fri in a virtualenv. To check if everything works as intented you can use pytest to run the unit tests. Just run the command poetry run pytest in the main project folder Using FRI Now we showcase the workflow of using FRI on a simple classification problem. Data To have something to work with, we need some data first. fri includes a generation method for binary classification and regression data. In our case we need some classification data. from fri import genClassificationData We want to create a small set with a few features. Because we want to showcase the all-relevant feature selection, we generate multiple strongly and weakly relevant features. n = 100 features = 6 strongly_relevant = 2 weakly_relevant = 2 X , y = genClassificationData ( n_samples = n , n_features = features , n_strel = strongly_relevant , n_redundant = weakly_relevant , random_state = 123 ) The method also prints out the parameters again. X . shape (100, 6) We created a binary classification set with 6 features of which 2 are strongly relevant and 2 weakly relevant. Preprocess Because our method expects mean centered data we need to standardize it first. This centers the values around 0 and deviation to the standard deviation from sklearn.preprocessing import StandardScaler X_scaled = StandardScaler () . fit_transform ( X ) Model Now we need to creata a Model. We use the FRI module. import fri fri provides a convenience class fri.FRI to create a model. fri.FRI needs the type of problem as a first argument of type ProblemName . Depending on the Problem you want to analyze pick from one of the available models in ProblemName . list ( fri . ProblemName ) [<ProblemName.CLASSIFICATION: <class 'fri.model.classification.Classification'>>, <ProblemName.REGRESSION: <class 'fri.model.regression.Regression'>>, <ProblemName.ORDINALREGRESSION: <class 'fri.model.ordinal_regression.OrdinalRegression'>>, <ProblemName.LUPI_CLASSIFICATION: <class 'fri.model.lupi_classification.LUPI_Classification'>>, <ProblemName.LUPI_REGRESSION: <class 'fri.model.lupi_regression.LUPI_Regression'>>, <ProblemName.LUPI_ORDREGRESSION: <class 'fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression'>>] Because we have Classification data we use the ProblemName.CLASSIFICATION to instantiate our model. fri_model = fri . FRI ( fri . ProblemName . CLASSIFICATION , slack_loss = 0.2 , slack_regularization = 0.2 ) fri_model FRI(n_jobs=1, n_param_search=10, n_probe_features=20, normalize=True, problemName=None, random_state=RandomState(MT19937) at 0x7F30AE749C00, slack_loss=None, slack_regularization=None, verbose=0) We used no parameters for creation so the defaults are active. Fitting to data Now we can just fit the model to the data using scikit-learn like commands. fri_model . fit ( X_scaled , y ) FRI(n_jobs=1, n_param_search=10, n_probe_features=20, normalize=True, problemName=None, random_state=RandomState(MT19937) at 0x7F30AE749C00, slack_loss=None, slack_regularization=None, verbose=0) The resulting feature relevance bounds are saved in the interval_ variable. fri_model . interval_ array([[0.28158379, 0.42206863], [0.26824834, 0.41605723], [0. , 0.48783056], [0. , 0.44949121], [0. , 0.04668038], [0. , 0.0604022 ]]) If you want to print out the relevance class use the print_interval_with_class() function. print ( fri_model . print_interval_with_class ()) ############## Relevance bounds ############## feature: [LB -- UB], relevance class 0: [0.3 -- 0.4], Strong relevant 1: [0.3 -- 0.4], Strong relevant 2: [0.0 -- 0.5], Weak relevant 3: [0.0 -- 0.4], Weak relevant 4: [0.0 -- 0.0], Irrelevant 5: [0.0 -- 0.1], Irrelevant The bounds are grouped in 2d sublists for each feature. To acess the relevance bounds for feature 2 we would use fri_model . interval_ [ 2 ] array([0. , 0.48783056]) The relevance classes are saved in the corresponding variable relevance_classes_ : fri_model . relevance_classes_ array([2, 2, 1, 1, 0, 0]) 2 denotes strongly relevant features, 1 weakly relevant and 0 irrelevant. Plot results The bounds in numerical form are useful for postprocesing. If we want a human to look at it, we recommend the plot function plot_relevance_bars . We can also color the bars according to relevance_classes_ # Import plot function from fri.plot import plot_relevance_bars import matplotlib.pyplot as plt % matplotlib inline # Create new figure, where we can put an axis on fig , ax = plt . subplots ( 1 , 1 , figsize = ( 6 , 3 )) # plot the bars on the axis, colored according to fri out = plot_relevance_bars ( ax , fri_model . interval_ , classes = fri_model . relevance_classes_ ) Setting constraints manually Our model also allows to compute relevance bounds when the user sets a given range for the features. We use a dictionary to encode our constraints. preset = {} Example As an example, let us constrain the third from our example to the minimum relevance bound. preset [ 2 ] = fri_model . interval_ [ 2 , 0 ] We use the function constrained_intervals . Note: we need to fit the model before we can use this function. We already did that, so we are fine. const_ints = fri_model . constrained_intervals ( preset = preset ) const_ints array([[3.33537278e-01, 3.98346587e-01], [3.16282293e-01, 3.99927655e-01], [0.00000000e+00, 0.00000000e+00], [3.77824890e-01, 4.49491208e-01], [0.00000000e+00, 2.56795434e-02], [6.28844147e-11, 4.97955367e-02]]) Feature 3 is set to its minimum (at 0). How does it look visually? fig , ax = plt . subplots ( 1 , 1 , figsize = ( 6 , 3 )) out = plot_relevance_bars ( ax , const_ints ) Feature 3 is reduced to its minimum (no contribution). In turn, its correlated partner feature 4 had to take its maximum contribution. Print internal Parameters If we want to take at internal parameters, we can use the verbose flag in the model creation. fri_model = fri . FRI ( fri . ProblemName . CLASSIFICATION , verbose = True ) fri_model . fit ( X_scaled , y ) Fitting 3 folds for each of 10 candidates , totalling 30 fits [ Parallel ( n_jobs = 1 )]: Using backend SequentialBackend with 1 concurrent workers . [ Parallel ( n_jobs = 1 )]: Done 30 out of 30 | elapsed : 0 . 3 s finished [ Parallel ( n_jobs = 1 )]: Using backend SequentialBackend with 1 concurrent workers . ******************** Best found baseline model ******************** Classification_SVM ( C = 6523 . 18940508926 ) score : 1 . 0 'loss: -3.2162506599275367e-12' 'w_l1: 28.743902865913274' 'w: shape (6,)' 'b: shape ()' 'slack: shape (100,)' ****************************** [ Parallel ( n_jobs = 1 )]: Done 18 out of 18 | elapsed : 0 . 2 s finished [ Parallel ( n_jobs = 1 )]: Using backend SequentialBackend with 1 concurrent workers . [ Parallel ( n_jobs = 1 )]: Done 40 out of 40 | elapsed : 0 . 4 s finished [ Parallel ( n_jobs = 1 )]: Using backend SequentialBackend with 1 concurrent workers . FS threshold : - 0 . 3485850797626653 - 0 . 37121449904943193 , Mean : 0 . 011314709643383315 , Std : 0 . 014497743389929868 , n_probes 4 FS threshold : - 0 . 09430000942634031 - 0 . 1585322164066591 , Mean : 0 . 0321161034901594 , Std : 0 . 014735763155338663 , n_probes 7 [ Parallel ( n_jobs = 1 )]: Done 20 out of 20 | elapsed : 0 . 3 s finished FRI ( n_jobs = 1 , n_param_search = 10 , n_probe_features = 20 , normalize = True , problemName = None , random_state = RandomState ( MT19937 ) at 0 x7F30AE749C00 , slack_loss = None , slack_regularization = None , verbose = True ) This prints out the parameters of the baseline model One can also see the best selected hyperparameter according to gridsearch and the training score of the model in score . Multiprocessing To enable multiprocessing simply use the n_jobs parameter when init. the model. It expects an integer parameter which defines the amount of processes used. n_jobs=-1 uses all available on the CPU. fri_model = fri . FRI ( fri . ProblemName . CLASSIFICATION , n_jobs =- 1 , verbose = 1 ) fri_model . fit ( X_scaled , y ) Fitting 3 folds for each of 10 candidates , totalling 30 fits [ Parallel ( n_jobs =- 1 )]: Using backend LokyBackend with 8 concurrent workers . [ Parallel ( n_jobs =- 1 )]: Done 15 out of 30 | elapsed : 0 . 1 s remaining : 0 . 1 s [ Parallel ( n_jobs =- 1 )]: Done 30 out of 30 | elapsed : 0 . 2 s finished [ Parallel ( n_jobs =- 1 )]: Using backend LokyBackend with 8 concurrent workers . [ Parallel ( n_jobs =- 1 )]: Done 3 out of 18 | elapsed : 0 . 1 s remaining : 0 . 4 s [ Parallel ( n_jobs =- 1 )]: Done 18 out of 18 | elapsed : 0 . 1 s finished [ Parallel ( n_jobs =- 1 )]: Using backend LokyBackend with 8 concurrent workers . ******************** Best found baseline model ******************** Classification_SVM ( C = 0 . 3735574324157715 ) score : 0 . 9499248120300752 'loss: 11.49412313137918' 'w_l1: 4.827039706578124' 'w: shape (6,)' 'b: shape ()' 'slack: shape (100,)' ****************************** [ Parallel ( n_jobs =- 1 )]: Done 40 out of 40 | elapsed : 0 . 6 s finished [ Parallel ( n_jobs =- 1 )]: Using backend LokyBackend with 8 concurrent workers . FS threshold : - 0 . 003062137274245256 - 0 . 0037609499356378584 , Mean : 0 . 00034940633069630115 , Std : 0 . 00057365780239467 , n_probes 11 FS threshold : - 0 . 2619964838392958 - 0 . 31312919443178205 , Mean : 0 . 025566355296243105 , Std : 0 . 03838315602969965 , n_probes 8 [ Parallel ( n_jobs =- 1 )]: Done 20 out of 20 | elapsed : 0 . 5 s finished FRI ( n_jobs =- 1 , n_param_search = 10 , n_probe_features = 20 , normalize = True , problemName = None , random_state = RandomState ( MT19937 ) at 0 x7F30AE749C00 , slack_loss = None , slack_regularization = None , verbose = 1 )","title":"Guide"},{"location":"docs/Guide/#quick-start-guide","text":"","title":"Quick start guide"},{"location":"docs/Guide/#installation","text":"","title":"Installation"},{"location":"docs/Guide/#stable","text":"Fri can be installed via the Python Package Index (PyPI). If you have pip installed just execute the command pip install fri to get the newest stable version. The dependencies should be installed and checked automatically. If you have problems installing please open issue at our tracker .","title":"Stable"},{"location":"docs/Guide/#development","text":"To install a bleeding edge dev version of FRI you can clone the GitHub repository using git clone git@github.com:lpfann/fri.git and then check out the dev branch: git checkout dev . We use poetry for dependency management. Run poetry install in the cloned repository to install fri in a virtualenv. To check if everything works as intented you can use pytest to run the unit tests. Just run the command poetry run pytest in the main project folder","title":"Development"},{"location":"docs/Guide/#using-fri","text":"Now we showcase the workflow of using FRI on a simple classification problem.","title":"Using FRI"},{"location":"docs/Guide/#data","text":"To have something to work with, we need some data first. fri includes a generation method for binary classification and regression data. In our case we need some classification data. from fri import genClassificationData We want to create a small set with a few features. Because we want to showcase the all-relevant feature selection, we generate multiple strongly and weakly relevant features. n = 100 features = 6 strongly_relevant = 2 weakly_relevant = 2 X , y = genClassificationData ( n_samples = n , n_features = features , n_strel = strongly_relevant , n_redundant = weakly_relevant , random_state = 123 ) The method also prints out the parameters again. X . shape (100, 6) We created a binary classification set with 6 features of which 2 are strongly relevant and 2 weakly relevant.","title":"Data"},{"location":"docs/Guide/#preprocess","text":"Because our method expects mean centered data we need to standardize it first. This centers the values around 0 and deviation to the standard deviation from sklearn.preprocessing import StandardScaler X_scaled = StandardScaler () . fit_transform ( X )","title":"Preprocess"},{"location":"docs/Guide/#model","text":"Now we need to creata a Model. We use the FRI module. import fri fri provides a convenience class fri.FRI to create a model. fri.FRI needs the type of problem as a first argument of type ProblemName . Depending on the Problem you want to analyze pick from one of the available models in ProblemName . list ( fri . ProblemName ) [<ProblemName.CLASSIFICATION: <class 'fri.model.classification.Classification'>>, <ProblemName.REGRESSION: <class 'fri.model.regression.Regression'>>, <ProblemName.ORDINALREGRESSION: <class 'fri.model.ordinal_regression.OrdinalRegression'>>, <ProblemName.LUPI_CLASSIFICATION: <class 'fri.model.lupi_classification.LUPI_Classification'>>, <ProblemName.LUPI_REGRESSION: <class 'fri.model.lupi_regression.LUPI_Regression'>>, <ProblemName.LUPI_ORDREGRESSION: <class 'fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression'>>] Because we have Classification data we use the ProblemName.CLASSIFICATION to instantiate our model. fri_model = fri . FRI ( fri . ProblemName . CLASSIFICATION , slack_loss = 0.2 , slack_regularization = 0.2 ) fri_model FRI(n_jobs=1, n_param_search=10, n_probe_features=20, normalize=True, problemName=None, random_state=RandomState(MT19937) at 0x7F30AE749C00, slack_loss=None, slack_regularization=None, verbose=0) We used no parameters for creation so the defaults are active.","title":"Model"},{"location":"docs/Guide/#fitting-to-data","text":"Now we can just fit the model to the data using scikit-learn like commands. fri_model . fit ( X_scaled , y ) FRI(n_jobs=1, n_param_search=10, n_probe_features=20, normalize=True, problemName=None, random_state=RandomState(MT19937) at 0x7F30AE749C00, slack_loss=None, slack_regularization=None, verbose=0) The resulting feature relevance bounds are saved in the interval_ variable. fri_model . interval_ array([[0.28158379, 0.42206863], [0.26824834, 0.41605723], [0. , 0.48783056], [0. , 0.44949121], [0. , 0.04668038], [0. , 0.0604022 ]]) If you want to print out the relevance class use the print_interval_with_class() function. print ( fri_model . print_interval_with_class ()) ############## Relevance bounds ############## feature: [LB -- UB], relevance class 0: [0.3 -- 0.4], Strong relevant 1: [0.3 -- 0.4], Strong relevant 2: [0.0 -- 0.5], Weak relevant 3: [0.0 -- 0.4], Weak relevant 4: [0.0 -- 0.0], Irrelevant 5: [0.0 -- 0.1], Irrelevant The bounds are grouped in 2d sublists for each feature. To acess the relevance bounds for feature 2 we would use fri_model . interval_ [ 2 ] array([0. , 0.48783056]) The relevance classes are saved in the corresponding variable relevance_classes_ : fri_model . relevance_classes_ array([2, 2, 1, 1, 0, 0]) 2 denotes strongly relevant features, 1 weakly relevant and 0 irrelevant.","title":"Fitting to data"},{"location":"docs/Guide/#plot-results","text":"The bounds in numerical form are useful for postprocesing. If we want a human to look at it, we recommend the plot function plot_relevance_bars . We can also color the bars according to relevance_classes_ # Import plot function from fri.plot import plot_relevance_bars import matplotlib.pyplot as plt % matplotlib inline # Create new figure, where we can put an axis on fig , ax = plt . subplots ( 1 , 1 , figsize = ( 6 , 3 )) # plot the bars on the axis, colored according to fri out = plot_relevance_bars ( ax , fri_model . interval_ , classes = fri_model . relevance_classes_ )","title":"Plot results"},{"location":"docs/Guide/#setting-constraints-manually","text":"Our model also allows to compute relevance bounds when the user sets a given range for the features. We use a dictionary to encode our constraints. preset = {}","title":"Setting constraints manually"},{"location":"docs/Guide/#example","text":"As an example, let us constrain the third from our example to the minimum relevance bound. preset [ 2 ] = fri_model . interval_ [ 2 , 0 ] We use the function constrained_intervals . Note: we need to fit the model before we can use this function. We already did that, so we are fine. const_ints = fri_model . constrained_intervals ( preset = preset ) const_ints array([[3.33537278e-01, 3.98346587e-01], [3.16282293e-01, 3.99927655e-01], [0.00000000e+00, 0.00000000e+00], [3.77824890e-01, 4.49491208e-01], [0.00000000e+00, 2.56795434e-02], [6.28844147e-11, 4.97955367e-02]]) Feature 3 is set to its minimum (at 0). How does it look visually? fig , ax = plt . subplots ( 1 , 1 , figsize = ( 6 , 3 )) out = plot_relevance_bars ( ax , const_ints ) Feature 3 is reduced to its minimum (no contribution). In turn, its correlated partner feature 4 had to take its maximum contribution.","title":"Example"},{"location":"docs/Guide/#print-internal-parameters","text":"If we want to take at internal parameters, we can use the verbose flag in the model creation. fri_model = fri . FRI ( fri . ProblemName . CLASSIFICATION , verbose = True ) fri_model . fit ( X_scaled , y ) Fitting 3 folds for each of 10 candidates , totalling 30 fits [ Parallel ( n_jobs = 1 )]: Using backend SequentialBackend with 1 concurrent workers . [ Parallel ( n_jobs = 1 )]: Done 30 out of 30 | elapsed : 0 . 3 s finished [ Parallel ( n_jobs = 1 )]: Using backend SequentialBackend with 1 concurrent workers . ******************** Best found baseline model ******************** Classification_SVM ( C = 6523 . 18940508926 ) score : 1 . 0 'loss: -3.2162506599275367e-12' 'w_l1: 28.743902865913274' 'w: shape (6,)' 'b: shape ()' 'slack: shape (100,)' ****************************** [ Parallel ( n_jobs = 1 )]: Done 18 out of 18 | elapsed : 0 . 2 s finished [ Parallel ( n_jobs = 1 )]: Using backend SequentialBackend with 1 concurrent workers . [ Parallel ( n_jobs = 1 )]: Done 40 out of 40 | elapsed : 0 . 4 s finished [ Parallel ( n_jobs = 1 )]: Using backend SequentialBackend with 1 concurrent workers . FS threshold : - 0 . 3485850797626653 - 0 . 37121449904943193 , Mean : 0 . 011314709643383315 , Std : 0 . 014497743389929868 , n_probes 4 FS threshold : - 0 . 09430000942634031 - 0 . 1585322164066591 , Mean : 0 . 0321161034901594 , Std : 0 . 014735763155338663 , n_probes 7 [ Parallel ( n_jobs = 1 )]: Done 20 out of 20 | elapsed : 0 . 3 s finished FRI ( n_jobs = 1 , n_param_search = 10 , n_probe_features = 20 , normalize = True , problemName = None , random_state = RandomState ( MT19937 ) at 0 x7F30AE749C00 , slack_loss = None , slack_regularization = None , verbose = True ) This prints out the parameters of the baseline model One can also see the best selected hyperparameter according to gridsearch and the training score of the model in score .","title":"Print internal Parameters"},{"location":"docs/Guide/#multiprocessing","text":"To enable multiprocessing simply use the n_jobs parameter when init. the model. It expects an integer parameter which defines the amount of processes used. n_jobs=-1 uses all available on the CPU. fri_model = fri . FRI ( fri . ProblemName . CLASSIFICATION , n_jobs =- 1 , verbose = 1 ) fri_model . fit ( X_scaled , y ) Fitting 3 folds for each of 10 candidates , totalling 30 fits [ Parallel ( n_jobs =- 1 )]: Using backend LokyBackend with 8 concurrent workers . [ Parallel ( n_jobs =- 1 )]: Done 15 out of 30 | elapsed : 0 . 1 s remaining : 0 . 1 s [ Parallel ( n_jobs =- 1 )]: Done 30 out of 30 | elapsed : 0 . 2 s finished [ Parallel ( n_jobs =- 1 )]: Using backend LokyBackend with 8 concurrent workers . [ Parallel ( n_jobs =- 1 )]: Done 3 out of 18 | elapsed : 0 . 1 s remaining : 0 . 4 s [ Parallel ( n_jobs =- 1 )]: Done 18 out of 18 | elapsed : 0 . 1 s finished [ Parallel ( n_jobs =- 1 )]: Using backend LokyBackend with 8 concurrent workers . ******************** Best found baseline model ******************** Classification_SVM ( C = 0 . 3735574324157715 ) score : 0 . 9499248120300752 'loss: 11.49412313137918' 'w_l1: 4.827039706578124' 'w: shape (6,)' 'b: shape ()' 'slack: shape (100,)' ****************************** [ Parallel ( n_jobs =- 1 )]: Done 40 out of 40 | elapsed : 0 . 6 s finished [ Parallel ( n_jobs =- 1 )]: Using backend LokyBackend with 8 concurrent workers . FS threshold : - 0 . 003062137274245256 - 0 . 0037609499356378584 , Mean : 0 . 00034940633069630115 , Std : 0 . 00057365780239467 , n_probes 11 FS threshold : - 0 . 2619964838392958 - 0 . 31312919443178205 , Mean : 0 . 025566355296243105 , Std : 0 . 03838315602969965 , n_probes 8 [ Parallel ( n_jobs =- 1 )]: Done 20 out of 20 | elapsed : 0 . 5 s finished FRI ( n_jobs =- 1 , n_param_search = 10 , n_probe_features = 20 , normalize = True , problemName = None , random_state = RandomState ( MT19937 ) at 0 x7F30AE749C00 , slack_loss = None , slack_regularization = None , verbose = 1 )","title":"Multiprocessing"},{"location":"docs/background/","text":"Background Note We presented [FRI] at the CIBCB conference. Check out the slides for a short primer into how it works. Feature selection is the task of finding relevant features used in a machine learning model. Often used for this task are models which produce a sparse subset of all input features by permitting the use of additional features (e.g. Lasso with L1 regularization). But these models are often tuned to filter out redundancies in the input set and produce only an unstable solution especially in the presence of higher dimensional data. FRI calculates relevance bound values for all input features. These bounds give rise to intervals which we named feature relevance intervals (FRI). A given interval symbolizes the allowed contribution each feature has, when it is allowed to be maximized and minimized independently from the others. This allows us to approximate the global solution instead of relying on the local solutions of the alternatives. With these we can classify features into three classes: Strongly relevant : features which are crucial for model performance Weakly relevant : features which are important but can be substituted by another weakly relevant feature Irrelevant : features which have no association with the target variable References G\u00f6pfert C, Pfannschmidt L, Hammer B: Feature Relevance Bounds for Linear Classification. In: Proceedings of the ESANN. 25th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/publication/2908201 G\u00f6pfert C, Pfannschmidt L, G\u00f6pfert JP, Hammer B: Interpretation of Linear Classifiers by Means of Feature Relevance Bounds. Neurocomputing. https://pub.uni-bielefeld.de/publication/2915273 Pfannschmidt L, Jakob J, Biehl M, Tino P, Hammer B: Feature Relevance Bounds for Ordinal Regression . Proceedings of the ESANN. 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/record/2933893 Pfannschmidt L, G\u00f6pfert C, Neumann U, Heider D, Hammer B: FRI - Feature Relevance Intervals for Interpretable and Interactive Data Exploration. Presented at the 16th IEEE International Conference on Computational Intelligence in Bioinformatics and Computational Biology, Certosa di Pontignano, Siena - Tuscany, Italy. https://ieeexplore.ieee.org/document/8791489","title":"Background"},{"location":"docs/background/#background","text":"Note We presented [FRI] at the CIBCB conference. Check out the slides for a short primer into how it works. Feature selection is the task of finding relevant features used in a machine learning model. Often used for this task are models which produce a sparse subset of all input features by permitting the use of additional features (e.g. Lasso with L1 regularization). But these models are often tuned to filter out redundancies in the input set and produce only an unstable solution especially in the presence of higher dimensional data. FRI calculates relevance bound values for all input features. These bounds give rise to intervals which we named feature relevance intervals (FRI). A given interval symbolizes the allowed contribution each feature has, when it is allowed to be maximized and minimized independently from the others. This allows us to approximate the global solution instead of relying on the local solutions of the alternatives. With these we can classify features into three classes: Strongly relevant : features which are crucial for model performance Weakly relevant : features which are important but can be substituted by another weakly relevant feature Irrelevant : features which have no association with the target variable","title":"Background"},{"location":"docs/background/#references","text":"G\u00f6pfert C, Pfannschmidt L, Hammer B: Feature Relevance Bounds for Linear Classification. In: Proceedings of the ESANN. 25th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/publication/2908201 G\u00f6pfert C, Pfannschmidt L, G\u00f6pfert JP, Hammer B: Interpretation of Linear Classifiers by Means of Feature Relevance Bounds. Neurocomputing. https://pub.uni-bielefeld.de/publication/2915273 Pfannschmidt L, Jakob J, Biehl M, Tino P, Hammer B: Feature Relevance Bounds for Ordinal Regression . Proceedings of the ESANN. 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/record/2933893 Pfannschmidt L, G\u00f6pfert C, Neumann U, Heider D, Hammer B: FRI - Feature Relevance Intervals for Interpretable and Interactive Data Exploration. Presented at the 16th IEEE International Conference on Computational Intelligence in Bioinformatics and Computational Biology, Certosa di Pontignano, Siena - Tuscany, Italy. https://ieeexplore.ieee.org/document/8791489","title":"References"},{"location":"docs/citing_FRI/","text":"Citing FRI If you use FRI in the academic context we would like if you cite one of the following references. [1] G\u00f6pfert C, Pfannschmidt L, Hammer B. Feature Relevance Bounds for Linear Classification. In: Proceedings of the ESANN. 25th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/publication/2908201 [2] G\u00f6pfert C, Pfannschmidt L, G\u00f6pfert JP, Hammer B. Interpretation of Linear Classifiers by Means of Feature Relevance Bounds. Neurocomputing. https://pub.uni-bielefeld.de/publication/2915273 [3] Lukas Pfannschmidt, Jonathan Jakob, Michael Biehl, Peter Tino, Barbara Hammer: Feature Relevance Bounds for Ordinal Regression. Proceedings of the ESANN. 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; Accepted. https://pub.uni-bielefeld.de/record/2933893 [4] Pfannschmidt L, G\u00f6pfert C, Neumann U, Heider D, Hammer B: FRI - Feature Relevance Intervals for Interpretable and Interactive Data Exploration. Presented at the 16th IEEE International Conference on Computational Intelligence in Bioinformatics and Computational Biology, Certosa di Pontignano, Siena - Tuscany, Italy. https://ieeexplore.ieee.org/document/8791489","title":"Citing Fri"},{"location":"docs/citing_FRI/#citing-fri","text":"If you use FRI in the academic context we would like if you cite one of the following references. [1] G\u00f6pfert C, Pfannschmidt L, Hammer B. Feature Relevance Bounds for Linear Classification. In: Proceedings of the ESANN. 25th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/publication/2908201 [2] G\u00f6pfert C, Pfannschmidt L, G\u00f6pfert JP, Hammer B. Interpretation of Linear Classifiers by Means of Feature Relevance Bounds. Neurocomputing. https://pub.uni-bielefeld.de/publication/2915273 [3] Lukas Pfannschmidt, Jonathan Jakob, Michael Biehl, Peter Tino, Barbara Hammer: Feature Relevance Bounds for Ordinal Regression. Proceedings of the ESANN. 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; Accepted. https://pub.uni-bielefeld.de/record/2933893 [4] Pfannschmidt L, G\u00f6pfert C, Neumann U, Heider D, Hammer B: FRI - Feature Relevance Intervals for Interpretable and Interactive Data Exploration. Presented at the 16th IEEE International Conference on Computational Intelligence in Bioinformatics and Computational Biology, Certosa di Pontignano, Siena - Tuscany, Italy. https://ieeexplore.ieee.org/document/8791489","title":"Citing FRI"},{"location":"reference/fri/","text":"Module fri View Source import logging # noinspection PyUnresolvedReferences from fri._version import __version__ logging . basicConfig ( level = logging . INFO ) from enum import Enum import fri.model class ProblemName ( Enum ): \"\"\" Enum which contains usable models for which feature relevance intervals can be computed in :func:`~FRI`. \"\"\" CLASSIFICATION = fri . model . Classification REGRESSION = fri . model . Regression ORDINALREGRESSION = fri . model . OrdinalRegression LUPI_CLASSIFICATION = fri . model . LUPI_Classification LUPI_REGRESSION = fri . model . LUPI_Regression LUPI_ORDREGRESSION = fri . model . LUPI_OrdinalRegression NORMAL_MODELS = [ ProblemName . CLASSIFICATION , ProblemName . REGRESSION , ProblemName . ORDINALREGRESSION , ] LUPI_MODELS = [ ProblemName . LUPI_CLASSIFICATION , ProblemName . LUPI_REGRESSION , ProblemName . LUPI_ORDREGRESSION , ] from fri.toydata import ( genRegressionData , genClassificationData , genOrdinalRegressionData , quick_generate , genLupiData , ) from fri.main import FRIBase from fri.plot import plot_intervals class FRI ( FRIBase ): def __init__ ( self , problemName : object , random_state : object = None , n_jobs : object = 1 , verbose : object = 0 , n_param_search : object = 10 , n_probe_features : object = 20 , slack_regularization : object = 0.001 , slack_loss : object = 0.001 , normalize : object = True , ** kwargs , ): \"\"\" Main class to use `FRI` in programattic fashion following the scikit-learn paradigm. Parameters ---------- problemName: `ProblemName` or str Type of Problem as enum value or explicit string (e.g. \"classification\") random_state: object or int Random state object or int n_jobs: int or None Number of threads or -1 for automatic. verbose: int Verbosity if > 0 n_param_search: int Number of parameter samples in random search for hyperparameters. n_probe_features: int Number of probes to generate to improve feature selection. slack_regularization: float Allow deviation from optimal L1 norm. slack_loss: float Allow deviation of loss. normalize: boolean Normalize relevace bounds to range of [0,1] depending on L1 norm. \"\"\" if isinstance ( problemName , ProblemName ): problemtype = problemName . value else : if problemName == \"classification\" or problemName == \"class\" : problemtype = ProblemName . CLASSIFICATION elif problemName == \"regression\" or problemName == \"reg\" : problemtype = ProblemName . REGRESSION elif problemName == \"ordinalregression\" or problemName == \"ordreg\" : problemtype = ProblemName . ORDINALREGRESSION elif problemName == \"lupi_classification\" or problemName == \"lupi_class\" : problemtype = ProblemName . LUPI_CLASSIFICATION if problemtype is None : names = [ enum . name . lower () for enum in ProblemName ] print ( f \"Parameter 'problemName' was not recognized or unset. Try one of {names}.\" ) else : super () . __init__ ( problemtype , random_state = random_state , n_jobs = n_jobs , verbose = verbose , n_param_search = n_param_search , n_probe_features = n_probe_features , w_l1_slack = slack_regularization , loss_slack = slack_loss , normalize = normalize , ** kwargs , ) __all__ = [ \"genRegressionData\" , \"genClassificationData\" , \"genOrdinalRegressionData\" , \"quick_generate\" , \"plot_intervals\" , \"ProblemName\" , \"FRI\" , \"LUPI_MODELS\" , \"NORMAL_MODELS\" , \"genLupiData\" , ] Sub-modules fri.compute fri.main fri.model fri.parameter_searcher fri.plot fri.toydata fri.utils Variables LUPI_MODELS NORMAL_MODELS Functions genClassificationData def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None ) Generate synthetic classification data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation. Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes. Raises ValueError Description ValueError Wrong parameters for specified amonut of features/samples. Examples X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None X.shape (200, 2) y.shape (200,) View Source def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None , ) : \"\"\"Generate synthetic classification data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes. Raises ------ ValueError Description ValueError Wrong parameters for specified amonut of features/samples. Examples --------- >>> X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None >>> X.shape (200, 2) >>> y.shape (200,) \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) X = np . zeros (( n_samples , n_features )) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 X_informative , Y = generate_binary_classification_problem ( n_samples , n_strel + part_size , random_state ) X = _fillVariableSpace ( X_informative , random_state , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) # Add target noise if flip_y > 0 : n_flip = int ( flip_y * n_samples ) Y [ random_state.choice(n_samples, n_flip) ] *= - 1 # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise / X . std ()) return X , Y genLupiData def genLupiData ( problemName : fri . ProblemName , n_samples : int = 100 , random_state : object = None , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 ) Generate Lupi Data for Classification, Regression and Ordinal Regression Problems Parameters problemName : ProblemName Defines the type of y-values of the problem. Example ProblemName.CLASSIFICATION . n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated. Returns X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] View Source def genLupiData ( problemName : ProblemName , n_samples : int = 100 , random_state : object = None , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 , ) : \"\"\" Generate Lupi Data for Classification, Regression and Ordinal Regression Problems Parameters ---------- problemName : ProblemName Defines the type of y-values of the problem. Example `ProblemName.CLASSIFICATION`. n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated. Returns ------- X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] \"\"\" random_state = check_random_state ( random_state ) n_informative = n_strel + ( n_weakrel > 0 ) # Create truth ( prototype ) vector which contains true feature contributions # We enforce minimum of 0.1 to circumvent problems when testing for relevance w = random_state . uniform ( low = 0.5 , high = 1 , size = n_informative ) X_informative = random_state . normal ( size = ( n_samples , n_informative )) scores = np . dot ( X_informative , w ) n_features = n_strel + n_weakrel + n_repeated + n_irrel X_priv = _fillVariableSpace ( X_informative , random_state , n_features = n_features , n_redundant = n_weakrel , n_strel = n_strel , n_repeated = n_repeated , partition =[ n_weakrel ] , ) if ( problemName == \"classification\" or problemName == ProblemName . LUPI_CLASSIFICATION ) : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.65 / X_priv . std () ) X = X_priv + e y = np . zeros_like ( scores ) y [ scores > 0 ] = 1 y [ scores <= 0 ] = - 1 elif problemName == \"regression\" or problemName == ProblemName . LUPI_REGRESSION : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.5 / X_priv . std () ) X = X_priv + e y = scores elif ( problemName == \"ordinalRegression\" or problemName == ProblemName . LUPI_ORDREGRESSION ) : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.2 / X_priv . std () ) X = X_priv + e step = 1 / ( n_ordinal_bins ) quantiles = [ i * step for i in range(1, n_ordinal_bins) ] bs = np . quantile ( scores , quantiles ) bs = np . append ( bs , np . inf ) scores = scores [ :, np.newaxis ] y = np . sum ( scores - bs >= 0 , - 1 ) if n_irrel > 0 : X = np . hstack ( [ X, random_state.normal(size=(n_samples, n_irrel)) ] ) X_priv = np . hstack ( [ X_priv, random_state.normal(size=(n_samples, n_irrel)) ] ) if label_noise > 0 : sample = random_state . choice ( len ( y ), int ( len ( y ) * label_noise )) y [ sample ] = random_state . permutation ( y [ sample ] ) return ( X , X_priv , y . squeeze ()) genOrdinalRegressionData def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 ) Generate ordinal regression data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ValueError Wrong parameters for specified amonut of features/samples. View Source def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 , ) : \"\"\" Generate ordinal regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) if not n_target_bins > 1 : raise ValueError ( \"At least 2 target bins needed\" ) # Use normal regression data as starting point X_regression , Y_regression = genRegressionData ( n_samples = int ( n_samples ), n_features = int ( n_features ), n_redundant = int ( n_redundant ), n_strel = int ( n_strel ), n_repeated = int ( n_repeated ), noise = 0 , random_state = random_state , partition = partition , ) bin_size = int ( np . floor ( n_samples / n_target_bins )) rest = int ( n_samples - ( bin_size * n_target_bins )) # Sort the target values and rearange the data accordingly sort_indices = np . argsort ( Y_regression ) X = X_regression [ sort_indices ] Y = Y_regression [ sort_indices ] # Assign ordinal classes as target values for i in range ( n_target_bins ) : Y [ bin_size * i : bin_size * (i + 1) ] = i # Put non divisable rest into last bin if rest > 0 : Y [ -rest: ] = n_target_bins - 1 X , Y = shuffle ( X , Y , random_state = random_state ) # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise ) return X , Y genRegressionData def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None ) -> object Generate synthetic regression data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ValueError Wrong parameters for specified amonut of features/samples. View Source def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , ) -> object : \"\"\"Generate synthetic regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 n_informative = n_strel + part_size X = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [ :n_informative, : ] = 0.3 bias = 0 y = np . dot ( X , ground_truth ) + bias # Add noise if noise > 0.0 : y += random_state . normal ( scale = noise , size = y . shape ) X = _fillVariableSpace ( X , random_state , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) y = np . squeeze ( y ) return X , y plot_intervals def plot_intervals ( model , ticklabels = None ) Plot the relevance intervals. Parameters model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) View Source def plot_intervals ( model , ticklabels = None ): \"\"\"Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) \"\"\" if model . interval_ is not None : plotIntervals ( model . interval_ , ticklabels = ticklabels , classes = model . relevance_classes_ ) else : print ( \"Intervals not computed. Try running fit() function first.\" ) quick_generate def quick_generate ( problem : object , ** kwargs ) -> [ < class ' numpy . ndarray '>, <class ' numpy . ndarray '>] Method to wrap individual data generation functions. Allows passing problem as a string such as \"classification\" or ProblemName object of the corresponding type. For possible kwargs see genClassificationData' or genLupiData`. Parameters problem : str or ProblemName Type of data to generate (e.g. \"classification\" or ProblemName.CLASSIFICATION kwargs : **dict arguments to pass to the generation functions Returns Tuple[numpy.ndarray, numpy.ndarray] View Source def quick_generate ( problem : object , ** kwargs ) -> [ np . ndarray , np . ndarray ]: \"\"\" Method to wrap individual data generation functions. Allows passing `problem` as a string such as \" classification \" or `ProblemName` object of the corresponding type. For possible kwargs see `genClassificationData' or `genLupiData`. Parameters ---------- problem : str or `ProblemName` Type of data to generate (e.g. \" classification \" or `ProblemName.CLASSIFICATION` kwargs : **dict arguments to pass to the generation functions Returns ------- Tuple[numpy.ndarray, numpy.ndarray] \"\"\" if problem is \"regression\" or problem is ProblemName . REGRESSION : gen = genRegressionData elif problem is \"classification\" or problem is ProblemName . CLASSIFICATION : gen = genClassificationData elif problem is \"ordreg\" or problem is ProblemName . ORDINALREGRESSION : gen = genOrdinalRegressionData elif problem is \"lupi_regression\" or problem is ProblemName . LUPI_REGRESSION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_REGRESSION elif problem is \"lupi_classification\" or problem is ProblemName . LUPI_CLASSIFICATION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_CLASSIFICATION elif problem is \"lupi_ordreg\" or problem is ProblemName . LUPI_ORDREGRESSION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_ORDREGRESSION else : raise ValueError ( \"Unknown problem type. Try 'regression', 'classification' or 'ordreg' and/or add 'lupi_' prefix\" ) return gen ( ** kwargs ) Classes FRI class FRI ( problemName : object , random_state : object = None , n_jobs : object = 1 , verbose : object = 0 , n_param_search : object = 10 , n_probe_features : object = 20 , slack_regularization : object = 0.001 , slack_loss : object = 0.001 , normalize : object = True , ** kwargs ) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). View Source class FRI ( FRIBase ): def __init__ ( self , problemName : object , random_state : object = None , n_jobs : object = 1 , verbose : object = 0 , n_param_search : object = 10 , n_probe_features : object = 20 , slack_regularization : object = 0 . 001 , slack_loss : object = 0 . 001 , normalize : object = True , ** kwargs , ): \"\"\" Main class to use `FRI` in programattic fashion following the scikit-learn paradigm. Parameters ---------- problemName: `ProblemName` or str Type of Problem as enum value or explicit string (e.g. \" classification \") random_state: object or int Random state object or int n_jobs: int or None Number of threads or -1 for automatic. verbose: int Verbosity if > 0 n_param_search: int Number of parameter samples in random search for hyperparameters. n_probe_features: int Number of probes to generate to improve feature selection. slack_regularization: float Allow deviation from optimal L1 norm. slack_loss: float Allow deviation of loss. normalize: boolean Normalize relevace bounds to range of [0,1] depending on L1 norm. \"\"\" if isinstance ( problemName , ProblemName ): problemtype = problemName . value else : if problemName == \"classification\" or problemName == \"class\" : problemtype = ProblemName . CLASSIFICATION elif problemName == \"regression\" or problemName == \"reg\" : problemtype = ProblemName . REGRESSION elif problemName == \"ordinalregression\" or problemName == \"ordreg\" : problemtype = ProblemName . ORDINALREGRESSION elif problemName == \"lupi_classification\" or problemName == \"lupi_class\" : problemtype = ProblemName . LUPI_CLASSIFICATION if problemtype is None : names = [ enum . name . lower () for enum in ProblemName ] print ( f \"Parameter 'problemName' was not recognized or unset. Try one of {names}.\" ) else : super (). __init__ ( problemtype , random_state = random_state , n_jobs = n_jobs , verbose = verbose , n_param_search = n_param_search , n_probe_features = n_probe_features , w_l1_slack = slack_regularization , loss_slack = slack_loss , normalize = normalize , ** kwargs , ) Ancestors (in MRO) fri.main.FRIBase sklearn.base.BaseEstimator sklearn.feature_selection._base.SelectorMixin sklearn.base.TransformerMixin Methods constrained_intervals def constrained_intervals ( self , preset : dict ) Method to return relevance intervals which are constrained using preset ranges or values. Parameters preset : dict like, {i:float} or {i:[float,float]} Keys denote feature index, values represent a fixed single value (float) or a range of allowed values (lower and upper bound). Example : To set feature 0 to a fixed value use >>> preset = { 0 : 0.1 } or to use the minimum relevance bound >>> preset [ 1 ] = self . interval_ [ 1 , 0 ] Returns array like Relevance bounds with user constraints View Source def constrained_intervals ( self , preset : dict ): \"\"\" Method to return relevance intervals which are constrained using preset ranges or values. Parameters ---------- preset : dict like, {i:float} or {i:[float,float]} Keys denote feature index, values represent a fixed single value (float) or a range of allowed values (lower and upper bound). Example: To set feature 0 to a fixed value use >>> preset = {0: 0.1} or to use the minimum relevance bound >>> preset[1] = self.interval_[1, 0] Returns ------- array like Relevance bounds with user constraints \"\"\" # Do we have intervals ? check_is_fitted ( self , \"interval_\" ) return self . _relevance_bounds_computer . compute_multi_preset_relevance_bounds ( preset = preset , lupi_features = self . lupi_features_ ) fit def fit ( self , X , y , lupi_features = 0 , ** kwargs ) Method to fit model on data. Parameters X : numpy.ndarray y : numpy.ndarray lupi_features : int Amount of features which are considered privileged information in X . The data is expected to be structured in a way that all lupi features are at the end of the set. For example lupi_features=1 would denote the last column of X to be privileged. kwargs : dict Dictionary of additional keyword arguments depending on the model . Returns FRIBase View Source def fit ( self , X , y , lupi_features = 0 , ** kwargs ): \"\"\" Method to fit model on data. Parameters ---------- X : numpy.ndarray y : numpy.ndarray lupi_features : int Amount of features which are considered privileged information in `X`. The data is expected to be structured in a way that all lupi features are at the end of the set. For example `lupi_features=1` would denote the last column of `X` to be privileged. kwargs : dict Dictionary of additional keyword arguments depending on the `model`. Returns ------- `FRIBase` \"\"\" self . lupi_features_ = lupi_features self . n_samples_ = X . shape [ 0 ] self . n_features_ = X . shape [ 1 ] - lupi_features self . optim_model_ , best_score = self . _fit_baseline ( X , y , lupi_features , ** kwargs ) data = self . problem_type_ . preprocessing (( X , y ), lupi_features = lupi_features ) self . _relevance_bounds_computer = RelevanceBoundsIntervals ( data , self . problem_type_ , self . optim_model_ , self . random_state , self . n_probe_features , self . n_jobs , self . verbose , normalize = self . normalize , ) if lupi_features == 0 : ( self . interval_ , feature_classes , ) = self . _relevance_bounds_computer . get_normalized_intervals () else : ( self . interval_ , feature_classes , ) = self . _relevance_bounds_computer . get_normalized_lupi_intervals ( lupi_features = lupi_features ) self . _get_relevance_mask ( feature_classes ) # Return the classifier return self fit_transform def fit_transform ( self , X , y = None , ** fit_params ) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. View Source def fit_transform ( self , X , y = None , ** fit_params ) : \"\"\" Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters ---------- X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns ------- X_new : numpy array of shape [n_samples, n_features_new] Transformed array. \"\"\" # non - optimized default implementation ; override when a better # method is possible for a given clustering algorithm if y is None : # fit method of arity 1 ( unsupervised transformation ) return self . fit ( X , ** fit_params ). transform ( X ) else : # fit method of arity 2 ( supervised transformation ) return self . fit ( X , y , ** fit_params ). transform ( X ) get_params def get_params ( self , deep = True ) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out get_support def get_support ( self , indices = False ) Get a mask, or integer index, of the features selected Parameters indices : boolean (default False) If True, the return value will be an array of integers, rather than a boolean mask. Returns support : array An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. View Source def get_support ( self , indices = False ): \"\"\" Get a mask, or integer index, of the features selected Parameters ---------- indices : boolean (default False) If True, the return value will be an array of integers, rather than a boolean mask. Returns ------- support : array An index that selects the retained features from a feature vector. If `indices` is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If `indices` is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. \"\"\" mask = self . _get_support_mask () return mask if not indices else np . where ( mask )[ 0 ] inverse_transform def inverse_transform ( self , X ) Reverse the transformation operation Parameters X : array of shape [n_samples, n_selected_features] The input samples. Returns X_r : array of shape [n_samples, n_original_features] X with columns of zeros inserted where features would have been removed by :meth: transform . View Source def inverse_transform ( self , X ): \"\"\" Reverse the transformation operation Parameters ---------- X : array of shape [n_samples, n_selected_features] The input samples. Returns ------- X_r : array of shape [n_samples, n_original_features] `X` with columns of zeros inserted where features would have been removed by :meth:`transform`. \"\"\" if issparse ( X ): X = X . tocsc () # insert additional entries in indptr: # e.g. if transform changed indptr from [0 2 6 7] to [0 2 3] # col_nonzeros here will be [2 0 1] so indptr becomes [0 2 2 3] it = self . inverse_transform ( np . diff ( X . indptr ). reshape ( 1 , - 1 )) col_nonzeros = it . ravel () indptr = np . concatenate ([[ 0 ], np . cumsum ( col_nonzeros )]) Xt = csc_matrix (( X . data , X . indices , indptr ), shape = ( X . shape [ 0 ], len ( indptr ) - 1 ), dtype = X . dtype ) return Xt support = self . get_support () X = check_array ( X , dtype = None ) if support . sum () != X . shape [ 1 ]: raise ValueError ( \"X has a different shape than during fitting.\" ) if X . ndim == 1 : X = X [ None , :] Xt = np . zeros (( X . shape [ 0 ], support . size ), dtype = X . dtype ) Xt [:, support ] = X return Xt print_interval_with_class def print_interval_with_class ( self ) Pretty print the relevance intervals and determined feature relevance class View Source def print_interval_with_class ( self ) : \"\"\" Pretty print the relevance intervals and determined feature relevance class \"\"\" output = \"\" if self . interval_ is None : output += \"Model is not fitted.\" output += \"############## Relevance bounds ##############\\n\" output += \"feature: [LB -- UB], relevance class\\n\" for i in range ( self . n_features_ + self . lupi_features_ ) : if i == self . n_features_ : output += \"########## LUPI Relevance bounds\\n\" output += ( f \"{i:7}: [{self.interval_[i, 0]:1.1f} -- {self.interval_[i, 1]:1.1f}],\" ) output += f \" {self.relevance_classes_string_[i]}\\n\" return output score def score ( self , X , y ) Using fitted model predict points for X and compare to truth y . Parameters X : numpy.ndarray y : numpy.ndarray Returns Model specific score (0 is worst, 1 is best) View Source def score ( self , X , y ): \"\"\" Using fitted model predict points for `X` and compare to truth `y`. Parameters ---------- X : numpy.ndarray y : numpy.ndarray Returns ------- Model specific score (0 is worst, 1 is best) \"\"\" if self . optim_model_ : return self . optim_model_ . score ( X , y ) else : raise NotFittedError () set_params def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self transform def transform ( self , X ) Reduce X to the selected features. Parameters X : array of shape [n_samples, n_features] The input samples. Returns X_r : array of shape [n_samples, n_selected_features] The input samples with only the selected features. View Source def transform ( self , X ): \"\"\"Reduce X to the selected features. Parameters ---------- X : array of shape [n_samples, n_features] The input samples. Returns ------- X_r : array of shape [n_samples, n_selected_features] The input samples with only the selected features. \"\"\" tags = self . _get_tags () X = check_array ( X , dtype = None , accept_sparse = 'csr' , force_all_finite = not tags . get ( 'allow_nan' , True )) mask = self . get_support () if not mask . any (): warn ( \"No features were selected: either the data is\" \" too noisy or the selection test too strict.\" , UserWarning ) return np . empty ( 0 ). reshape (( X . shape [ 0 ], 0 )) if len ( mask ) != X . shape [ 1 ]: raise ValueError ( \"X has a different shape than during fitting.\" ) return X [:, safe_mask ( X , mask )] ProblemName class ProblemName ( / , * args , ** kwargs ) Enum which contains usable models for which feature relevance intervals can be computed in :func: ~FRI . View Source class ProblemName ( Enum ): \"\"\" Enum which contains usable models for which feature relevance intervals can be computed in :func:`~FRI`. \"\"\" CLASSIFICATION = fri . model . Classification REGRESSION = fri . model . Regression ORDINALREGRESSION = fri . model . OrdinalRegression LUPI_CLASSIFICATION = fri . model . LUPI_Classification LUPI_REGRESSION = fri . model . LUPI_Regression LUPI_ORDREGRESSION = fri . model . LUPI_OrdinalRegression Ancestors (in MRO) enum.Enum Class variables CLASSIFICATION LUPI_CLASSIFICATION LUPI_ORDREGRESSION LUPI_REGRESSION ORDINALREGRESSION REGRESSION name value","title":"Index"},{"location":"reference/fri/#module-fri","text":"View Source import logging # noinspection PyUnresolvedReferences from fri._version import __version__ logging . basicConfig ( level = logging . INFO ) from enum import Enum import fri.model class ProblemName ( Enum ): \"\"\" Enum which contains usable models for which feature relevance intervals can be computed in :func:`~FRI`. \"\"\" CLASSIFICATION = fri . model . Classification REGRESSION = fri . model . Regression ORDINALREGRESSION = fri . model . OrdinalRegression LUPI_CLASSIFICATION = fri . model . LUPI_Classification LUPI_REGRESSION = fri . model . LUPI_Regression LUPI_ORDREGRESSION = fri . model . LUPI_OrdinalRegression NORMAL_MODELS = [ ProblemName . CLASSIFICATION , ProblemName . REGRESSION , ProblemName . ORDINALREGRESSION , ] LUPI_MODELS = [ ProblemName . LUPI_CLASSIFICATION , ProblemName . LUPI_REGRESSION , ProblemName . LUPI_ORDREGRESSION , ] from fri.toydata import ( genRegressionData , genClassificationData , genOrdinalRegressionData , quick_generate , genLupiData , ) from fri.main import FRIBase from fri.plot import plot_intervals class FRI ( FRIBase ): def __init__ ( self , problemName : object , random_state : object = None , n_jobs : object = 1 , verbose : object = 0 , n_param_search : object = 10 , n_probe_features : object = 20 , slack_regularization : object = 0.001 , slack_loss : object = 0.001 , normalize : object = True , ** kwargs , ): \"\"\" Main class to use `FRI` in programattic fashion following the scikit-learn paradigm. Parameters ---------- problemName: `ProblemName` or str Type of Problem as enum value or explicit string (e.g. \"classification\") random_state: object or int Random state object or int n_jobs: int or None Number of threads or -1 for automatic. verbose: int Verbosity if > 0 n_param_search: int Number of parameter samples in random search for hyperparameters. n_probe_features: int Number of probes to generate to improve feature selection. slack_regularization: float Allow deviation from optimal L1 norm. slack_loss: float Allow deviation of loss. normalize: boolean Normalize relevace bounds to range of [0,1] depending on L1 norm. \"\"\" if isinstance ( problemName , ProblemName ): problemtype = problemName . value else : if problemName == \"classification\" or problemName == \"class\" : problemtype = ProblemName . CLASSIFICATION elif problemName == \"regression\" or problemName == \"reg\" : problemtype = ProblemName . REGRESSION elif problemName == \"ordinalregression\" or problemName == \"ordreg\" : problemtype = ProblemName . ORDINALREGRESSION elif problemName == \"lupi_classification\" or problemName == \"lupi_class\" : problemtype = ProblemName . LUPI_CLASSIFICATION if problemtype is None : names = [ enum . name . lower () for enum in ProblemName ] print ( f \"Parameter 'problemName' was not recognized or unset. Try one of {names}.\" ) else : super () . __init__ ( problemtype , random_state = random_state , n_jobs = n_jobs , verbose = verbose , n_param_search = n_param_search , n_probe_features = n_probe_features , w_l1_slack = slack_regularization , loss_slack = slack_loss , normalize = normalize , ** kwargs , ) __all__ = [ \"genRegressionData\" , \"genClassificationData\" , \"genOrdinalRegressionData\" , \"quick_generate\" , \"plot_intervals\" , \"ProblemName\" , \"FRI\" , \"LUPI_MODELS\" , \"NORMAL_MODELS\" , \"genLupiData\" , ]","title":"Module fri"},{"location":"reference/fri/#sub-modules","text":"fri.compute fri.main fri.model fri.parameter_searcher fri.plot fri.toydata fri.utils","title":"Sub-modules"},{"location":"reference/fri/#variables","text":"LUPI_MODELS NORMAL_MODELS","title":"Variables"},{"location":"reference/fri/#functions","text":"","title":"Functions"},{"location":"reference/fri/#genclassificationdata","text":"def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None ) Generate synthetic classification data","title":"genClassificationData"},{"location":"reference/fri/#parameters","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation.","title":"Parameters"},{"location":"reference/fri/#returns","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes.","title":"Returns"},{"location":"reference/fri/#raises","text":"ValueError Description ValueError Wrong parameters for specified amonut of features/samples.","title":"Raises"},{"location":"reference/fri/#examples","text":"X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None X.shape (200, 2) y.shape (200,) View Source def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None , ) : \"\"\"Generate synthetic classification data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes. Raises ------ ValueError Description ValueError Wrong parameters for specified amonut of features/samples. Examples --------- >>> X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None >>> X.shape (200, 2) >>> y.shape (200,) \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) X = np . zeros (( n_samples , n_features )) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 X_informative , Y = generate_binary_classification_problem ( n_samples , n_strel + part_size , random_state ) X = _fillVariableSpace ( X_informative , random_state , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) # Add target noise if flip_y > 0 : n_flip = int ( flip_y * n_samples ) Y [ random_state.choice(n_samples, n_flip) ] *= - 1 # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise / X . std ()) return X , Y","title":"Examples"},{"location":"reference/fri/#genlupidata","text":"def genLupiData ( problemName : fri . ProblemName , n_samples : int = 100 , random_state : object = None , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 ) Generate Lupi Data for Classification, Regression and Ordinal Regression Problems","title":"genLupiData"},{"location":"reference/fri/#parameters_1","text":"problemName : ProblemName Defines the type of y-values of the problem. Example ProblemName.CLASSIFICATION . n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated.","title":"Parameters"},{"location":"reference/fri/#returns_1","text":"X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] View Source def genLupiData ( problemName : ProblemName , n_samples : int = 100 , random_state : object = None , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 , ) : \"\"\" Generate Lupi Data for Classification, Regression and Ordinal Regression Problems Parameters ---------- problemName : ProblemName Defines the type of y-values of the problem. Example `ProblemName.CLASSIFICATION`. n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated. Returns ------- X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] \"\"\" random_state = check_random_state ( random_state ) n_informative = n_strel + ( n_weakrel > 0 ) # Create truth ( prototype ) vector which contains true feature contributions # We enforce minimum of 0.1 to circumvent problems when testing for relevance w = random_state . uniform ( low = 0.5 , high = 1 , size = n_informative ) X_informative = random_state . normal ( size = ( n_samples , n_informative )) scores = np . dot ( X_informative , w ) n_features = n_strel + n_weakrel + n_repeated + n_irrel X_priv = _fillVariableSpace ( X_informative , random_state , n_features = n_features , n_redundant = n_weakrel , n_strel = n_strel , n_repeated = n_repeated , partition =[ n_weakrel ] , ) if ( problemName == \"classification\" or problemName == ProblemName . LUPI_CLASSIFICATION ) : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.65 / X_priv . std () ) X = X_priv + e y = np . zeros_like ( scores ) y [ scores > 0 ] = 1 y [ scores <= 0 ] = - 1 elif problemName == \"regression\" or problemName == ProblemName . LUPI_REGRESSION : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.5 / X_priv . std () ) X = X_priv + e y = scores elif ( problemName == \"ordinalRegression\" or problemName == ProblemName . LUPI_ORDREGRESSION ) : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.2 / X_priv . std () ) X = X_priv + e step = 1 / ( n_ordinal_bins ) quantiles = [ i * step for i in range(1, n_ordinal_bins) ] bs = np . quantile ( scores , quantiles ) bs = np . append ( bs , np . inf ) scores = scores [ :, np.newaxis ] y = np . sum ( scores - bs >= 0 , - 1 ) if n_irrel > 0 : X = np . hstack ( [ X, random_state.normal(size=(n_samples, n_irrel)) ] ) X_priv = np . hstack ( [ X_priv, random_state.normal(size=(n_samples, n_irrel)) ] ) if label_noise > 0 : sample = random_state . choice ( len ( y ), int ( len ( y ) * label_noise )) y [ sample ] = random_state . permutation ( y [ sample ] ) return ( X , X_priv , y . squeeze ())","title":"Returns"},{"location":"reference/fri/#genordinalregressiondata","text":"def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 ) Generate ordinal regression data","title":"genOrdinalRegressionData"},{"location":"reference/fri/#parameters_2","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes","title":"Parameters"},{"location":"reference/fri/#returns_2","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target).","title":"Returns"},{"location":"reference/fri/#raises_1","text":"ValueError Wrong parameters for specified amonut of features/samples. View Source def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 , ) : \"\"\" Generate ordinal regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) if not n_target_bins > 1 : raise ValueError ( \"At least 2 target bins needed\" ) # Use normal regression data as starting point X_regression , Y_regression = genRegressionData ( n_samples = int ( n_samples ), n_features = int ( n_features ), n_redundant = int ( n_redundant ), n_strel = int ( n_strel ), n_repeated = int ( n_repeated ), noise = 0 , random_state = random_state , partition = partition , ) bin_size = int ( np . floor ( n_samples / n_target_bins )) rest = int ( n_samples - ( bin_size * n_target_bins )) # Sort the target values and rearange the data accordingly sort_indices = np . argsort ( Y_regression ) X = X_regression [ sort_indices ] Y = Y_regression [ sort_indices ] # Assign ordinal classes as target values for i in range ( n_target_bins ) : Y [ bin_size * i : bin_size * (i + 1) ] = i # Put non divisable rest into last bin if rest > 0 : Y [ -rest: ] = n_target_bins - 1 X , Y = shuffle ( X , Y , random_state = random_state ) # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise ) return X , Y","title":"Raises"},{"location":"reference/fri/#genregressiondata","text":"def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None ) -> object Generate synthetic regression data","title":"genRegressionData"},{"location":"reference/fri/#parameters_3","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation.","title":"Parameters"},{"location":"reference/fri/#returns_3","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target).","title":"Returns"},{"location":"reference/fri/#raises_2","text":"ValueError Wrong parameters for specified amonut of features/samples. View Source def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , ) -> object : \"\"\"Generate synthetic regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 n_informative = n_strel + part_size X = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [ :n_informative, : ] = 0.3 bias = 0 y = np . dot ( X , ground_truth ) + bias # Add noise if noise > 0.0 : y += random_state . normal ( scale = noise , size = y . shape ) X = _fillVariableSpace ( X , random_state , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) y = np . squeeze ( y ) return X , y","title":"Raises"},{"location":"reference/fri/#plot_intervals","text":"def plot_intervals ( model , ticklabels = None ) Plot the relevance intervals.","title":"plot_intervals"},{"location":"reference/fri/#parameters_4","text":"model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) View Source def plot_intervals ( model , ticklabels = None ): \"\"\"Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) \"\"\" if model . interval_ is not None : plotIntervals ( model . interval_ , ticklabels = ticklabels , classes = model . relevance_classes_ ) else : print ( \"Intervals not computed. Try running fit() function first.\" )","title":"Parameters"},{"location":"reference/fri/#quick_generate","text":"def quick_generate ( problem : object , ** kwargs ) -> [ < class ' numpy . ndarray '>, <class ' numpy . ndarray '>] Method to wrap individual data generation functions. Allows passing problem as a string such as \"classification\" or ProblemName object of the corresponding type. For possible kwargs see genClassificationData' or genLupiData`.","title":"quick_generate"},{"location":"reference/fri/#parameters_5","text":"problem : str or ProblemName Type of data to generate (e.g. \"classification\" or ProblemName.CLASSIFICATION kwargs : **dict arguments to pass to the generation functions","title":"Parameters"},{"location":"reference/fri/#returns_4","text":"Tuple[numpy.ndarray, numpy.ndarray] View Source def quick_generate ( problem : object , ** kwargs ) -> [ np . ndarray , np . ndarray ]: \"\"\" Method to wrap individual data generation functions. Allows passing `problem` as a string such as \" classification \" or `ProblemName` object of the corresponding type. For possible kwargs see `genClassificationData' or `genLupiData`. Parameters ---------- problem : str or `ProblemName` Type of data to generate (e.g. \" classification \" or `ProblemName.CLASSIFICATION` kwargs : **dict arguments to pass to the generation functions Returns ------- Tuple[numpy.ndarray, numpy.ndarray] \"\"\" if problem is \"regression\" or problem is ProblemName . REGRESSION : gen = genRegressionData elif problem is \"classification\" or problem is ProblemName . CLASSIFICATION : gen = genClassificationData elif problem is \"ordreg\" or problem is ProblemName . ORDINALREGRESSION : gen = genOrdinalRegressionData elif problem is \"lupi_regression\" or problem is ProblemName . LUPI_REGRESSION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_REGRESSION elif problem is \"lupi_classification\" or problem is ProblemName . LUPI_CLASSIFICATION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_CLASSIFICATION elif problem is \"lupi_ordreg\" or problem is ProblemName . LUPI_ORDREGRESSION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_ORDREGRESSION else : raise ValueError ( \"Unknown problem type. Try 'regression', 'classification' or 'ordreg' and/or add 'lupi_' prefix\" ) return gen ( ** kwargs )","title":"Returns"},{"location":"reference/fri/#classes","text":"","title":"Classes"},{"location":"reference/fri/#fri","text":"class FRI ( problemName : object , random_state : object = None , n_jobs : object = 1 , verbose : object = 0 , n_param_search : object = 10 , n_probe_features : object = 20 , slack_regularization : object = 0.001 , slack_loss : object = 0.001 , normalize : object = True , ** kwargs ) Base class for all estimators in scikit-learn","title":"FRI"},{"location":"reference/fri/#notes","text":"All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). View Source class FRI ( FRIBase ): def __init__ ( self , problemName : object , random_state : object = None , n_jobs : object = 1 , verbose : object = 0 , n_param_search : object = 10 , n_probe_features : object = 20 , slack_regularization : object = 0 . 001 , slack_loss : object = 0 . 001 , normalize : object = True , ** kwargs , ): \"\"\" Main class to use `FRI` in programattic fashion following the scikit-learn paradigm. Parameters ---------- problemName: `ProblemName` or str Type of Problem as enum value or explicit string (e.g. \" classification \") random_state: object or int Random state object or int n_jobs: int or None Number of threads or -1 for automatic. verbose: int Verbosity if > 0 n_param_search: int Number of parameter samples in random search for hyperparameters. n_probe_features: int Number of probes to generate to improve feature selection. slack_regularization: float Allow deviation from optimal L1 norm. slack_loss: float Allow deviation of loss. normalize: boolean Normalize relevace bounds to range of [0,1] depending on L1 norm. \"\"\" if isinstance ( problemName , ProblemName ): problemtype = problemName . value else : if problemName == \"classification\" or problemName == \"class\" : problemtype = ProblemName . CLASSIFICATION elif problemName == \"regression\" or problemName == \"reg\" : problemtype = ProblemName . REGRESSION elif problemName == \"ordinalregression\" or problemName == \"ordreg\" : problemtype = ProblemName . ORDINALREGRESSION elif problemName == \"lupi_classification\" or problemName == \"lupi_class\" : problemtype = ProblemName . LUPI_CLASSIFICATION if problemtype is None : names = [ enum . name . lower () for enum in ProblemName ] print ( f \"Parameter 'problemName' was not recognized or unset. Try one of {names}.\" ) else : super (). __init__ ( problemtype , random_state = random_state , n_jobs = n_jobs , verbose = verbose , n_param_search = n_param_search , n_probe_features = n_probe_features , w_l1_slack = slack_regularization , loss_slack = slack_loss , normalize = normalize , ** kwargs , )","title":"Notes"},{"location":"reference/fri/#ancestors-in-mro","text":"fri.main.FRIBase sklearn.base.BaseEstimator sklearn.feature_selection._base.SelectorMixin sklearn.base.TransformerMixin","title":"Ancestors (in MRO)"},{"location":"reference/fri/#methods","text":"","title":"Methods"},{"location":"reference/fri/#constrained_intervals","text":"def constrained_intervals ( self , preset : dict ) Method to return relevance intervals which are constrained using preset ranges or values.","title":"constrained_intervals"},{"location":"reference/fri/#parameters_6","text":"preset : dict like, {i:float} or {i:[float,float]} Keys denote feature index, values represent a fixed single value (float) or a range of allowed values (lower and upper bound). Example : To set feature 0 to a fixed value use >>> preset = { 0 : 0.1 } or to use the minimum relevance bound >>> preset [ 1 ] = self . interval_ [ 1 , 0 ]","title":"Parameters"},{"location":"reference/fri/#returns_5","text":"array like Relevance bounds with user constraints View Source def constrained_intervals ( self , preset : dict ): \"\"\" Method to return relevance intervals which are constrained using preset ranges or values. Parameters ---------- preset : dict like, {i:float} or {i:[float,float]} Keys denote feature index, values represent a fixed single value (float) or a range of allowed values (lower and upper bound). Example: To set feature 0 to a fixed value use >>> preset = {0: 0.1} or to use the minimum relevance bound >>> preset[1] = self.interval_[1, 0] Returns ------- array like Relevance bounds with user constraints \"\"\" # Do we have intervals ? check_is_fitted ( self , \"interval_\" ) return self . _relevance_bounds_computer . compute_multi_preset_relevance_bounds ( preset = preset , lupi_features = self . lupi_features_ )","title":"Returns"},{"location":"reference/fri/#fit","text":"def fit ( self , X , y , lupi_features = 0 , ** kwargs ) Method to fit model on data.","title":"fit"},{"location":"reference/fri/#parameters_7","text":"X : numpy.ndarray y : numpy.ndarray lupi_features : int Amount of features which are considered privileged information in X . The data is expected to be structured in a way that all lupi features are at the end of the set. For example lupi_features=1 would denote the last column of X to be privileged. kwargs : dict Dictionary of additional keyword arguments depending on the model .","title":"Parameters"},{"location":"reference/fri/#returns_6","text":"FRIBase View Source def fit ( self , X , y , lupi_features = 0 , ** kwargs ): \"\"\" Method to fit model on data. Parameters ---------- X : numpy.ndarray y : numpy.ndarray lupi_features : int Amount of features which are considered privileged information in `X`. The data is expected to be structured in a way that all lupi features are at the end of the set. For example `lupi_features=1` would denote the last column of `X` to be privileged. kwargs : dict Dictionary of additional keyword arguments depending on the `model`. Returns ------- `FRIBase` \"\"\" self . lupi_features_ = lupi_features self . n_samples_ = X . shape [ 0 ] self . n_features_ = X . shape [ 1 ] - lupi_features self . optim_model_ , best_score = self . _fit_baseline ( X , y , lupi_features , ** kwargs ) data = self . problem_type_ . preprocessing (( X , y ), lupi_features = lupi_features ) self . _relevance_bounds_computer = RelevanceBoundsIntervals ( data , self . problem_type_ , self . optim_model_ , self . random_state , self . n_probe_features , self . n_jobs , self . verbose , normalize = self . normalize , ) if lupi_features == 0 : ( self . interval_ , feature_classes , ) = self . _relevance_bounds_computer . get_normalized_intervals () else : ( self . interval_ , feature_classes , ) = self . _relevance_bounds_computer . get_normalized_lupi_intervals ( lupi_features = lupi_features ) self . _get_relevance_mask ( feature_classes ) # Return the classifier return self","title":"Returns"},{"location":"reference/fri/#fit_transform","text":"def fit_transform ( self , X , y = None , ** fit_params ) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.","title":"fit_transform"},{"location":"reference/fri/#parameters_8","text":"X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters.","title":"Parameters"},{"location":"reference/fri/#returns_7","text":"X_new : numpy array of shape [n_samples, n_features_new] Transformed array. View Source def fit_transform ( self , X , y = None , ** fit_params ) : \"\"\" Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters ---------- X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns ------- X_new : numpy array of shape [n_samples, n_features_new] Transformed array. \"\"\" # non - optimized default implementation ; override when a better # method is possible for a given clustering algorithm if y is None : # fit method of arity 1 ( unsupervised transformation ) return self . fit ( X , ** fit_params ). transform ( X ) else : # fit method of arity 2 ( supervised transformation ) return self . fit ( X , y , ** fit_params ). transform ( X )","title":"Returns"},{"location":"reference/fri/#get_params","text":"def get_params ( self , deep = True ) Get parameters for this estimator.","title":"get_params"},{"location":"reference/fri/#parameters_9","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"reference/fri/#returns_8","text":"params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out","title":"Returns"},{"location":"reference/fri/#get_support","text":"def get_support ( self , indices = False ) Get a mask, or integer index, of the features selected","title":"get_support"},{"location":"reference/fri/#parameters_10","text":"indices : boolean (default False) If True, the return value will be an array of integers, rather than a boolean mask.","title":"Parameters"},{"location":"reference/fri/#returns_9","text":"support : array An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. View Source def get_support ( self , indices = False ): \"\"\" Get a mask, or integer index, of the features selected Parameters ---------- indices : boolean (default False) If True, the return value will be an array of integers, rather than a boolean mask. Returns ------- support : array An index that selects the retained features from a feature vector. If `indices` is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If `indices` is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. \"\"\" mask = self . _get_support_mask () return mask if not indices else np . where ( mask )[ 0 ]","title":"Returns"},{"location":"reference/fri/#inverse_transform","text":"def inverse_transform ( self , X ) Reverse the transformation operation","title":"inverse_transform"},{"location":"reference/fri/#parameters_11","text":"X : array of shape [n_samples, n_selected_features] The input samples.","title":"Parameters"},{"location":"reference/fri/#returns_10","text":"X_r : array of shape [n_samples, n_original_features] X with columns of zeros inserted where features would have been removed by :meth: transform . View Source def inverse_transform ( self , X ): \"\"\" Reverse the transformation operation Parameters ---------- X : array of shape [n_samples, n_selected_features] The input samples. Returns ------- X_r : array of shape [n_samples, n_original_features] `X` with columns of zeros inserted where features would have been removed by :meth:`transform`. \"\"\" if issparse ( X ): X = X . tocsc () # insert additional entries in indptr: # e.g. if transform changed indptr from [0 2 6 7] to [0 2 3] # col_nonzeros here will be [2 0 1] so indptr becomes [0 2 2 3] it = self . inverse_transform ( np . diff ( X . indptr ). reshape ( 1 , - 1 )) col_nonzeros = it . ravel () indptr = np . concatenate ([[ 0 ], np . cumsum ( col_nonzeros )]) Xt = csc_matrix (( X . data , X . indices , indptr ), shape = ( X . shape [ 0 ], len ( indptr ) - 1 ), dtype = X . dtype ) return Xt support = self . get_support () X = check_array ( X , dtype = None ) if support . sum () != X . shape [ 1 ]: raise ValueError ( \"X has a different shape than during fitting.\" ) if X . ndim == 1 : X = X [ None , :] Xt = np . zeros (( X . shape [ 0 ], support . size ), dtype = X . dtype ) Xt [:, support ] = X return Xt","title":"Returns"},{"location":"reference/fri/#print_interval_with_class","text":"def print_interval_with_class ( self ) Pretty print the relevance intervals and determined feature relevance class View Source def print_interval_with_class ( self ) : \"\"\" Pretty print the relevance intervals and determined feature relevance class \"\"\" output = \"\" if self . interval_ is None : output += \"Model is not fitted.\" output += \"############## Relevance bounds ##############\\n\" output += \"feature: [LB -- UB], relevance class\\n\" for i in range ( self . n_features_ + self . lupi_features_ ) : if i == self . n_features_ : output += \"########## LUPI Relevance bounds\\n\" output += ( f \"{i:7}: [{self.interval_[i, 0]:1.1f} -- {self.interval_[i, 1]:1.1f}],\" ) output += f \" {self.relevance_classes_string_[i]}\\n\" return output","title":"print_interval_with_class"},{"location":"reference/fri/#score","text":"def score ( self , X , y ) Using fitted model predict points for X and compare to truth y .","title":"score"},{"location":"reference/fri/#parameters_12","text":"X : numpy.ndarray y : numpy.ndarray","title":"Parameters"},{"location":"reference/fri/#returns_11","text":"Model specific score (0 is worst, 1 is best) View Source def score ( self , X , y ): \"\"\" Using fitted model predict points for `X` and compare to truth `y`. Parameters ---------- X : numpy.ndarray y : numpy.ndarray Returns ------- Model specific score (0 is worst, 1 is best) \"\"\" if self . optim_model_ : return self . optim_model_ . score ( X , y ) else : raise NotFittedError ()","title":"Returns"},{"location":"reference/fri/#set_params","text":"def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params"},{"location":"reference/fri/#parameters_13","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"reference/fri/#returns_12","text":"self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Returns"},{"location":"reference/fri/#transform","text":"def transform ( self , X ) Reduce X to the selected features.","title":"transform"},{"location":"reference/fri/#parameters_14","text":"X : array of shape [n_samples, n_features] The input samples.","title":"Parameters"},{"location":"reference/fri/#returns_13","text":"X_r : array of shape [n_samples, n_selected_features] The input samples with only the selected features. View Source def transform ( self , X ): \"\"\"Reduce X to the selected features. Parameters ---------- X : array of shape [n_samples, n_features] The input samples. Returns ------- X_r : array of shape [n_samples, n_selected_features] The input samples with only the selected features. \"\"\" tags = self . _get_tags () X = check_array ( X , dtype = None , accept_sparse = 'csr' , force_all_finite = not tags . get ( 'allow_nan' , True )) mask = self . get_support () if not mask . any (): warn ( \"No features were selected: either the data is\" \" too noisy or the selection test too strict.\" , UserWarning ) return np . empty ( 0 ). reshape (( X . shape [ 0 ], 0 )) if len ( mask ) != X . shape [ 1 ]: raise ValueError ( \"X has a different shape than during fitting.\" ) return X [:, safe_mask ( X , mask )]","title":"Returns"},{"location":"reference/fri/#problemname","text":"class ProblemName ( / , * args , ** kwargs ) Enum which contains usable models for which feature relevance intervals can be computed in :func: ~FRI . View Source class ProblemName ( Enum ): \"\"\" Enum which contains usable models for which feature relevance intervals can be computed in :func:`~FRI`. \"\"\" CLASSIFICATION = fri . model . Classification REGRESSION = fri . model . Regression ORDINALREGRESSION = fri . model . OrdinalRegression LUPI_CLASSIFICATION = fri . model . LUPI_Classification LUPI_REGRESSION = fri . model . LUPI_Regression LUPI_ORDREGRESSION = fri . model . LUPI_OrdinalRegression","title":"ProblemName"},{"location":"reference/fri/#ancestors-in-mro_1","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/fri/#class-variables","text":"CLASSIFICATION LUPI_CLASSIFICATION LUPI_ORDREGRESSION LUPI_REGRESSION ORDINALREGRESSION REGRESSION name value","title":"Class variables"},{"location":"reference/fri/compute/","text":"Module fri.compute This module includes all important computation functions which are used internally. They (normally) should not be used by users. View Source \"\"\"This module includes all important computation functions which are used internally. They (normally) should not be used by users. \"\"\" import logging from collections import defaultdict import attr import joblib import numpy as np from scipy import stats from fri.model.base_cvxproblem import Relevance_CVXProblem from fri.model.base_initmodel import InitModel from fri.model.base_type import ProblemType from fri.utils import permutate_feature_in_data MIN_N_PROBE_FEATURES = 20 # Lower bound of probe features def _start_solver_worker ( bound : Relevance_CVXProblem ): \"\"\" Worker thread method for parallel computation \"\"\" return bound . solve () class RelevanceBoundsIntervals ( object ): def __init__ ( self , data , problem_type : ProblemType , best_init_model : InitModel , random_state , n_resampling , n_jobs , verbose , normalize = True , ): self . data = data self . problem_type = problem_type self . verbose = verbose self . n_jobs = n_jobs self . n_resampling = n_resampling self . random_state = random_state self . best_init_model = best_init_model self . best_hyperparameters = best_init_model . get_params () self . normalize = normalize # Relax constraints to improve stability relaxed_constraints = problem_type . get_relaxed_constraints ( best_init_model . constraints ) self . init_constraints = relaxed_constraints def get_normalized_lupi_intervals ( self , lupi_features , presetModel = None ): # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO: handle other data formats all_d = X . shape [ 1 ] normal_d = all_d - lupi_features # Compute relevance bounds and probes for normal features and LUPI with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : d_n = _get_necessary_dimensions ( normal_d , presetModel ) rb = self . compute_relevance_bounds ( d_n , parallel = parallel ) probe_upper = self . compute_probe_values ( d_n , True , parallel = parallel ) probe_lower = self . compute_probe_values ( d_n , False , parallel = parallel ) d_l = _get_necessary_dimensions ( all_d , presetModel , start = normal_d ) rb_l = self . compute_relevance_bounds ( d_l , parallel = parallel ) probe_priv_upper = self . compute_probe_values ( d_l , True , parallel = parallel ) probe_priv_lower = self . compute_probe_values ( d_l , False , parallel = parallel ) probes = [ probe_lower , probe_upper , probe_priv_lower , probe_priv_upper ] # # Postprocess # # Get Scaling Parameters l1 = self . init_constraints [ \"w_l1\" ] l1_priv = self . init_constraints [ \"w_priv_l1\" ] l1 = l1 + l1_priv # Normalize Normal and Lupi features rb_norm = self . _postprocessing ( l1 , rb ) rb_l_norm = self . _postprocessing ( l1 , rb_l ) interval_ = np . concatenate ([ rb_norm , rb_l_norm ]) # Normalize Probes probe_lower = self . _postprocessing ( l1 , probe_lower ) probe_upper = self . _postprocessing ( l1 , probe_upper ) probe_priv_lower = self . _postprocessing ( l1 , probe_priv_lower ) probe_priv_upper = self . _postprocessing ( l1 , probe_priv_upper ) # # # Classify features self . f_classifier = FeatureClassifier ( probe_lower , probe_upper , verbose = self . verbose ) feature_classes = self . f_classifier . classify ( rb_norm ) self . f_classifier_lupi = FeatureClassifier ( probe_priv_lower , probe_priv_upper , verbose = self . verbose ) feature_classes_lupi = self . f_classifier_lupi . classify ( rb_l_norm ) fc_both = np . concatenate ([ feature_classes , feature_classes_lupi ]) return interval_ , fc_both def get_normalized_intervals ( self , presetModel = None ): # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO: handle other data formats d = X . shape [ 1 ] # Depending on the preset model, we dont need to compute all bounds # e.g. in the case of fixed features we skip those dims = _get_necessary_dimensions ( d , presetModel ) with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : relevance_bounds = self . compute_relevance_bounds ( dims , parallel = parallel , presetModel = presetModel ) probe_values_upper = self . compute_probe_values ( dims , isUpper = True , parallel = parallel , presetModel = presetModel ) probe_values_lower = self . compute_probe_values ( dims , isUpper = False , parallel = parallel , presetModel = presetModel ) # Postprocess bounds norm_bounds = self . _postprocessing ( self . best_init_model . L1_factor , relevance_bounds ) norm_probe_values_upper = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_upper ) norm_probe_values_lower = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_lower ) self . f_classifier = FeatureClassifier ( norm_probe_values_lower , norm_probe_values_upper , verbose = self . verbose ) feature_classes = self . f_classifier . classify ( norm_bounds ) return norm_bounds , feature_classes def compute_relevance_bounds ( self , dims , parallel = None , presetModel = None , solverargs = None ): init_model_state = self . best_init_model . model_state work_queue = self . _generate_relevance_bounds_tasks ( dims , self . data , presetModel , init_model_state ) # Solve relevance bounds in parallel (when available) if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) bound_results = parallel ( map ( joblib . delayed ( _start_solver_worker ), work_queue )) # Retrieve results and aggregate values in dict solved_bounds = defaultdict ( list ) for finished_bound in bound_results : # Only add bounds with feasible solutions if finished_bound . is_solved : solved_bounds [ finished_bound . current_feature ] . append ( finished_bound ) # Initalize array for pair of bounds(= intervals) length = len ( dims ) intervals = np . zeros (( length , 2 )) for abs_index , rel_index in zip ( dims , range ( length )): # Return interval for feature i (can be a fixed value when set beforehand) interval_i = self . _create_interval ( abs_index , solved_bounds , presetModel ) intervals [ rel_index ] = interval_i return intervals # TODO: add model model_state (omega, bias) to return value def compute_probe_values ( self , dims , isUpper = True , parallel = None , presetModel = None ): # Get model parameters init_model_state = self . best_init_model . model_state # Prepare parallel framework if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) # Generate probe_queue = self . _generate_probe_value_tasks ( self . data , dims , isUpper , self . n_resampling , self . random_state , presetModel , init_model_state , ) # Compute solution probe_results = parallel ( map ( joblib . delayed ( _start_solver_worker ), probe_queue )) # probe_values.extend([probe.objective.value for probe in probe_results if probe.is_solved]) candidates = defaultdict ( list ) for candidate in probe_results : # Only add bounds with feasible solutions if candidate . is_solved : candidates [ candidate . probeID ] . append ( candidate ) probe_values = [] for probes_for_ID in candidates . values (): if isUpper : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( probes_for_ID ) ) else : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( probes_for_ID ) ) return np . array ( probe_values ) def _generate_relevance_bounds_tasks ( self , dims , data , preset_model = None , best_model_state = None ): # Do not compute bounds for fixed features if preset_model is not None : dims = [ di for di in dims if di not in preset_model ] # Instantiate objects for computation later for di in dims : # Add Lower Bound problem(s) to work list yield from self . problem_type . get_cvxproblem_template . generate_lower_bound_problem ( self . best_hyperparameters , self . init_constraints , best_model_state , data , di , preset_model , ) # Add problem(s) for Upper bound yield from self . problem_type . get_cvxproblem_template . generate_upper_bound_problem ( self . best_hyperparameters , self . init_constraints , best_model_state , data , di , preset_model , ) def _generate_probe_value_tasks ( self , data , dims , isUpper , n_resampling , random_state , preset_model = None , best_model_state = None , ): if isUpper : factory = ( self . problem_type . get_cvxproblem_template . generate_upper_bound_problem ) else : factory = ( self . problem_type . get_cvxproblem_template . generate_lower_bound_problem ) # Random sample n_resampling shadow features by permuting real features and computing upper bound random_choice = random_state . choice ( a = dims , size = n_resampling ) # Instantiate objects for i , di in enumerate ( random_choice ): data_perm = permutate_feature_in_data ( data , di , random_state ) # We only use upper bounds as probe features yield from factory ( self . best_hyperparameters , self . init_constraints , best_model_state , data_perm , di , preset_model , probeID = i , ) def _create_interval ( self , feature : int , solved_bounds : dict , presetModel : dict = None ): # Return preset values for fixed features if presetModel is not None : if feature in presetModel : return presetModel [ feature ] . squeeze () all_bounds = solved_bounds [ feature ] min_problems_candidates = [ p for p in all_bounds if p . isLowerBound ] max_problems_candidates = [ p for p in all_bounds if not p . isLowerBound ] if len ( all_bounds ) < 2 : logging . error ( f \"(Some) relevance bounds for feature {feature} were not solved.\" ) raise Exception ( \"Infeasible bound(s).\" ) lower_bound = self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( min_problems_candidates ) upper_bound = self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( max_problems_candidates ) return lower_bound , upper_bound def compute_single_preset_relevance_bounds ( self , i : int , signed_preset_i : [ float , float ] ): \"\"\" Method to run method once for one restricted feature Parameters ---------- i: restricted feature signed_preset_i: restricted range of feature i (set before optimization = preset) \"\"\" preset = { i : signed_preset_i } rangevector = self . compute_multi_preset_relevance_bounds ( preset ) return rangevector def compute_multi_preset_relevance_bounds ( self , preset , lupi_features = 0 ): \"\"\" Method to run method with preset values Parameters ---------- lupi_features \"\"\" # The user is working with normalized values while we compute them unscaled if self . normalize : normalized = {} for k , v in preset . items (): normalized [ k ] = np . asarray ( v ) * self . best_init_model . L1_factor preset = normalized # Add sign to presets preset = self . _add_sign_to_preset ( preset ) # Calculate all bounds with feature i set to min_i if lupi_features > 0 : rangevector , f_classes = self . get_normalized_lupi_intervals ( lupi_features , presetModel = preset ) else : rangevector , f_classes = self . get_normalized_intervals ( presetModel = preset ) return rangevector def _add_sign_to_preset ( self , unsigned_presets ): \"\"\" We need signed presets for our convex problem definition later. We reuse the coefficients of the optimal model for this Parameters ---------- unsigned_presets : dict Returns ------- dict \"\"\" signed_presets = {} # Obtain optimal model parameters w = self . best_init_model . model_state [ \"w\" ] sum = 0 for i , preset in unsigned_presets . items (): preset = np . array ( preset ) if preset . size == 1 : preset = np . repeat ( preset , 2 ) unsigned_preset_i = np . sign ( w [ i ]) * preset # accumulate maximal feature contribution sum += unsigned_preset_i [ 1 ] # Take upper preset signed_presets [ i ] = unsigned_preset_i # Check if unsigned_presets makes sense l1 = self . init_constraints [ \"w_l1\" ] if sum > l1 : print ( \"maximum L1 norm of presets: \" , sum ) print ( \"L1 allowed:\" , l1 ) print ( \"Presets are not feasible. Try lowering values.\" ) return return signed_presets def _postprocessing ( self , L1 , rangevector , round_to_zero = True ): if self . normalize : assert L1 > 0 rangevector = rangevector . copy () / L1 if round_to_zero : rangevector [ rangevector <= 1e-11 ] = 0 return rangevector def _get_necessary_dimensions ( d : int , presetModel : dict = None , start = 0 ): dims = np . arange ( start , d ) # if presetModel is not None: # # Exclude fixed (preset) dimensions from being redundantly computed # dims = [di for di in dims if di not in presetModel.keys()] # TODO: check the removal of this block return dims class FeatureClassifier : def __init__ ( self , probes_low , probes_up , fpr = 1e-4 , verbose = 0 ): logging . info ( \"**** Feature Selection ****\" ) logging . info ( \"Generating Lower Probe Statistic\" ) self . lower_stat = create_probe_statistic ( probes_low , fpr , verbose = verbose ) logging . info ( self . lower_stat ) logging . info ( \"Generating Upper Probe Statistic\" ) self . upper_stat = create_probe_statistic ( probes_up , fpr , verbose = verbose ) logging . info ( self . upper_stat ) def classify ( self , relevance_bounds ): \"\"\" Parameters ---------- relevance_bounds : numpy.ndarray two dimensional array with relevance bounds first column coresponds to minrel and second to maxrel \"\"\" weakly = relevance_bounds [:, 1 ] > self . upper_stat . upper_threshold strongly = relevance_bounds [:, 0 ] > self . lower_stat . upper_threshold both = np . logical_and ( weakly , strongly ) prediction = np . zeros ( relevance_bounds . shape [ 0 ], dtype = np . int ) prediction [ weakly ] = 1 prediction [ both ] = 2 return prediction @attr . s class ProbeStatistic : \"\"\" Collects the threshold values about the statistics from one kind of relevance bounds (minrel or maxrel). \"\"\" lower_threshold = attr . ib ( type = float ) upper_threshold = attr . ib ( type = float ) n_probes = attr . ib ( type = int ) def create_probe_statistic ( probe_values , fpr , verbose = 0 ): # Create prediction interval statistics based on randomly permutated probe features (based on real features) n = len ( probe_values ) if n == 0 : if verbose > 0 : logging . info ( \"All probes were infeasible. All features considered relevant.\" ) # # If all probes were infeasible we expect an empty list # # If they are infeasible it also means that only strongly relevant features were in the data # # As such we just set the prediction without considering the statistics low_t = 0 up_t = 0 elif n == 1 : val = probe_values [ 0 ] low_t = val up_t = val else : probe_values = np . asarray ( probe_values ) mean = probe_values . mean () s = probe_values . std () low_t = mean + stats . t ( df = n - 1 ) . ppf ( fpr ) * s * np . sqrt ( 1 + ( 1 / n )) up_t = mean - stats . t ( df = n - 1 ) . ppf ( fpr ) * s * np . sqrt ( 1 + ( 1 / n )) return ProbeStatistic ( low_t , up_t , n ) Variables MIN_N_PROBE_FEATURES Functions create_probe_statistic def create_probe_statistic ( probe_values , fpr , verbose = 0 ) View Source def create_probe_statistic ( probe_values , fpr , verbose = 0 ): # Create prediction interval statistics based on randomly permutated probe features ( based on real features ) n = len ( probe_values ) if n == 0 : if verbose > 0 : logging . info ( \"All probes were infeasible. All features considered relevant.\" ) # # If all probes were infeasible we expect an empty list # # If they are infeasible it also means that only strongly relevant features were in the data # # As such we just set the prediction without considering the statistics low_t = 0 up_t = 0 elif n == 1 : val = probe_values [ 0 ] low_t = val up_t = val else : probe_values = np . asarray ( probe_values ) mean = probe_values . mean () s = probe_values . std () low_t = mean + stats . t ( df = n - 1 ). ppf ( fpr ) * s * np . sqrt ( 1 + ( 1 / n )) up_t = mean - stats . t ( df = n - 1 ). ppf ( fpr ) * s * np . sqrt ( 1 + ( 1 / n )) return ProbeStatistic ( low_t , up_t , n ) Classes FeatureClassifier class FeatureClassifier ( probes_low , probes_up , fpr = 0.0001 , verbose = 0 ) View Source class FeatureClassifier : def __init__ ( self , probes_low , probes_up , fpr = 1e-4 , verbose = 0 ) : logging . info ( \"**** Feature Selection ****\" ) logging . info ( \"Generating Lower Probe Statistic\" ) self . lower_stat = create_probe_statistic ( probes_low , fpr , verbose = verbose ) logging . info ( self . lower_stat ) logging . info ( \"Generating Upper Probe Statistic\" ) self . upper_stat = create_probe_statistic ( probes_up , fpr , verbose = verbose ) logging . info ( self . upper_stat ) def classify ( self , relevance_bounds ) : \"\"\" Parameters ---------- relevance_bounds : numpy.ndarray two dimensional array with relevance bounds first column coresponds to minrel and second to maxrel \"\"\" weakly = relevance_bounds [ :, 1 ] > self . upper_stat . upper_threshold strongly = relevance_bounds [ :, 0 ] > self . lower_stat . upper_threshold both = np . logical_and ( weakly , strongly ) prediction = np . zeros ( relevance_bounds . shape [ 0 ] , dtype = np . int ) prediction [ weakly ] = 1 prediction [ both ] = 2 return prediction Methods classify def classify ( self , relevance_bounds ) Parameters relevance_bounds : numpy.ndarray two dimensional array with relevance bounds first column coresponds to minrel and second to maxrel View Source def classify ( self , relevance_bounds ) : \"\"\" Parameters ---------- relevance_bounds : numpy.ndarray two dimensional array with relevance bounds first column coresponds to minrel and second to maxrel \"\"\" weakly = relevance_bounds [ :, 1 ] > self . upper_stat . upper_threshold strongly = relevance_bounds [ :, 0 ] > self . lower_stat . upper_threshold both = np . logical_and ( weakly , strongly ) prediction = np . zeros ( relevance_bounds . shape [ 0 ] , dtype = np . int ) prediction [ weakly ] = 1 prediction [ both ] = 2 return prediction ProbeStatistic class ProbeStatistic ( lower_threshold : float , upper_threshold : float , n_probes : int ) Collects the threshold values about the statistics from one kind of relevance bounds (minrel or maxrel). View Source class ProbeStatistic: \"\"\" Collects the threshold values about the statistics from one kind of relevance bounds (minrel or maxrel). \"\"\" lower_threshold = attr . ib ( type = float ) upper_threshold = attr . ib ( type = float ) n_probes = attr . ib ( type = int ) Class variables lower_threshold n_probes upper_threshold RelevanceBoundsIntervals class RelevanceBoundsIntervals ( data , problem_type : fri . model . base_type . ProblemType , best_init_model : fri . model . base_initmodel . InitModel , random_state , n_resampling , n_jobs , verbose , normalize = True ) View Source class RelevanceBoundsIntervals ( object ) : def __init__ ( self , data , problem_type : ProblemType , best_init_model : InitModel , random_state , n_resampling , n_jobs , verbose , normalize = True , ) : self . data = data self . problem_type = problem_type self . verbose = verbose self . n_jobs = n_jobs self . n_resampling = n_resampling self . random_state = random_state self . best_init_model = best_init_model self . best_hyperparameters = best_init_model . get_params () self . normalize = normalize # Relax constraints to improve stability relaxed_constraints = problem_type . get_relaxed_constraints ( best_init_model . constraints ) self . init_constraints = relaxed_constraints def get_normalized_lupi_intervals ( self , lupi_features , presetModel = None ) : # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO : handle other data formats all_d = X . shape [ 1 ] normal_d = all_d - lupi_features # Compute relevance bounds and probes for normal features and LUPI with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : d_n = _get_necessary_dimensions ( normal_d , presetModel ) rb = self . compute_relevance_bounds ( d_n , parallel = parallel ) probe_upper = self . compute_probe_values ( d_n , True , parallel = parallel ) probe_lower = self . compute_probe_values ( d_n , False , parallel = parallel ) d_l = _get_necessary_dimensions ( all_d , presetModel , start = normal_d ) rb_l = self . compute_relevance_bounds ( d_l , parallel = parallel ) probe_priv_upper = self . compute_probe_values ( d_l , True , parallel = parallel ) probe_priv_lower = self . compute_probe_values ( d_l , False , parallel = parallel ) probes = [ probe_lower, probe_upper, probe_priv_lower, probe_priv_upper ] # # Postprocess # # Get Scaling Parameters l1 = self . init_constraints [ \"w_l1\" ] l1_priv = self . init_constraints [ \"w_priv_l1\" ] l1 = l1 + l1_priv # Normalize Normal and Lupi features rb_norm = self . _postprocessing ( l1 , rb ) rb_l_norm = self . _postprocessing ( l1 , rb_l ) interval_ = np . concatenate ( [ rb_norm, rb_l_norm ] ) # Normalize Probes probe_lower = self . _postprocessing ( l1 , probe_lower ) probe_upper = self . _postprocessing ( l1 , probe_upper ) probe_priv_lower = self . _postprocessing ( l1 , probe_priv_lower ) probe_priv_upper = self . _postprocessing ( l1 , probe_priv_upper ) # # # Classify features self . f_classifier = FeatureClassifier ( probe_lower , probe_upper , verbose = self . verbose ) feature_classes = self . f_classifier . classify ( rb_norm ) self . f_classifier_lupi = FeatureClassifier ( probe_priv_lower , probe_priv_upper , verbose = self . verbose ) feature_classes_lupi = self . f_classifier_lupi . classify ( rb_l_norm ) fc_both = np . concatenate ( [ feature_classes, feature_classes_lupi ] ) return interval_ , fc_both def get_normalized_intervals ( self , presetModel = None ) : # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO : handle other data formats d = X . shape [ 1 ] # Depending on the preset model , we dont need to compute all bounds # e . g . in the case of fixed features we skip those dims = _get_necessary_dimensions ( d , presetModel ) with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : relevance_bounds = self . compute_relevance_bounds ( dims , parallel = parallel , presetModel = presetModel ) probe_values_upper = self . compute_probe_values ( dims , isUpper = True , parallel = parallel , presetModel = presetModel ) probe_values_lower = self . compute_probe_values ( dims , isUpper = False , parallel = parallel , presetModel = presetModel ) # Postprocess bounds norm_bounds = self . _postprocessing ( self . best_init_model . L1_factor , relevance_bounds ) norm_probe_values_upper = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_upper ) norm_probe_values_lower = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_lower ) self . f_classifier = FeatureClassifier ( norm_probe_values_lower , norm_probe_values_upper , verbose = self . verbose ) feature_classes = self . f_classifier . classify ( norm_bounds ) return norm_bounds , feature_classes def compute_relevance_bounds ( self , dims , parallel = None , presetModel = None , solverargs = None ) : init_model_state = self . best_init_model . model_state work_queue = self . _generate_relevance_bounds_tasks ( dims , self . data , presetModel , init_model_state ) # Solve relevance bounds in parallel ( when available ) if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) bound_results = parallel ( map ( joblib . delayed ( _start_solver_worker ), work_queue )) # Retrieve results and aggregate values in dict solved_bounds = defaultdict ( list ) for finished_bound in bound_results : # Only add bounds with feasible solutions if finished_bound . is_solved : solved_bounds [ finished_bound.current_feature ] . append ( finished_bound ) # Initalize array for pair of bounds ( = intervals ) length = len ( dims ) intervals = np . zeros (( length , 2 )) for abs_index , rel_index in zip ( dims , range ( length )) : # Return interval for feature i ( can be a fixed value when set beforehand ) interval_i = self . _create_interval ( abs_index , solved_bounds , presetModel ) intervals [ rel_index ] = interval_i return intervals # TODO : add model model_state ( omega , bias ) to return value def compute_probe_values ( self , dims , isUpper = True , parallel = None , presetModel = None ) : # Get model parameters init_model_state = self . best_init_model . model_state # Prepare parallel framework if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) # Generate probe_queue = self . _generate_probe_value_tasks ( self . data , dims , isUpper , self . n_resampling , self . random_state , presetModel , init_model_state , ) # Compute solution probe_results = parallel ( map ( joblib . delayed ( _start_solver_worker ), probe_queue )) # probe_values . extend ( [ probe.objective.value for probe in probe_results if probe.is_solved ] ) candidates = defaultdict ( list ) for candidate in probe_results : # Only add bounds with feasible solutions if candidate . is_solved : candidates [ candidate.probeID ] . append ( candidate ) probe_values = [] for probes_for_ID in candidates . values () : if isUpper : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( probes_for_ID ) ) else : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( probes_for_ID ) ) return np . array ( probe_values ) def _generate_relevance_bounds_tasks ( self , dims , data , preset_model = None , best_model_state = None ) : # Do not compute bounds for fixed features if preset_model is not None : dims = [ di for di in dims if di not in preset_model ] # Instantiate objects for computation later for di in dims : # Add Lower Bound problem ( s ) to work list yield from self . problem_type . get_cvxproblem_template . generate_lower_bound_problem ( self . best_hyperparameters , self . init_constraints , best_model_state , data , di , preset_model , ) # Add problem ( s ) for Upper bound yield from self . problem_type . get_cvxproblem_template . generate_upper_bound_problem ( self . best_hyperparameters , self . init_constraints , best_model_state , data , di , preset_model , ) def _generate_probe_value_tasks ( self , data , dims , isUpper , n_resampling , random_state , preset_model = None , best_model_state = None , ) : if isUpper : factory = ( self . problem_type . get_cvxproblem_template . generate_upper_bound_problem ) else : factory = ( self . problem_type . get_cvxproblem_template . generate_lower_bound_problem ) # Random sample n_resampling shadow features by permuting real features and computing upper bound random_choice = random_state . choice ( a = dims , size = n_resampling ) # Instantiate objects for i , di in enumerate ( random_choice ) : data_perm = permutate_feature_in_data ( data , di , random_state ) # We only use upper bounds as probe features yield from factory ( self . best_hyperparameters , self . init_constraints , best_model_state , data_perm , di , preset_model , probeID = i , ) def _create_interval ( self , feature : int , solved_bounds : dict , presetModel : dict = None ) : # Return preset values for fixed features if presetModel is not None : if feature in presetModel : return presetModel [ feature ] . squeeze () all_bounds = solved_bounds [ feature ] min_problems_candidates = [ p for p in all_bounds if p.isLowerBound ] max_problems_candidates = [ p for p in all_bounds if not p.isLowerBound ] if len ( all_bounds ) < 2 : logging . error ( f \"(Some) relevance bounds for feature {feature} were not solved.\" ) raise Exception ( \"Infeasible bound(s).\" ) lower_bound = self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( min_problems_candidates ) upper_bound = self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( max_problems_candidates ) return lower_bound , upper_bound def compute_single_preset_relevance_bounds ( self , i : int , signed_preset_i : [ float, float ] ) : \"\"\" Method to run method once for one restricted feature Parameters ---------- i: restricted feature signed_preset_i: restricted range of feature i (set before optimization = preset) \"\"\" preset = { i : signed_preset_i } rangevector = self . compute_multi_preset_relevance_bounds ( preset ) return rangevector def compute_multi_preset_relevance_bounds ( self , preset , lupi_features = 0 ) : \"\"\" Method to run method with preset values Parameters ---------- lupi_features \"\"\" # The user is working with normalized values while we compute them unscaled if self . normalize : normalized = {} for k , v in preset . items () : normalized [ k ] = np . asarray ( v ) * self . best_init_model . L1_factor preset = normalized # Add sign to presets preset = self . _add_sign_to_preset ( preset ) # Calculate all bounds with feature i set to min_i if lupi_features > 0 : rangevector , f_classes = self . get_normalized_lupi_intervals ( lupi_features , presetModel = preset ) else : rangevector , f_classes = self . get_normalized_intervals ( presetModel = preset ) return rangevector def _add_sign_to_preset ( self , unsigned_presets ) : \"\"\" We need signed presets for our convex problem definition later. We reuse the coefficients of the optimal model for this Parameters ---------- unsigned_presets : dict Returns ------- dict \"\"\" signed_presets = {} # Obtain optimal model parameters w = self . best_init_model . model_state [ \"w\" ] sum = 0 for i , preset in unsigned_presets . items () : preset = np . array ( preset ) if preset . size == 1 : preset = np . repeat ( preset , 2 ) unsigned_preset_i = np . sign ( w [ i ] ) * preset # accumulate maximal feature contribution sum += unsigned_preset_i [ 1 ] # Take upper preset signed_presets [ i ] = unsigned_preset_i # Check if unsigned_presets makes sense l1 = self . init_constraints [ \"w_l1\" ] if sum > l1 : print ( \"maximum L1 norm of presets: \" , sum ) print ( \"L1 allowed:\" , l1 ) print ( \"Presets are not feasible. Try lowering values.\" ) return return signed_presets def _postprocessing ( self , L1 , rangevector , round_to_zero = True ) : if self . normalize : assert L1 > 0 rangevector = rangevector . copy () / L1 if round_to_zero : rangevector [ rangevector <= 1e-11 ] = 0 return rangevector Methods compute_multi_preset_relevance_bounds def compute_multi_preset_relevance_bounds ( self , preset , lupi_features = 0 ) Method to run method with preset values Parameters lupi_features View Source def compute_multi_preset_relevance_bounds ( self , preset , lupi_features = 0 ) : \"\"\" Method to run method with preset values Parameters ---------- lupi_features \"\"\" # The user is working with normalized values while we compute them unscaled if self . normalize : normalized = {} for k , v in preset . items () : normalized [ k ] = np . asarray ( v ) * self . best_init_model . L1_factor preset = normalized # Add sign to presets preset = self . _add_sign_to_preset ( preset ) # Calculate all bounds with feature i set to min_i if lupi_features > 0 : rangevector , f_classes = self . get_normalized_lupi_intervals ( lupi_features , presetModel = preset ) else : rangevector , f_classes = self . get_normalized_intervals ( presetModel = preset ) return rangevector compute_probe_values def compute_probe_values ( self , dims , isUpper = True , parallel = None , presetModel = None ) View Source def compute_probe_values ( self , dims , isUpper = True , parallel = None , presetModel = None ): # Get model parameters init_model_state = self . best_init_model . model_state # Prepare parallel framework if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) # Generate probe_queue = self . _generate_probe_value_tasks ( self . data , dims , isUpper , self . n_resampling , self . random_state , presetModel , init_model_state , ) # Compute solution probe_results = parallel ( map ( joblib . delayed ( _start_solver_worker ), probe_queue )) # probe_values . extend ([ probe . objective . value for probe in probe_results if probe . is_solved ]) candidates = defaultdict ( list ) for candidate in probe_results : # Only add bounds with feasible solutions if candidate . is_solved : candidates [ candidate . probeID ]. append ( candidate ) probe_values = [] for probes_for_ID in candidates . values (): if isUpper : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( probes_for_ID ) ) else : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( probes_for_ID ) ) return np . array ( probe_values ) compute_relevance_bounds def compute_relevance_bounds ( self , dims , parallel = None , presetModel = None , solverargs = None ) View Source def compute_relevance_bounds ( self , dims , parallel = None , presetModel = None , solverargs = None ) : init_model_state = self . best_init_model . model_state work_queue = self . _generate_relevance_bounds_tasks ( dims , self . data , presetModel , init_model_state ) # Solve relevance bounds in parallel ( when available ) if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) bound_results = parallel ( map ( joblib . delayed ( _start_solver_worker ), work_queue )) # Retrieve results and aggregate values in dict solved_bounds = defaultdict ( list ) for finished_bound in bound_results : # Only add bounds with feasible solutions if finished_bound . is_solved : solved_bounds [ finished_bound.current_feature ] . append ( finished_bound ) # Initalize array for pair of bounds ( = intervals ) length = len ( dims ) intervals = np . zeros (( length , 2 )) for abs_index , rel_index in zip ( dims , range ( length )) : # Return interval for feature i ( can be a fixed value when set beforehand ) interval_i = self . _create_interval ( abs_index , solved_bounds , presetModel ) intervals [ rel_index ] = interval_i return intervals # TODO : add model model_state ( omega , bias ) to return value compute_single_preset_relevance_bounds def compute_single_preset_relevance_bounds ( self , i : int , signed_preset_i :[ < class ' float '>, <class ' float '>] ) Method to run method once for one restricted feature Parameters i: restricted feature signed_preset_i: restricted range of feature i (set before optimization = preset) View Source def compute_single_preset_relevance_bounds ( self , i : int , signed_preset_i : [ float , float ] ): \"\"\" Method to run method once for one restricted feature Parameters ---------- i: restricted feature signed_preset_i: restricted range of feature i (set before optimization = preset) \"\"\" preset = { i : signed_preset_i } rangevector = self . compute_multi_preset_relevance_bounds ( preset ) return rangevector get_normalized_intervals def get_normalized_intervals ( self , presetModel = None ) View Source def get_normalized_intervals ( self , presetModel = None ): # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO : handle other data formats d = X . shape [ 1 ] # Depending on the preset model , we dont need to compute all bounds # e . g . in the case of fixed features we skip those dims = _get_necessary_dimensions ( d , presetModel ) with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : relevance_bounds = self . compute_relevance_bounds ( dims , parallel = parallel , presetModel = presetModel ) probe_values_upper = self . compute_probe_values ( dims , isUpper = True , parallel = parallel , presetModel = presetModel ) probe_values_lower = self . compute_probe_values ( dims , isUpper = False , parallel = parallel , presetModel = presetModel ) # Postprocess bounds norm_bounds = self . _postprocessing ( self . best_init_model . L1_factor , relevance_bounds ) norm_probe_values_upper = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_upper ) norm_probe_values_lower = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_lower ) self . f_classifier = FeatureClassifier ( norm_probe_values_lower , norm_probe_values_upper , verbose = self . verbose ) feature_classes = self . f_classifier . classify ( norm_bounds ) return norm_bounds , feature_classes get_normalized_lupi_intervals def get_normalized_lupi_intervals ( self , lupi_features , presetModel = None ) View Source def get_normalized_lupi_intervals ( self , lupi_features , presetModel = None ): # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO : handle other data formats all_d = X . shape [ 1 ] normal_d = all_d - lupi_features # Compute relevance bounds and probes for normal features and LUPI with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : d_n = _get_necessary_dimensions ( normal_d , presetModel ) rb = self . compute_relevance_bounds ( d_n , parallel = parallel ) probe_upper = self . compute_probe_values ( d_n , True , parallel = parallel ) probe_lower = self . compute_probe_values ( d_n , False , parallel = parallel ) d_l = _get_necessary_dimensions ( all_d , presetModel , start = normal_d ) rb_l = self . compute_relevance_bounds ( d_l , parallel = parallel ) probe_priv_upper = self . compute_probe_values ( d_l , True , parallel = parallel ) probe_priv_lower = self . compute_probe_values ( d_l , False , parallel = parallel ) probes = [ probe_lower , probe_upper , probe_priv_lower , probe_priv_upper ] # # Postprocess # # Get Scaling Parameters l1 = self . init_constraints [ \"w_l1\" ] l1_priv = self . init_constraints [ \"w_priv_l1\" ] l1 = l1 + l1_priv # Normalize Normal and Lupi features rb_norm = self . _postprocessing ( l1 , rb ) rb_l_norm = self . _postprocessing ( l1 , rb_l ) interval_ = np . concatenate ([ rb_norm , rb_l_norm ]) # Normalize Probes probe_lower = self . _postprocessing ( l1 , probe_lower ) probe_upper = self . _postprocessing ( l1 , probe_upper ) probe_priv_lower = self . _postprocessing ( l1 , probe_priv_lower ) probe_priv_upper = self . _postprocessing ( l1 , probe_priv_upper ) # # # Classify features self . f_classifier = FeatureClassifier ( probe_lower , probe_upper , verbose = self . verbose ) feature_classes = self . f_classifier . classify ( rb_norm ) self . f_classifier_lupi = FeatureClassifier ( probe_priv_lower , probe_priv_upper , verbose = self . verbose ) feature_classes_lupi = self . f_classifier_lupi . classify ( rb_l_norm ) fc_both = np . concatenate ([ feature_classes , feature_classes_lupi ]) return interval_ , fc_both","title":"Compute"},{"location":"reference/fri/compute/#module-fricompute","text":"This module includes all important computation functions which are used internally. They (normally) should not be used by users. View Source \"\"\"This module includes all important computation functions which are used internally. They (normally) should not be used by users. \"\"\" import logging from collections import defaultdict import attr import joblib import numpy as np from scipy import stats from fri.model.base_cvxproblem import Relevance_CVXProblem from fri.model.base_initmodel import InitModel from fri.model.base_type import ProblemType from fri.utils import permutate_feature_in_data MIN_N_PROBE_FEATURES = 20 # Lower bound of probe features def _start_solver_worker ( bound : Relevance_CVXProblem ): \"\"\" Worker thread method for parallel computation \"\"\" return bound . solve () class RelevanceBoundsIntervals ( object ): def __init__ ( self , data , problem_type : ProblemType , best_init_model : InitModel , random_state , n_resampling , n_jobs , verbose , normalize = True , ): self . data = data self . problem_type = problem_type self . verbose = verbose self . n_jobs = n_jobs self . n_resampling = n_resampling self . random_state = random_state self . best_init_model = best_init_model self . best_hyperparameters = best_init_model . get_params () self . normalize = normalize # Relax constraints to improve stability relaxed_constraints = problem_type . get_relaxed_constraints ( best_init_model . constraints ) self . init_constraints = relaxed_constraints def get_normalized_lupi_intervals ( self , lupi_features , presetModel = None ): # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO: handle other data formats all_d = X . shape [ 1 ] normal_d = all_d - lupi_features # Compute relevance bounds and probes for normal features and LUPI with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : d_n = _get_necessary_dimensions ( normal_d , presetModel ) rb = self . compute_relevance_bounds ( d_n , parallel = parallel ) probe_upper = self . compute_probe_values ( d_n , True , parallel = parallel ) probe_lower = self . compute_probe_values ( d_n , False , parallel = parallel ) d_l = _get_necessary_dimensions ( all_d , presetModel , start = normal_d ) rb_l = self . compute_relevance_bounds ( d_l , parallel = parallel ) probe_priv_upper = self . compute_probe_values ( d_l , True , parallel = parallel ) probe_priv_lower = self . compute_probe_values ( d_l , False , parallel = parallel ) probes = [ probe_lower , probe_upper , probe_priv_lower , probe_priv_upper ] # # Postprocess # # Get Scaling Parameters l1 = self . init_constraints [ \"w_l1\" ] l1_priv = self . init_constraints [ \"w_priv_l1\" ] l1 = l1 + l1_priv # Normalize Normal and Lupi features rb_norm = self . _postprocessing ( l1 , rb ) rb_l_norm = self . _postprocessing ( l1 , rb_l ) interval_ = np . concatenate ([ rb_norm , rb_l_norm ]) # Normalize Probes probe_lower = self . _postprocessing ( l1 , probe_lower ) probe_upper = self . _postprocessing ( l1 , probe_upper ) probe_priv_lower = self . _postprocessing ( l1 , probe_priv_lower ) probe_priv_upper = self . _postprocessing ( l1 , probe_priv_upper ) # # # Classify features self . f_classifier = FeatureClassifier ( probe_lower , probe_upper , verbose = self . verbose ) feature_classes = self . f_classifier . classify ( rb_norm ) self . f_classifier_lupi = FeatureClassifier ( probe_priv_lower , probe_priv_upper , verbose = self . verbose ) feature_classes_lupi = self . f_classifier_lupi . classify ( rb_l_norm ) fc_both = np . concatenate ([ feature_classes , feature_classes_lupi ]) return interval_ , fc_both def get_normalized_intervals ( self , presetModel = None ): # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO: handle other data formats d = X . shape [ 1 ] # Depending on the preset model, we dont need to compute all bounds # e.g. in the case of fixed features we skip those dims = _get_necessary_dimensions ( d , presetModel ) with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : relevance_bounds = self . compute_relevance_bounds ( dims , parallel = parallel , presetModel = presetModel ) probe_values_upper = self . compute_probe_values ( dims , isUpper = True , parallel = parallel , presetModel = presetModel ) probe_values_lower = self . compute_probe_values ( dims , isUpper = False , parallel = parallel , presetModel = presetModel ) # Postprocess bounds norm_bounds = self . _postprocessing ( self . best_init_model . L1_factor , relevance_bounds ) norm_probe_values_upper = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_upper ) norm_probe_values_lower = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_lower ) self . f_classifier = FeatureClassifier ( norm_probe_values_lower , norm_probe_values_upper , verbose = self . verbose ) feature_classes = self . f_classifier . classify ( norm_bounds ) return norm_bounds , feature_classes def compute_relevance_bounds ( self , dims , parallel = None , presetModel = None , solverargs = None ): init_model_state = self . best_init_model . model_state work_queue = self . _generate_relevance_bounds_tasks ( dims , self . data , presetModel , init_model_state ) # Solve relevance bounds in parallel (when available) if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) bound_results = parallel ( map ( joblib . delayed ( _start_solver_worker ), work_queue )) # Retrieve results and aggregate values in dict solved_bounds = defaultdict ( list ) for finished_bound in bound_results : # Only add bounds with feasible solutions if finished_bound . is_solved : solved_bounds [ finished_bound . current_feature ] . append ( finished_bound ) # Initalize array for pair of bounds(= intervals) length = len ( dims ) intervals = np . zeros (( length , 2 )) for abs_index , rel_index in zip ( dims , range ( length )): # Return interval for feature i (can be a fixed value when set beforehand) interval_i = self . _create_interval ( abs_index , solved_bounds , presetModel ) intervals [ rel_index ] = interval_i return intervals # TODO: add model model_state (omega, bias) to return value def compute_probe_values ( self , dims , isUpper = True , parallel = None , presetModel = None ): # Get model parameters init_model_state = self . best_init_model . model_state # Prepare parallel framework if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) # Generate probe_queue = self . _generate_probe_value_tasks ( self . data , dims , isUpper , self . n_resampling , self . random_state , presetModel , init_model_state , ) # Compute solution probe_results = parallel ( map ( joblib . delayed ( _start_solver_worker ), probe_queue )) # probe_values.extend([probe.objective.value for probe in probe_results if probe.is_solved]) candidates = defaultdict ( list ) for candidate in probe_results : # Only add bounds with feasible solutions if candidate . is_solved : candidates [ candidate . probeID ] . append ( candidate ) probe_values = [] for probes_for_ID in candidates . values (): if isUpper : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( probes_for_ID ) ) else : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( probes_for_ID ) ) return np . array ( probe_values ) def _generate_relevance_bounds_tasks ( self , dims , data , preset_model = None , best_model_state = None ): # Do not compute bounds for fixed features if preset_model is not None : dims = [ di for di in dims if di not in preset_model ] # Instantiate objects for computation later for di in dims : # Add Lower Bound problem(s) to work list yield from self . problem_type . get_cvxproblem_template . generate_lower_bound_problem ( self . best_hyperparameters , self . init_constraints , best_model_state , data , di , preset_model , ) # Add problem(s) for Upper bound yield from self . problem_type . get_cvxproblem_template . generate_upper_bound_problem ( self . best_hyperparameters , self . init_constraints , best_model_state , data , di , preset_model , ) def _generate_probe_value_tasks ( self , data , dims , isUpper , n_resampling , random_state , preset_model = None , best_model_state = None , ): if isUpper : factory = ( self . problem_type . get_cvxproblem_template . generate_upper_bound_problem ) else : factory = ( self . problem_type . get_cvxproblem_template . generate_lower_bound_problem ) # Random sample n_resampling shadow features by permuting real features and computing upper bound random_choice = random_state . choice ( a = dims , size = n_resampling ) # Instantiate objects for i , di in enumerate ( random_choice ): data_perm = permutate_feature_in_data ( data , di , random_state ) # We only use upper bounds as probe features yield from factory ( self . best_hyperparameters , self . init_constraints , best_model_state , data_perm , di , preset_model , probeID = i , ) def _create_interval ( self , feature : int , solved_bounds : dict , presetModel : dict = None ): # Return preset values for fixed features if presetModel is not None : if feature in presetModel : return presetModel [ feature ] . squeeze () all_bounds = solved_bounds [ feature ] min_problems_candidates = [ p for p in all_bounds if p . isLowerBound ] max_problems_candidates = [ p for p in all_bounds if not p . isLowerBound ] if len ( all_bounds ) < 2 : logging . error ( f \"(Some) relevance bounds for feature {feature} were not solved.\" ) raise Exception ( \"Infeasible bound(s).\" ) lower_bound = self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( min_problems_candidates ) upper_bound = self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( max_problems_candidates ) return lower_bound , upper_bound def compute_single_preset_relevance_bounds ( self , i : int , signed_preset_i : [ float , float ] ): \"\"\" Method to run method once for one restricted feature Parameters ---------- i: restricted feature signed_preset_i: restricted range of feature i (set before optimization = preset) \"\"\" preset = { i : signed_preset_i } rangevector = self . compute_multi_preset_relevance_bounds ( preset ) return rangevector def compute_multi_preset_relevance_bounds ( self , preset , lupi_features = 0 ): \"\"\" Method to run method with preset values Parameters ---------- lupi_features \"\"\" # The user is working with normalized values while we compute them unscaled if self . normalize : normalized = {} for k , v in preset . items (): normalized [ k ] = np . asarray ( v ) * self . best_init_model . L1_factor preset = normalized # Add sign to presets preset = self . _add_sign_to_preset ( preset ) # Calculate all bounds with feature i set to min_i if lupi_features > 0 : rangevector , f_classes = self . get_normalized_lupi_intervals ( lupi_features , presetModel = preset ) else : rangevector , f_classes = self . get_normalized_intervals ( presetModel = preset ) return rangevector def _add_sign_to_preset ( self , unsigned_presets ): \"\"\" We need signed presets for our convex problem definition later. We reuse the coefficients of the optimal model for this Parameters ---------- unsigned_presets : dict Returns ------- dict \"\"\" signed_presets = {} # Obtain optimal model parameters w = self . best_init_model . model_state [ \"w\" ] sum = 0 for i , preset in unsigned_presets . items (): preset = np . array ( preset ) if preset . size == 1 : preset = np . repeat ( preset , 2 ) unsigned_preset_i = np . sign ( w [ i ]) * preset # accumulate maximal feature contribution sum += unsigned_preset_i [ 1 ] # Take upper preset signed_presets [ i ] = unsigned_preset_i # Check if unsigned_presets makes sense l1 = self . init_constraints [ \"w_l1\" ] if sum > l1 : print ( \"maximum L1 norm of presets: \" , sum ) print ( \"L1 allowed:\" , l1 ) print ( \"Presets are not feasible. Try lowering values.\" ) return return signed_presets def _postprocessing ( self , L1 , rangevector , round_to_zero = True ): if self . normalize : assert L1 > 0 rangevector = rangevector . copy () / L1 if round_to_zero : rangevector [ rangevector <= 1e-11 ] = 0 return rangevector def _get_necessary_dimensions ( d : int , presetModel : dict = None , start = 0 ): dims = np . arange ( start , d ) # if presetModel is not None: # # Exclude fixed (preset) dimensions from being redundantly computed # dims = [di for di in dims if di not in presetModel.keys()] # TODO: check the removal of this block return dims class FeatureClassifier : def __init__ ( self , probes_low , probes_up , fpr = 1e-4 , verbose = 0 ): logging . info ( \"**** Feature Selection ****\" ) logging . info ( \"Generating Lower Probe Statistic\" ) self . lower_stat = create_probe_statistic ( probes_low , fpr , verbose = verbose ) logging . info ( self . lower_stat ) logging . info ( \"Generating Upper Probe Statistic\" ) self . upper_stat = create_probe_statistic ( probes_up , fpr , verbose = verbose ) logging . info ( self . upper_stat ) def classify ( self , relevance_bounds ): \"\"\" Parameters ---------- relevance_bounds : numpy.ndarray two dimensional array with relevance bounds first column coresponds to minrel and second to maxrel \"\"\" weakly = relevance_bounds [:, 1 ] > self . upper_stat . upper_threshold strongly = relevance_bounds [:, 0 ] > self . lower_stat . upper_threshold both = np . logical_and ( weakly , strongly ) prediction = np . zeros ( relevance_bounds . shape [ 0 ], dtype = np . int ) prediction [ weakly ] = 1 prediction [ both ] = 2 return prediction @attr . s class ProbeStatistic : \"\"\" Collects the threshold values about the statistics from one kind of relevance bounds (minrel or maxrel). \"\"\" lower_threshold = attr . ib ( type = float ) upper_threshold = attr . ib ( type = float ) n_probes = attr . ib ( type = int ) def create_probe_statistic ( probe_values , fpr , verbose = 0 ): # Create prediction interval statistics based on randomly permutated probe features (based on real features) n = len ( probe_values ) if n == 0 : if verbose > 0 : logging . info ( \"All probes were infeasible. All features considered relevant.\" ) # # If all probes were infeasible we expect an empty list # # If they are infeasible it also means that only strongly relevant features were in the data # # As such we just set the prediction without considering the statistics low_t = 0 up_t = 0 elif n == 1 : val = probe_values [ 0 ] low_t = val up_t = val else : probe_values = np . asarray ( probe_values ) mean = probe_values . mean () s = probe_values . std () low_t = mean + stats . t ( df = n - 1 ) . ppf ( fpr ) * s * np . sqrt ( 1 + ( 1 / n )) up_t = mean - stats . t ( df = n - 1 ) . ppf ( fpr ) * s * np . sqrt ( 1 + ( 1 / n )) return ProbeStatistic ( low_t , up_t , n )","title":"Module fri.compute"},{"location":"reference/fri/compute/#variables","text":"MIN_N_PROBE_FEATURES","title":"Variables"},{"location":"reference/fri/compute/#functions","text":"","title":"Functions"},{"location":"reference/fri/compute/#create_probe_statistic","text":"def create_probe_statistic ( probe_values , fpr , verbose = 0 ) View Source def create_probe_statistic ( probe_values , fpr , verbose = 0 ): # Create prediction interval statistics based on randomly permutated probe features ( based on real features ) n = len ( probe_values ) if n == 0 : if verbose > 0 : logging . info ( \"All probes were infeasible. All features considered relevant.\" ) # # If all probes were infeasible we expect an empty list # # If they are infeasible it also means that only strongly relevant features were in the data # # As such we just set the prediction without considering the statistics low_t = 0 up_t = 0 elif n == 1 : val = probe_values [ 0 ] low_t = val up_t = val else : probe_values = np . asarray ( probe_values ) mean = probe_values . mean () s = probe_values . std () low_t = mean + stats . t ( df = n - 1 ). ppf ( fpr ) * s * np . sqrt ( 1 + ( 1 / n )) up_t = mean - stats . t ( df = n - 1 ). ppf ( fpr ) * s * np . sqrt ( 1 + ( 1 / n )) return ProbeStatistic ( low_t , up_t , n )","title":"create_probe_statistic"},{"location":"reference/fri/compute/#classes","text":"","title":"Classes"},{"location":"reference/fri/compute/#featureclassifier","text":"class FeatureClassifier ( probes_low , probes_up , fpr = 0.0001 , verbose = 0 ) View Source class FeatureClassifier : def __init__ ( self , probes_low , probes_up , fpr = 1e-4 , verbose = 0 ) : logging . info ( \"**** Feature Selection ****\" ) logging . info ( \"Generating Lower Probe Statistic\" ) self . lower_stat = create_probe_statistic ( probes_low , fpr , verbose = verbose ) logging . info ( self . lower_stat ) logging . info ( \"Generating Upper Probe Statistic\" ) self . upper_stat = create_probe_statistic ( probes_up , fpr , verbose = verbose ) logging . info ( self . upper_stat ) def classify ( self , relevance_bounds ) : \"\"\" Parameters ---------- relevance_bounds : numpy.ndarray two dimensional array with relevance bounds first column coresponds to minrel and second to maxrel \"\"\" weakly = relevance_bounds [ :, 1 ] > self . upper_stat . upper_threshold strongly = relevance_bounds [ :, 0 ] > self . lower_stat . upper_threshold both = np . logical_and ( weakly , strongly ) prediction = np . zeros ( relevance_bounds . shape [ 0 ] , dtype = np . int ) prediction [ weakly ] = 1 prediction [ both ] = 2 return prediction","title":"FeatureClassifier"},{"location":"reference/fri/compute/#methods","text":"","title":"Methods"},{"location":"reference/fri/compute/#classify","text":"def classify ( self , relevance_bounds )","title":"classify"},{"location":"reference/fri/compute/#parameters","text":"relevance_bounds : numpy.ndarray two dimensional array with relevance bounds first column coresponds to minrel and second to maxrel View Source def classify ( self , relevance_bounds ) : \"\"\" Parameters ---------- relevance_bounds : numpy.ndarray two dimensional array with relevance bounds first column coresponds to minrel and second to maxrel \"\"\" weakly = relevance_bounds [ :, 1 ] > self . upper_stat . upper_threshold strongly = relevance_bounds [ :, 0 ] > self . lower_stat . upper_threshold both = np . logical_and ( weakly , strongly ) prediction = np . zeros ( relevance_bounds . shape [ 0 ] , dtype = np . int ) prediction [ weakly ] = 1 prediction [ both ] = 2 return prediction","title":"Parameters"},{"location":"reference/fri/compute/#probestatistic","text":"class ProbeStatistic ( lower_threshold : float , upper_threshold : float , n_probes : int ) Collects the threshold values about the statistics from one kind of relevance bounds (minrel or maxrel). View Source class ProbeStatistic: \"\"\" Collects the threshold values about the statistics from one kind of relevance bounds (minrel or maxrel). \"\"\" lower_threshold = attr . ib ( type = float ) upper_threshold = attr . ib ( type = float ) n_probes = attr . ib ( type = int )","title":"ProbeStatistic"},{"location":"reference/fri/compute/#class-variables","text":"lower_threshold n_probes upper_threshold","title":"Class variables"},{"location":"reference/fri/compute/#relevanceboundsintervals","text":"class RelevanceBoundsIntervals ( data , problem_type : fri . model . base_type . ProblemType , best_init_model : fri . model . base_initmodel . InitModel , random_state , n_resampling , n_jobs , verbose , normalize = True ) View Source class RelevanceBoundsIntervals ( object ) : def __init__ ( self , data , problem_type : ProblemType , best_init_model : InitModel , random_state , n_resampling , n_jobs , verbose , normalize = True , ) : self . data = data self . problem_type = problem_type self . verbose = verbose self . n_jobs = n_jobs self . n_resampling = n_resampling self . random_state = random_state self . best_init_model = best_init_model self . best_hyperparameters = best_init_model . get_params () self . normalize = normalize # Relax constraints to improve stability relaxed_constraints = problem_type . get_relaxed_constraints ( best_init_model . constraints ) self . init_constraints = relaxed_constraints def get_normalized_lupi_intervals ( self , lupi_features , presetModel = None ) : # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO : handle other data formats all_d = X . shape [ 1 ] normal_d = all_d - lupi_features # Compute relevance bounds and probes for normal features and LUPI with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : d_n = _get_necessary_dimensions ( normal_d , presetModel ) rb = self . compute_relevance_bounds ( d_n , parallel = parallel ) probe_upper = self . compute_probe_values ( d_n , True , parallel = parallel ) probe_lower = self . compute_probe_values ( d_n , False , parallel = parallel ) d_l = _get_necessary_dimensions ( all_d , presetModel , start = normal_d ) rb_l = self . compute_relevance_bounds ( d_l , parallel = parallel ) probe_priv_upper = self . compute_probe_values ( d_l , True , parallel = parallel ) probe_priv_lower = self . compute_probe_values ( d_l , False , parallel = parallel ) probes = [ probe_lower, probe_upper, probe_priv_lower, probe_priv_upper ] # # Postprocess # # Get Scaling Parameters l1 = self . init_constraints [ \"w_l1\" ] l1_priv = self . init_constraints [ \"w_priv_l1\" ] l1 = l1 + l1_priv # Normalize Normal and Lupi features rb_norm = self . _postprocessing ( l1 , rb ) rb_l_norm = self . _postprocessing ( l1 , rb_l ) interval_ = np . concatenate ( [ rb_norm, rb_l_norm ] ) # Normalize Probes probe_lower = self . _postprocessing ( l1 , probe_lower ) probe_upper = self . _postprocessing ( l1 , probe_upper ) probe_priv_lower = self . _postprocessing ( l1 , probe_priv_lower ) probe_priv_upper = self . _postprocessing ( l1 , probe_priv_upper ) # # # Classify features self . f_classifier = FeatureClassifier ( probe_lower , probe_upper , verbose = self . verbose ) feature_classes = self . f_classifier . classify ( rb_norm ) self . f_classifier_lupi = FeatureClassifier ( probe_priv_lower , probe_priv_upper , verbose = self . verbose ) feature_classes_lupi = self . f_classifier_lupi . classify ( rb_l_norm ) fc_both = np . concatenate ( [ feature_classes, feature_classes_lupi ] ) return interval_ , fc_both def get_normalized_intervals ( self , presetModel = None ) : # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO : handle other data formats d = X . shape [ 1 ] # Depending on the preset model , we dont need to compute all bounds # e . g . in the case of fixed features we skip those dims = _get_necessary_dimensions ( d , presetModel ) with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : relevance_bounds = self . compute_relevance_bounds ( dims , parallel = parallel , presetModel = presetModel ) probe_values_upper = self . compute_probe_values ( dims , isUpper = True , parallel = parallel , presetModel = presetModel ) probe_values_lower = self . compute_probe_values ( dims , isUpper = False , parallel = parallel , presetModel = presetModel ) # Postprocess bounds norm_bounds = self . _postprocessing ( self . best_init_model . L1_factor , relevance_bounds ) norm_probe_values_upper = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_upper ) norm_probe_values_lower = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_lower ) self . f_classifier = FeatureClassifier ( norm_probe_values_lower , norm_probe_values_upper , verbose = self . verbose ) feature_classes = self . f_classifier . classify ( norm_bounds ) return norm_bounds , feature_classes def compute_relevance_bounds ( self , dims , parallel = None , presetModel = None , solverargs = None ) : init_model_state = self . best_init_model . model_state work_queue = self . _generate_relevance_bounds_tasks ( dims , self . data , presetModel , init_model_state ) # Solve relevance bounds in parallel ( when available ) if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) bound_results = parallel ( map ( joblib . delayed ( _start_solver_worker ), work_queue )) # Retrieve results and aggregate values in dict solved_bounds = defaultdict ( list ) for finished_bound in bound_results : # Only add bounds with feasible solutions if finished_bound . is_solved : solved_bounds [ finished_bound.current_feature ] . append ( finished_bound ) # Initalize array for pair of bounds ( = intervals ) length = len ( dims ) intervals = np . zeros (( length , 2 )) for abs_index , rel_index in zip ( dims , range ( length )) : # Return interval for feature i ( can be a fixed value when set beforehand ) interval_i = self . _create_interval ( abs_index , solved_bounds , presetModel ) intervals [ rel_index ] = interval_i return intervals # TODO : add model model_state ( omega , bias ) to return value def compute_probe_values ( self , dims , isUpper = True , parallel = None , presetModel = None ) : # Get model parameters init_model_state = self . best_init_model . model_state # Prepare parallel framework if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) # Generate probe_queue = self . _generate_probe_value_tasks ( self . data , dims , isUpper , self . n_resampling , self . random_state , presetModel , init_model_state , ) # Compute solution probe_results = parallel ( map ( joblib . delayed ( _start_solver_worker ), probe_queue )) # probe_values . extend ( [ probe.objective.value for probe in probe_results if probe.is_solved ] ) candidates = defaultdict ( list ) for candidate in probe_results : # Only add bounds with feasible solutions if candidate . is_solved : candidates [ candidate.probeID ] . append ( candidate ) probe_values = [] for probes_for_ID in candidates . values () : if isUpper : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( probes_for_ID ) ) else : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( probes_for_ID ) ) return np . array ( probe_values ) def _generate_relevance_bounds_tasks ( self , dims , data , preset_model = None , best_model_state = None ) : # Do not compute bounds for fixed features if preset_model is not None : dims = [ di for di in dims if di not in preset_model ] # Instantiate objects for computation later for di in dims : # Add Lower Bound problem ( s ) to work list yield from self . problem_type . get_cvxproblem_template . generate_lower_bound_problem ( self . best_hyperparameters , self . init_constraints , best_model_state , data , di , preset_model , ) # Add problem ( s ) for Upper bound yield from self . problem_type . get_cvxproblem_template . generate_upper_bound_problem ( self . best_hyperparameters , self . init_constraints , best_model_state , data , di , preset_model , ) def _generate_probe_value_tasks ( self , data , dims , isUpper , n_resampling , random_state , preset_model = None , best_model_state = None , ) : if isUpper : factory = ( self . problem_type . get_cvxproblem_template . generate_upper_bound_problem ) else : factory = ( self . problem_type . get_cvxproblem_template . generate_lower_bound_problem ) # Random sample n_resampling shadow features by permuting real features and computing upper bound random_choice = random_state . choice ( a = dims , size = n_resampling ) # Instantiate objects for i , di in enumerate ( random_choice ) : data_perm = permutate_feature_in_data ( data , di , random_state ) # We only use upper bounds as probe features yield from factory ( self . best_hyperparameters , self . init_constraints , best_model_state , data_perm , di , preset_model , probeID = i , ) def _create_interval ( self , feature : int , solved_bounds : dict , presetModel : dict = None ) : # Return preset values for fixed features if presetModel is not None : if feature in presetModel : return presetModel [ feature ] . squeeze () all_bounds = solved_bounds [ feature ] min_problems_candidates = [ p for p in all_bounds if p.isLowerBound ] max_problems_candidates = [ p for p in all_bounds if not p.isLowerBound ] if len ( all_bounds ) < 2 : logging . error ( f \"(Some) relevance bounds for feature {feature} were not solved.\" ) raise Exception ( \"Infeasible bound(s).\" ) lower_bound = self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( min_problems_candidates ) upper_bound = self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( max_problems_candidates ) return lower_bound , upper_bound def compute_single_preset_relevance_bounds ( self , i : int , signed_preset_i : [ float, float ] ) : \"\"\" Method to run method once for one restricted feature Parameters ---------- i: restricted feature signed_preset_i: restricted range of feature i (set before optimization = preset) \"\"\" preset = { i : signed_preset_i } rangevector = self . compute_multi_preset_relevance_bounds ( preset ) return rangevector def compute_multi_preset_relevance_bounds ( self , preset , lupi_features = 0 ) : \"\"\" Method to run method with preset values Parameters ---------- lupi_features \"\"\" # The user is working with normalized values while we compute them unscaled if self . normalize : normalized = {} for k , v in preset . items () : normalized [ k ] = np . asarray ( v ) * self . best_init_model . L1_factor preset = normalized # Add sign to presets preset = self . _add_sign_to_preset ( preset ) # Calculate all bounds with feature i set to min_i if lupi_features > 0 : rangevector , f_classes = self . get_normalized_lupi_intervals ( lupi_features , presetModel = preset ) else : rangevector , f_classes = self . get_normalized_intervals ( presetModel = preset ) return rangevector def _add_sign_to_preset ( self , unsigned_presets ) : \"\"\" We need signed presets for our convex problem definition later. We reuse the coefficients of the optimal model for this Parameters ---------- unsigned_presets : dict Returns ------- dict \"\"\" signed_presets = {} # Obtain optimal model parameters w = self . best_init_model . model_state [ \"w\" ] sum = 0 for i , preset in unsigned_presets . items () : preset = np . array ( preset ) if preset . size == 1 : preset = np . repeat ( preset , 2 ) unsigned_preset_i = np . sign ( w [ i ] ) * preset # accumulate maximal feature contribution sum += unsigned_preset_i [ 1 ] # Take upper preset signed_presets [ i ] = unsigned_preset_i # Check if unsigned_presets makes sense l1 = self . init_constraints [ \"w_l1\" ] if sum > l1 : print ( \"maximum L1 norm of presets: \" , sum ) print ( \"L1 allowed:\" , l1 ) print ( \"Presets are not feasible. Try lowering values.\" ) return return signed_presets def _postprocessing ( self , L1 , rangevector , round_to_zero = True ) : if self . normalize : assert L1 > 0 rangevector = rangevector . copy () / L1 if round_to_zero : rangevector [ rangevector <= 1e-11 ] = 0 return rangevector","title":"RelevanceBoundsIntervals"},{"location":"reference/fri/compute/#methods_1","text":"","title":"Methods"},{"location":"reference/fri/compute/#compute_multi_preset_relevance_bounds","text":"def compute_multi_preset_relevance_bounds ( self , preset , lupi_features = 0 ) Method to run method with preset values","title":"compute_multi_preset_relevance_bounds"},{"location":"reference/fri/compute/#parameters_1","text":"lupi_features View Source def compute_multi_preset_relevance_bounds ( self , preset , lupi_features = 0 ) : \"\"\" Method to run method with preset values Parameters ---------- lupi_features \"\"\" # The user is working with normalized values while we compute them unscaled if self . normalize : normalized = {} for k , v in preset . items () : normalized [ k ] = np . asarray ( v ) * self . best_init_model . L1_factor preset = normalized # Add sign to presets preset = self . _add_sign_to_preset ( preset ) # Calculate all bounds with feature i set to min_i if lupi_features > 0 : rangevector , f_classes = self . get_normalized_lupi_intervals ( lupi_features , presetModel = preset ) else : rangevector , f_classes = self . get_normalized_intervals ( presetModel = preset ) return rangevector","title":"Parameters"},{"location":"reference/fri/compute/#compute_probe_values","text":"def compute_probe_values ( self , dims , isUpper = True , parallel = None , presetModel = None ) View Source def compute_probe_values ( self , dims , isUpper = True , parallel = None , presetModel = None ): # Get model parameters init_model_state = self . best_init_model . model_state # Prepare parallel framework if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) # Generate probe_queue = self . _generate_probe_value_tasks ( self . data , dims , isUpper , self . n_resampling , self . random_state , presetModel , init_model_state , ) # Compute solution probe_results = parallel ( map ( joblib . delayed ( _start_solver_worker ), probe_queue )) # probe_values . extend ([ probe . objective . value for probe in probe_results if probe . is_solved ]) candidates = defaultdict ( list ) for candidate in probe_results : # Only add bounds with feasible solutions if candidate . is_solved : candidates [ candidate . probeID ]. append ( candidate ) probe_values = [] for probes_for_ID in candidates . values (): if isUpper : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( probes_for_ID ) ) else : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( probes_for_ID ) ) return np . array ( probe_values )","title":"compute_probe_values"},{"location":"reference/fri/compute/#compute_relevance_bounds","text":"def compute_relevance_bounds ( self , dims , parallel = None , presetModel = None , solverargs = None ) View Source def compute_relevance_bounds ( self , dims , parallel = None , presetModel = None , solverargs = None ) : init_model_state = self . best_init_model . model_state work_queue = self . _generate_relevance_bounds_tasks ( dims , self . data , presetModel , init_model_state ) # Solve relevance bounds in parallel ( when available ) if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) bound_results = parallel ( map ( joblib . delayed ( _start_solver_worker ), work_queue )) # Retrieve results and aggregate values in dict solved_bounds = defaultdict ( list ) for finished_bound in bound_results : # Only add bounds with feasible solutions if finished_bound . is_solved : solved_bounds [ finished_bound.current_feature ] . append ( finished_bound ) # Initalize array for pair of bounds ( = intervals ) length = len ( dims ) intervals = np . zeros (( length , 2 )) for abs_index , rel_index in zip ( dims , range ( length )) : # Return interval for feature i ( can be a fixed value when set beforehand ) interval_i = self . _create_interval ( abs_index , solved_bounds , presetModel ) intervals [ rel_index ] = interval_i return intervals # TODO : add model model_state ( omega , bias ) to return value","title":"compute_relevance_bounds"},{"location":"reference/fri/compute/#compute_single_preset_relevance_bounds","text":"def compute_single_preset_relevance_bounds ( self , i : int , signed_preset_i :[ < class ' float '>, <class ' float '>] ) Method to run method once for one restricted feature Parameters i: restricted feature signed_preset_i: restricted range of feature i (set before optimization = preset) View Source def compute_single_preset_relevance_bounds ( self , i : int , signed_preset_i : [ float , float ] ): \"\"\" Method to run method once for one restricted feature Parameters ---------- i: restricted feature signed_preset_i: restricted range of feature i (set before optimization = preset) \"\"\" preset = { i : signed_preset_i } rangevector = self . compute_multi_preset_relevance_bounds ( preset ) return rangevector","title":"compute_single_preset_relevance_bounds"},{"location":"reference/fri/compute/#get_normalized_intervals","text":"def get_normalized_intervals ( self , presetModel = None ) View Source def get_normalized_intervals ( self , presetModel = None ): # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO : handle other data formats d = X . shape [ 1 ] # Depending on the preset model , we dont need to compute all bounds # e . g . in the case of fixed features we skip those dims = _get_necessary_dimensions ( d , presetModel ) with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : relevance_bounds = self . compute_relevance_bounds ( dims , parallel = parallel , presetModel = presetModel ) probe_values_upper = self . compute_probe_values ( dims , isUpper = True , parallel = parallel , presetModel = presetModel ) probe_values_lower = self . compute_probe_values ( dims , isUpper = False , parallel = parallel , presetModel = presetModel ) # Postprocess bounds norm_bounds = self . _postprocessing ( self . best_init_model . L1_factor , relevance_bounds ) norm_probe_values_upper = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_upper ) norm_probe_values_lower = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_lower ) self . f_classifier = FeatureClassifier ( norm_probe_values_lower , norm_probe_values_upper , verbose = self . verbose ) feature_classes = self . f_classifier . classify ( norm_bounds ) return norm_bounds , feature_classes","title":"get_normalized_intervals"},{"location":"reference/fri/compute/#get_normalized_lupi_intervals","text":"def get_normalized_lupi_intervals ( self , lupi_features , presetModel = None ) View Source def get_normalized_lupi_intervals ( self , lupi_features , presetModel = None ): # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO : handle other data formats all_d = X . shape [ 1 ] normal_d = all_d - lupi_features # Compute relevance bounds and probes for normal features and LUPI with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : d_n = _get_necessary_dimensions ( normal_d , presetModel ) rb = self . compute_relevance_bounds ( d_n , parallel = parallel ) probe_upper = self . compute_probe_values ( d_n , True , parallel = parallel ) probe_lower = self . compute_probe_values ( d_n , False , parallel = parallel ) d_l = _get_necessary_dimensions ( all_d , presetModel , start = normal_d ) rb_l = self . compute_relevance_bounds ( d_l , parallel = parallel ) probe_priv_upper = self . compute_probe_values ( d_l , True , parallel = parallel ) probe_priv_lower = self . compute_probe_values ( d_l , False , parallel = parallel ) probes = [ probe_lower , probe_upper , probe_priv_lower , probe_priv_upper ] # # Postprocess # # Get Scaling Parameters l1 = self . init_constraints [ \"w_l1\" ] l1_priv = self . init_constraints [ \"w_priv_l1\" ] l1 = l1 + l1_priv # Normalize Normal and Lupi features rb_norm = self . _postprocessing ( l1 , rb ) rb_l_norm = self . _postprocessing ( l1 , rb_l ) interval_ = np . concatenate ([ rb_norm , rb_l_norm ]) # Normalize Probes probe_lower = self . _postprocessing ( l1 , probe_lower ) probe_upper = self . _postprocessing ( l1 , probe_upper ) probe_priv_lower = self . _postprocessing ( l1 , probe_priv_lower ) probe_priv_upper = self . _postprocessing ( l1 , probe_priv_upper ) # # # Classify features self . f_classifier = FeatureClassifier ( probe_lower , probe_upper , verbose = self . verbose ) feature_classes = self . f_classifier . classify ( rb_norm ) self . f_classifier_lupi = FeatureClassifier ( probe_priv_lower , probe_priv_upper , verbose = self . verbose ) feature_classes_lupi = self . f_classifier_lupi . classify ( rb_l_norm ) fc_both = np . concatenate ([ feature_classes , feature_classes_lupi ]) return interval_ , fc_both","title":"get_normalized_lupi_intervals"},{"location":"reference/fri/main/","text":"Module fri.main View Source from sklearn.base import BaseEstimator from sklearn.exceptions import NotFittedError from sklearn.feature_selection.base import SelectorMixin from sklearn.utils import check_random_state from sklearn.utils.validation import check_is_fitted from fri.compute import RelevanceBoundsIntervals from fri.model.base_type import ProblemType from fri.parameter_searcher import find_best_model RELEVANCE_MAPPING = { 0 : \"Irrelevant\" , 1 : \"Weak relevant\" , 2 : \"Strong relevant\" } class NotFeasibleForParameters ( Exception ): \"\"\" Problem was infeasible with the current parameter set. \"\"\" class FRIBase ( BaseEstimator , SelectorMixin ): def __init__ ( self , problem_type : ProblemType , random_state = None , n_jobs = 1 , verbose = 0 , n_param_search = 30 , n_probe_features = 40 , normalize = True , ** kwargs , ): \"\"\" Parameters ---------- problem_type : abc.ABCMeta random_state : Union[mtrand.RandomState, int, None, None, None, None, None, None, None] n_jobs : int verbose : int n_param_search : int n_probe_features : int normalize : bool kwargs : Attributes ---------- interval_ : array-like Feature relevance Intervals optim_model_ : `InitModel` Baseline model fitted on data relevance_classes_ : list(int) Classes of relevance encoded as int: 0 irrelevant, 1 weakly relevant, 2 strongly relevant relevance_classes_string_ : list(str) Classes of relevance encoded as string allrel_prediction_ : list(int) Relevance prediction encoded as boolean: 0 irrelevant, 1 relevant \"\"\" self . n_probe_features = n_probe_features self . n_param_search = n_param_search # assert issubclass(problem_type, ProblemType) self . problem_type_ = problem_type ( ** kwargs ) self . random_state = check_random_state ( random_state ) self . n_jobs = n_jobs self . verbose = verbose self . normalize = normalize self . interval_ = None self . optim_model_ = None self . relevance_classes_ = None self . relevance_classes_string_ = None self . allrel_prediction_ = None def fit ( self , X , y , lupi_features = 0 , ** kwargs ): \"\"\" Method to fit model on data. Parameters ---------- X : numpy.ndarray y : numpy.ndarray lupi_features : int Amount of features which are considered privileged information in `X`. The data is expected to be structured in a way that all lupi features are at the end of the set. For example `lupi_features=1` would denote the last column of `X` to be privileged. kwargs : dict Dictionary of additional keyword arguments depending on the `model`. Returns ------- `FRIBase` \"\"\" self . lupi_features_ = lupi_features self . n_samples_ = X . shape [ 0 ] self . n_features_ = X . shape [ 1 ] - lupi_features self . optim_model_ , best_score = self . _fit_baseline ( X , y , lupi_features , ** kwargs ) data = self . problem_type_ . preprocessing (( X , y ), lupi_features = lupi_features ) self . _relevance_bounds_computer = RelevanceBoundsIntervals ( data , self . problem_type_ , self . optim_model_ , self . random_state , self . n_probe_features , self . n_jobs , self . verbose , normalize = self . normalize , ) if lupi_features == 0 : ( self . interval_ , feature_classes , ) = self . _relevance_bounds_computer . get_normalized_intervals () else : ( self . interval_ , feature_classes , ) = self . _relevance_bounds_computer . get_normalized_lupi_intervals ( lupi_features = lupi_features ) self . _get_relevance_mask ( feature_classes ) # Return the classifier return self def _fit_baseline ( self , X , y , lupi_features = 0 , ** kwargs ): # Preprocessing data = self . problem_type_ . preprocessing (( X , y ), lupi_features = lupi_features ) # Get predefined template for our init. model init_model_template = self . problem_type_ . get_initmodel_template # Get hyperparameters which are predefined to our model template and can be seleted by user choice hyperparameters = self . problem_type_ . get_all_parameters () # search_samples = len(hyperparameters) * self.n_param_search # TODO: remove this search_samples = self . n_param_search # Find an optimal, fitted model using hyperparemeter search optimal_model , best_score = find_best_model ( init_model_template , hyperparameters , data , self . random_state , search_samples , self . n_jobs , self . verbose , lupi_features = lupi_features , ** kwargs , ) return optimal_model , best_score def _get_relevance_mask ( self , prediction ): \"\"\"Determines relevancy using feature relevance interval values Parameters ---------- fpr : float, optional false positive rate allowed under H_0 Returns ------- boolean array Relevancy prediction for each feature \"\"\" self . relevance_classes_ = prediction self . relevance_classes_string_ = [ RELEVANCE_MAPPING [ p ] for p in prediction ] self . allrel_prediction_ = prediction > 0 self . allrel_prediction_nonpriv_ = self . allrel_prediction_ [: self . n_features_ ] self . allrel_prediction_priv_ = self . allrel_prediction_ [ self . n_features_ :] self . relevance_classes_nonpriv_ = self . relevance_classes_ [: self . n_features_ ] self . relevance_classes_priv_ = self . relevance_classes_ [ self . n_features_ :] return self . allrel_prediction_ def _n_selected_features ( self ): \"\"\" Returns the number of selected features. ------- \"\"\" check_is_fitted ( self , \"allrel_prediction_\" ) return sum ( self . allrel_prediction_ ) def _get_support_mask ( self ): \"\"\"Method for SelectorMixin Returns ------- boolean array \"\"\" return self . allrel_prediction_ def score ( self , X , y ): \"\"\" Using fitted model predict points for `X` and compare to truth `y`. Parameters ---------- X : numpy.ndarray y : numpy.ndarray Returns ------- Model specific score (0 is worst, 1 is best) \"\"\" if self . optim_model_ : return self . optim_model_ . score ( X , y ) else : raise NotFittedError () def constrained_intervals ( self , preset : dict ): \"\"\" Method to return relevance intervals which are constrained using preset ranges or values. Parameters ---------- preset : dict like, {i:float} or {i:[float,float]} Keys denote feature index, values represent a fixed single value (float) or a range of allowed values (lower and upper bound). Example: To set feature 0 to a fixed value use >>> preset = {0: 0.1} or to use the minimum relevance bound >>> preset[1] = self.interval_[1, 0] Returns ------- array like Relevance bounds with user constraints \"\"\" # Do we have intervals? check_is_fitted ( self , \"interval_\" ) return self . _relevance_bounds_computer . compute_multi_preset_relevance_bounds ( preset = preset , lupi_features = self . lupi_features_ ) def print_interval_with_class ( self ): \"\"\" Pretty print the relevance intervals and determined feature relevance class \"\"\" output = \"\" if self . interval_ is None : output += \"Model is not fitted.\" output += \"############## Relevance bounds ############## \\n \" output += \"feature: [LB -- UB], relevance class \\n \" for i in range ( self . n_features_ + self . lupi_features_ ): if i == self . n_features_ : output += \"########## LUPI Relevance bounds \\n \" output += ( f \"{i:7}: [{self.interval_[i, 0]:1.1f} -- {self.interval_[i, 1]:1.1f}],\" ) output += f \" {self.relevance_classes_string_[i]} \\n \" return output Variables RELEVANCE_MAPPING Classes FRIBase class FRIBase ( problem_type : fri . model . base_type . ProblemType , random_state = None , n_jobs = 1 , verbose = 0 , n_param_search = 30 , n_probe_features = 40 , normalize = True , ** kwargs ) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). View Source class FRIBase ( BaseEstimator , SelectorMixin ): def __init__ ( self , problem_type : ProblemType , random_state = None , n_jobs = 1 , verbose = 0 , n_param_search = 30 , n_probe_features = 40 , normalize = True , ** kwargs , ): \"\"\" Parameters ---------- problem_type : abc.ABCMeta random_state : Union[mtrand.RandomState, int, None, None, None, None, None, None, None] n_jobs : int verbose : int n_param_search : int n_probe_features : int normalize : bool kwargs : Attributes ---------- interval_ : array-like Feature relevance Intervals optim_model_ : `InitModel` Baseline model fitted on data relevance_classes_ : list(int) Classes of relevance encoded as int: 0 irrelevant, 1 weakly relevant, 2 strongly relevant relevance_classes_string_ : list(str) Classes of relevance encoded as string allrel_prediction_ : list(int) Relevance prediction encoded as boolean: 0 irrelevant, 1 relevant \"\"\" self . n_probe_features = n_probe_features self . n_param_search = n_param_search # assert issubclass(problem_type, ProblemType) self . problem_type_ = problem_type ( ** kwargs ) self . random_state = check_random_state ( random_state ) self . n_jobs = n_jobs self . verbose = verbose self . normalize = normalize self . interval_ = None self . optim_model_ = None self . relevance_classes_ = None self . relevance_classes_string_ = None self . allrel_prediction_ = None def fit ( self , X , y , lupi_features = 0 , ** kwargs ): \"\"\" Method to fit model on data. Parameters ---------- X : numpy.ndarray y : numpy.ndarray lupi_features : int Amount of features which are considered privileged information in `X`. The data is expected to be structured in a way that all lupi features are at the end of the set. For example `lupi_features=1` would denote the last column of `X` to be privileged. kwargs : dict Dictionary of additional keyword arguments depending on the `model`. Returns ------- `FRIBase` \"\"\" self . lupi_features_ = lupi_features self . n_samples_ = X . shape [ 0 ] self . n_features_ = X . shape [ 1 ] - lupi_features self . optim_model_ , best_score = self . _fit_baseline ( X , y , lupi_features , ** kwargs ) data = self . problem_type_ . preprocessing (( X , y ), lupi_features = lupi_features ) self . _relevance_bounds_computer = RelevanceBoundsIntervals ( data , self . problem_type_ , self . optim_model_ , self . random_state , self . n_probe_features , self . n_jobs , self . verbose , normalize = self . normalize , ) if lupi_features == 0 : ( self . interval_ , feature_classes , ) = self . _relevance_bounds_computer . get_normalized_intervals () else : ( self . interval_ , feature_classes , ) = self . _relevance_bounds_computer . get_normalized_lupi_intervals ( lupi_features = lupi_features ) self . _get_relevance_mask ( feature_classes ) # Return the classifier return self def _fit_baseline ( self , X , y , lupi_features = 0 , ** kwargs ): # Preprocessing data = self . problem_type_ . preprocessing (( X , y ), lupi_features = lupi_features ) # Get predefined template for our init. model init_model_template = self . problem_type_ . get_initmodel_template # Get hyperparameters which are predefined to our model template and can be seleted by user choice hyperparameters = self . problem_type_ . get_all_parameters () # search_samples = len(hyperparameters) * self.n_param_search # TODO: remove this search_samples = self . n_param_search # Find an optimal, fitted model using hyperparemeter search optimal_model , best_score = find_best_model ( init_model_template , hyperparameters , data , self . random_state , search_samples , self . n_jobs , self . verbose , lupi_features = lupi_features , ** kwargs , ) return optimal_model , best_score def _get_relevance_mask ( self , prediction ): \"\"\"Determines relevancy using feature relevance interval values Parameters ---------- fpr : float, optional false positive rate allowed under H_0 Returns ------- boolean array Relevancy prediction for each feature \"\"\" self . relevance_classes_ = prediction self . relevance_classes_string_ = [ RELEVANCE_MAPPING [ p ] for p in prediction ] self . allrel_prediction_ = prediction > 0 self . allrel_prediction_nonpriv_ = self . allrel_prediction_ [: self . n_features_ ] self . allrel_prediction_priv_ = self . allrel_prediction_ [ self . n_features_ :] self . relevance_classes_nonpriv_ = self . relevance_classes_ [: self . n_features_ ] self . relevance_classes_priv_ = self . relevance_classes_ [ self . n_features_ :] return self . allrel_prediction_ def _n_selected_features ( self ): \"\"\" Returns the number of selected features. ------- \"\"\" check_is_fitted ( self , \"allrel_prediction_\" ) return sum ( self . allrel_prediction_ ) def _get_support_mask ( self ): \"\"\"Method for SelectorMixin Returns ------- boolean array \"\"\" return self . allrel_prediction_ def score ( self , X , y ): \"\"\" Using fitted model predict points for `X` and compare to truth `y`. Parameters ---------- X : numpy.ndarray y : numpy.ndarray Returns ------- Model specific score (0 is worst, 1 is best) \"\"\" if self . optim_model_ : return self . optim_model_ . score ( X , y ) else : raise NotFittedError () def constrained_intervals ( self , preset : dict ): \"\"\" Method to return relevance intervals which are constrained using preset ranges or values. Parameters ---------- preset : dict like, {i:float} or {i:[float,float]} Keys denote feature index, values represent a fixed single value (float) or a range of allowed values (lower and upper bound). Example: To set feature 0 to a fixed value use >>> preset = {0: 0.1} or to use the minimum relevance bound >>> preset[1] = self.interval_[1, 0] Returns ------- array like Relevance bounds with user constraints \"\"\" # Do we have intervals? check_is_fitted ( self , \"interval_\" ) return self . _relevance_bounds_computer . compute_multi_preset_relevance_bounds ( preset = preset , lupi_features = self . lupi_features_ ) def print_interval_with_class ( self ): \"\"\" Pretty print the relevance intervals and determined feature relevance class \"\"\" output = \"\" if self . interval_ is None : output += \"Model is not fitted.\" output += \"############## Relevance bounds ##############\\n\" output += \"feature: [LB -- UB], relevance class\\n\" for i in range ( self . n_features_ + self . lupi_features_ ): if i == self . n_features_ : output += \"########## LUPI Relevance bounds\\n\" output += ( f \"{i:7}: [{self.interval_[i, 0]:1.1f} -- {self.interval_[i, 1]:1.1f}],\" ) output += f \" {self.relevance_classes_string_[i]}\\n\" return output Ancestors (in MRO) sklearn.base.BaseEstimator sklearn.feature_selection._base.SelectorMixin sklearn.base.TransformerMixin Descendants fri.FRI Methods constrained_intervals def constrained_intervals ( self , preset : dict ) Method to return relevance intervals which are constrained using preset ranges or values. Parameters preset : dict like, {i:float} or {i:[float,float]} Keys denote feature index, values represent a fixed single value (float) or a range of allowed values (lower and upper bound). Example : To set feature 0 to a fixed value use >>> preset = { 0 : 0.1 } or to use the minimum relevance bound >>> preset [ 1 ] = self . interval_ [ 1 , 0 ] Returns array like Relevance bounds with user constraints View Source def constrained_intervals ( self , preset : dict ): \"\"\" Method to return relevance intervals which are constrained using preset ranges or values. Parameters ---------- preset : dict like, {i:float} or {i:[float,float]} Keys denote feature index, values represent a fixed single value (float) or a range of allowed values (lower and upper bound). Example: To set feature 0 to a fixed value use >>> preset = {0: 0.1} or to use the minimum relevance bound >>> preset[1] = self.interval_[1, 0] Returns ------- array like Relevance bounds with user constraints \"\"\" # Do we have intervals ? check_is_fitted ( self , \"interval_\" ) return self . _relevance_bounds_computer . compute_multi_preset_relevance_bounds ( preset = preset , lupi_features = self . lupi_features_ ) fit def fit ( self , X , y , lupi_features = 0 , ** kwargs ) Method to fit model on data. Parameters X : numpy.ndarray y : numpy.ndarray lupi_features : int Amount of features which are considered privileged information in X . The data is expected to be structured in a way that all lupi features are at the end of the set. For example lupi_features=1 would denote the last column of X to be privileged. kwargs : dict Dictionary of additional keyword arguments depending on the model . Returns FRIBase View Source def fit ( self , X , y , lupi_features = 0 , ** kwargs ): \"\"\" Method to fit model on data. Parameters ---------- X : numpy.ndarray y : numpy.ndarray lupi_features : int Amount of features which are considered privileged information in `X`. The data is expected to be structured in a way that all lupi features are at the end of the set. For example `lupi_features=1` would denote the last column of `X` to be privileged. kwargs : dict Dictionary of additional keyword arguments depending on the `model`. Returns ------- `FRIBase` \"\"\" self . lupi_features_ = lupi_features self . n_samples_ = X . shape [ 0 ] self . n_features_ = X . shape [ 1 ] - lupi_features self . optim_model_ , best_score = self . _fit_baseline ( X , y , lupi_features , ** kwargs ) data = self . problem_type_ . preprocessing (( X , y ), lupi_features = lupi_features ) self . _relevance_bounds_computer = RelevanceBoundsIntervals ( data , self . problem_type_ , self . optim_model_ , self . random_state , self . n_probe_features , self . n_jobs , self . verbose , normalize = self . normalize , ) if lupi_features == 0 : ( self . interval_ , feature_classes , ) = self . _relevance_bounds_computer . get_normalized_intervals () else : ( self . interval_ , feature_classes , ) = self . _relevance_bounds_computer . get_normalized_lupi_intervals ( lupi_features = lupi_features ) self . _get_relevance_mask ( feature_classes ) # Return the classifier return self fit_transform def fit_transform ( self , X , y = None , ** fit_params ) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. View Source def fit_transform ( self , X , y = None , ** fit_params ) : \"\"\" Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters ---------- X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns ------- X_new : numpy array of shape [n_samples, n_features_new] Transformed array. \"\"\" # non - optimized default implementation ; override when a better # method is possible for a given clustering algorithm if y is None : # fit method of arity 1 ( unsupervised transformation ) return self . fit ( X , ** fit_params ). transform ( X ) else : # fit method of arity 2 ( supervised transformation ) return self . fit ( X , y , ** fit_params ). transform ( X ) get_params def get_params ( self , deep = True ) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out get_support def get_support ( self , indices = False ) Get a mask, or integer index, of the features selected Parameters indices : boolean (default False) If True, the return value will be an array of integers, rather than a boolean mask. Returns support : array An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. View Source def get_support ( self , indices = False ): \"\"\" Get a mask, or integer index, of the features selected Parameters ---------- indices : boolean (default False) If True, the return value will be an array of integers, rather than a boolean mask. Returns ------- support : array An index that selects the retained features from a feature vector. If `indices` is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If `indices` is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. \"\"\" mask = self . _get_support_mask () return mask if not indices else np . where ( mask )[ 0 ] inverse_transform def inverse_transform ( self , X ) Reverse the transformation operation Parameters X : array of shape [n_samples, n_selected_features] The input samples. Returns X_r : array of shape [n_samples, n_original_features] X with columns of zeros inserted where features would have been removed by :meth: transform . View Source def inverse_transform ( self , X ): \"\"\" Reverse the transformation operation Parameters ---------- X : array of shape [n_samples, n_selected_features] The input samples. Returns ------- X_r : array of shape [n_samples, n_original_features] `X` with columns of zeros inserted where features would have been removed by :meth:`transform`. \"\"\" if issparse ( X ): X = X . tocsc () # insert additional entries in indptr: # e.g. if transform changed indptr from [0 2 6 7] to [0 2 3] # col_nonzeros here will be [2 0 1] so indptr becomes [0 2 2 3] it = self . inverse_transform ( np . diff ( X . indptr ). reshape ( 1 , - 1 )) col_nonzeros = it . ravel () indptr = np . concatenate ([[ 0 ], np . cumsum ( col_nonzeros )]) Xt = csc_matrix (( X . data , X . indices , indptr ), shape = ( X . shape [ 0 ], len ( indptr ) - 1 ), dtype = X . dtype ) return Xt support = self . get_support () X = check_array ( X , dtype = None ) if support . sum () != X . shape [ 1 ]: raise ValueError ( \"X has a different shape than during fitting.\" ) if X . ndim == 1 : X = X [ None , :] Xt = np . zeros (( X . shape [ 0 ], support . size ), dtype = X . dtype ) Xt [:, support ] = X return Xt print_interval_with_class def print_interval_with_class ( self ) Pretty print the relevance intervals and determined feature relevance class View Source def print_interval_with_class ( self ) : \"\"\" Pretty print the relevance intervals and determined feature relevance class \"\"\" output = \"\" if self . interval_ is None : output += \"Model is not fitted.\" output += \"############## Relevance bounds ##############\\n\" output += \"feature: [LB -- UB], relevance class\\n\" for i in range ( self . n_features_ + self . lupi_features_ ) : if i == self . n_features_ : output += \"########## LUPI Relevance bounds\\n\" output += ( f \"{i:7}: [{self.interval_[i, 0]:1.1f} -- {self.interval_[i, 1]:1.1f}],\" ) output += f \" {self.relevance_classes_string_[i]}\\n\" return output score def score ( self , X , y ) Using fitted model predict points for X and compare to truth y . Parameters X : numpy.ndarray y : numpy.ndarray Returns Model specific score (0 is worst, 1 is best) View Source def score ( self , X , y ): \"\"\" Using fitted model predict points for `X` and compare to truth `y`. Parameters ---------- X : numpy.ndarray y : numpy.ndarray Returns ------- Model specific score (0 is worst, 1 is best) \"\"\" if self . optim_model_ : return self . optim_model_ . score ( X , y ) else : raise NotFittedError () set_params def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self transform def transform ( self , X ) Reduce X to the selected features. Parameters X : array of shape [n_samples, n_features] The input samples. Returns X_r : array of shape [n_samples, n_selected_features] The input samples with only the selected features. View Source def transform ( self , X ): \"\"\"Reduce X to the selected features. Parameters ---------- X : array of shape [n_samples, n_features] The input samples. Returns ------- X_r : array of shape [n_samples, n_selected_features] The input samples with only the selected features. \"\"\" tags = self . _get_tags () X = check_array ( X , dtype = None , accept_sparse = 'csr' , force_all_finite = not tags . get ( 'allow_nan' , True )) mask = self . get_support () if not mask . any (): warn ( \"No features were selected: either the data is\" \" too noisy or the selection test too strict.\" , UserWarning ) return np . empty ( 0 ). reshape (( X . shape [ 0 ], 0 )) if len ( mask ) != X . shape [ 1 ]: raise ValueError ( \"X has a different shape than during fitting.\" ) return X [:, safe_mask ( X , mask )] NotFeasibleForParameters class NotFeasibleForParameters ( / , * args , ** kwargs ) Problem was infeasible with the current parameter set. View Source class NotFeasibleForParameters ( Exception ): \"\"\" Problem was infeasible with the current parameter set. \"\"\" Ancestors (in MRO) builtins.Exception builtins.BaseException Class variables args Methods with_traceback def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"Main"},{"location":"reference/fri/main/#module-frimain","text":"View Source from sklearn.base import BaseEstimator from sklearn.exceptions import NotFittedError from sklearn.feature_selection.base import SelectorMixin from sklearn.utils import check_random_state from sklearn.utils.validation import check_is_fitted from fri.compute import RelevanceBoundsIntervals from fri.model.base_type import ProblemType from fri.parameter_searcher import find_best_model RELEVANCE_MAPPING = { 0 : \"Irrelevant\" , 1 : \"Weak relevant\" , 2 : \"Strong relevant\" } class NotFeasibleForParameters ( Exception ): \"\"\" Problem was infeasible with the current parameter set. \"\"\" class FRIBase ( BaseEstimator , SelectorMixin ): def __init__ ( self , problem_type : ProblemType , random_state = None , n_jobs = 1 , verbose = 0 , n_param_search = 30 , n_probe_features = 40 , normalize = True , ** kwargs , ): \"\"\" Parameters ---------- problem_type : abc.ABCMeta random_state : Union[mtrand.RandomState, int, None, None, None, None, None, None, None] n_jobs : int verbose : int n_param_search : int n_probe_features : int normalize : bool kwargs : Attributes ---------- interval_ : array-like Feature relevance Intervals optim_model_ : `InitModel` Baseline model fitted on data relevance_classes_ : list(int) Classes of relevance encoded as int: 0 irrelevant, 1 weakly relevant, 2 strongly relevant relevance_classes_string_ : list(str) Classes of relevance encoded as string allrel_prediction_ : list(int) Relevance prediction encoded as boolean: 0 irrelevant, 1 relevant \"\"\" self . n_probe_features = n_probe_features self . n_param_search = n_param_search # assert issubclass(problem_type, ProblemType) self . problem_type_ = problem_type ( ** kwargs ) self . random_state = check_random_state ( random_state ) self . n_jobs = n_jobs self . verbose = verbose self . normalize = normalize self . interval_ = None self . optim_model_ = None self . relevance_classes_ = None self . relevance_classes_string_ = None self . allrel_prediction_ = None def fit ( self , X , y , lupi_features = 0 , ** kwargs ): \"\"\" Method to fit model on data. Parameters ---------- X : numpy.ndarray y : numpy.ndarray lupi_features : int Amount of features which are considered privileged information in `X`. The data is expected to be structured in a way that all lupi features are at the end of the set. For example `lupi_features=1` would denote the last column of `X` to be privileged. kwargs : dict Dictionary of additional keyword arguments depending on the `model`. Returns ------- `FRIBase` \"\"\" self . lupi_features_ = lupi_features self . n_samples_ = X . shape [ 0 ] self . n_features_ = X . shape [ 1 ] - lupi_features self . optim_model_ , best_score = self . _fit_baseline ( X , y , lupi_features , ** kwargs ) data = self . problem_type_ . preprocessing (( X , y ), lupi_features = lupi_features ) self . _relevance_bounds_computer = RelevanceBoundsIntervals ( data , self . problem_type_ , self . optim_model_ , self . random_state , self . n_probe_features , self . n_jobs , self . verbose , normalize = self . normalize , ) if lupi_features == 0 : ( self . interval_ , feature_classes , ) = self . _relevance_bounds_computer . get_normalized_intervals () else : ( self . interval_ , feature_classes , ) = self . _relevance_bounds_computer . get_normalized_lupi_intervals ( lupi_features = lupi_features ) self . _get_relevance_mask ( feature_classes ) # Return the classifier return self def _fit_baseline ( self , X , y , lupi_features = 0 , ** kwargs ): # Preprocessing data = self . problem_type_ . preprocessing (( X , y ), lupi_features = lupi_features ) # Get predefined template for our init. model init_model_template = self . problem_type_ . get_initmodel_template # Get hyperparameters which are predefined to our model template and can be seleted by user choice hyperparameters = self . problem_type_ . get_all_parameters () # search_samples = len(hyperparameters) * self.n_param_search # TODO: remove this search_samples = self . n_param_search # Find an optimal, fitted model using hyperparemeter search optimal_model , best_score = find_best_model ( init_model_template , hyperparameters , data , self . random_state , search_samples , self . n_jobs , self . verbose , lupi_features = lupi_features , ** kwargs , ) return optimal_model , best_score def _get_relevance_mask ( self , prediction ): \"\"\"Determines relevancy using feature relevance interval values Parameters ---------- fpr : float, optional false positive rate allowed under H_0 Returns ------- boolean array Relevancy prediction for each feature \"\"\" self . relevance_classes_ = prediction self . relevance_classes_string_ = [ RELEVANCE_MAPPING [ p ] for p in prediction ] self . allrel_prediction_ = prediction > 0 self . allrel_prediction_nonpriv_ = self . allrel_prediction_ [: self . n_features_ ] self . allrel_prediction_priv_ = self . allrel_prediction_ [ self . n_features_ :] self . relevance_classes_nonpriv_ = self . relevance_classes_ [: self . n_features_ ] self . relevance_classes_priv_ = self . relevance_classes_ [ self . n_features_ :] return self . allrel_prediction_ def _n_selected_features ( self ): \"\"\" Returns the number of selected features. ------- \"\"\" check_is_fitted ( self , \"allrel_prediction_\" ) return sum ( self . allrel_prediction_ ) def _get_support_mask ( self ): \"\"\"Method for SelectorMixin Returns ------- boolean array \"\"\" return self . allrel_prediction_ def score ( self , X , y ): \"\"\" Using fitted model predict points for `X` and compare to truth `y`. Parameters ---------- X : numpy.ndarray y : numpy.ndarray Returns ------- Model specific score (0 is worst, 1 is best) \"\"\" if self . optim_model_ : return self . optim_model_ . score ( X , y ) else : raise NotFittedError () def constrained_intervals ( self , preset : dict ): \"\"\" Method to return relevance intervals which are constrained using preset ranges or values. Parameters ---------- preset : dict like, {i:float} or {i:[float,float]} Keys denote feature index, values represent a fixed single value (float) or a range of allowed values (lower and upper bound). Example: To set feature 0 to a fixed value use >>> preset = {0: 0.1} or to use the minimum relevance bound >>> preset[1] = self.interval_[1, 0] Returns ------- array like Relevance bounds with user constraints \"\"\" # Do we have intervals? check_is_fitted ( self , \"interval_\" ) return self . _relevance_bounds_computer . compute_multi_preset_relevance_bounds ( preset = preset , lupi_features = self . lupi_features_ ) def print_interval_with_class ( self ): \"\"\" Pretty print the relevance intervals and determined feature relevance class \"\"\" output = \"\" if self . interval_ is None : output += \"Model is not fitted.\" output += \"############## Relevance bounds ############## \\n \" output += \"feature: [LB -- UB], relevance class \\n \" for i in range ( self . n_features_ + self . lupi_features_ ): if i == self . n_features_ : output += \"########## LUPI Relevance bounds \\n \" output += ( f \"{i:7}: [{self.interval_[i, 0]:1.1f} -- {self.interval_[i, 1]:1.1f}],\" ) output += f \" {self.relevance_classes_string_[i]} \\n \" return output","title":"Module fri.main"},{"location":"reference/fri/main/#variables","text":"RELEVANCE_MAPPING","title":"Variables"},{"location":"reference/fri/main/#classes","text":"","title":"Classes"},{"location":"reference/fri/main/#fribase","text":"class FRIBase ( problem_type : fri . model . base_type . ProblemType , random_state = None , n_jobs = 1 , verbose = 0 , n_param_search = 30 , n_probe_features = 40 , normalize = True , ** kwargs ) Base class for all estimators in scikit-learn","title":"FRIBase"},{"location":"reference/fri/main/#notes","text":"All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). View Source class FRIBase ( BaseEstimator , SelectorMixin ): def __init__ ( self , problem_type : ProblemType , random_state = None , n_jobs = 1 , verbose = 0 , n_param_search = 30 , n_probe_features = 40 , normalize = True , ** kwargs , ): \"\"\" Parameters ---------- problem_type : abc.ABCMeta random_state : Union[mtrand.RandomState, int, None, None, None, None, None, None, None] n_jobs : int verbose : int n_param_search : int n_probe_features : int normalize : bool kwargs : Attributes ---------- interval_ : array-like Feature relevance Intervals optim_model_ : `InitModel` Baseline model fitted on data relevance_classes_ : list(int) Classes of relevance encoded as int: 0 irrelevant, 1 weakly relevant, 2 strongly relevant relevance_classes_string_ : list(str) Classes of relevance encoded as string allrel_prediction_ : list(int) Relevance prediction encoded as boolean: 0 irrelevant, 1 relevant \"\"\" self . n_probe_features = n_probe_features self . n_param_search = n_param_search # assert issubclass(problem_type, ProblemType) self . problem_type_ = problem_type ( ** kwargs ) self . random_state = check_random_state ( random_state ) self . n_jobs = n_jobs self . verbose = verbose self . normalize = normalize self . interval_ = None self . optim_model_ = None self . relevance_classes_ = None self . relevance_classes_string_ = None self . allrel_prediction_ = None def fit ( self , X , y , lupi_features = 0 , ** kwargs ): \"\"\" Method to fit model on data. Parameters ---------- X : numpy.ndarray y : numpy.ndarray lupi_features : int Amount of features which are considered privileged information in `X`. The data is expected to be structured in a way that all lupi features are at the end of the set. For example `lupi_features=1` would denote the last column of `X` to be privileged. kwargs : dict Dictionary of additional keyword arguments depending on the `model`. Returns ------- `FRIBase` \"\"\" self . lupi_features_ = lupi_features self . n_samples_ = X . shape [ 0 ] self . n_features_ = X . shape [ 1 ] - lupi_features self . optim_model_ , best_score = self . _fit_baseline ( X , y , lupi_features , ** kwargs ) data = self . problem_type_ . preprocessing (( X , y ), lupi_features = lupi_features ) self . _relevance_bounds_computer = RelevanceBoundsIntervals ( data , self . problem_type_ , self . optim_model_ , self . random_state , self . n_probe_features , self . n_jobs , self . verbose , normalize = self . normalize , ) if lupi_features == 0 : ( self . interval_ , feature_classes , ) = self . _relevance_bounds_computer . get_normalized_intervals () else : ( self . interval_ , feature_classes , ) = self . _relevance_bounds_computer . get_normalized_lupi_intervals ( lupi_features = lupi_features ) self . _get_relevance_mask ( feature_classes ) # Return the classifier return self def _fit_baseline ( self , X , y , lupi_features = 0 , ** kwargs ): # Preprocessing data = self . problem_type_ . preprocessing (( X , y ), lupi_features = lupi_features ) # Get predefined template for our init. model init_model_template = self . problem_type_ . get_initmodel_template # Get hyperparameters which are predefined to our model template and can be seleted by user choice hyperparameters = self . problem_type_ . get_all_parameters () # search_samples = len(hyperparameters) * self.n_param_search # TODO: remove this search_samples = self . n_param_search # Find an optimal, fitted model using hyperparemeter search optimal_model , best_score = find_best_model ( init_model_template , hyperparameters , data , self . random_state , search_samples , self . n_jobs , self . verbose , lupi_features = lupi_features , ** kwargs , ) return optimal_model , best_score def _get_relevance_mask ( self , prediction ): \"\"\"Determines relevancy using feature relevance interval values Parameters ---------- fpr : float, optional false positive rate allowed under H_0 Returns ------- boolean array Relevancy prediction for each feature \"\"\" self . relevance_classes_ = prediction self . relevance_classes_string_ = [ RELEVANCE_MAPPING [ p ] for p in prediction ] self . allrel_prediction_ = prediction > 0 self . allrel_prediction_nonpriv_ = self . allrel_prediction_ [: self . n_features_ ] self . allrel_prediction_priv_ = self . allrel_prediction_ [ self . n_features_ :] self . relevance_classes_nonpriv_ = self . relevance_classes_ [: self . n_features_ ] self . relevance_classes_priv_ = self . relevance_classes_ [ self . n_features_ :] return self . allrel_prediction_ def _n_selected_features ( self ): \"\"\" Returns the number of selected features. ------- \"\"\" check_is_fitted ( self , \"allrel_prediction_\" ) return sum ( self . allrel_prediction_ ) def _get_support_mask ( self ): \"\"\"Method for SelectorMixin Returns ------- boolean array \"\"\" return self . allrel_prediction_ def score ( self , X , y ): \"\"\" Using fitted model predict points for `X` and compare to truth `y`. Parameters ---------- X : numpy.ndarray y : numpy.ndarray Returns ------- Model specific score (0 is worst, 1 is best) \"\"\" if self . optim_model_ : return self . optim_model_ . score ( X , y ) else : raise NotFittedError () def constrained_intervals ( self , preset : dict ): \"\"\" Method to return relevance intervals which are constrained using preset ranges or values. Parameters ---------- preset : dict like, {i:float} or {i:[float,float]} Keys denote feature index, values represent a fixed single value (float) or a range of allowed values (lower and upper bound). Example: To set feature 0 to a fixed value use >>> preset = {0: 0.1} or to use the minimum relevance bound >>> preset[1] = self.interval_[1, 0] Returns ------- array like Relevance bounds with user constraints \"\"\" # Do we have intervals? check_is_fitted ( self , \"interval_\" ) return self . _relevance_bounds_computer . compute_multi_preset_relevance_bounds ( preset = preset , lupi_features = self . lupi_features_ ) def print_interval_with_class ( self ): \"\"\" Pretty print the relevance intervals and determined feature relevance class \"\"\" output = \"\" if self . interval_ is None : output += \"Model is not fitted.\" output += \"############## Relevance bounds ##############\\n\" output += \"feature: [LB -- UB], relevance class\\n\" for i in range ( self . n_features_ + self . lupi_features_ ): if i == self . n_features_ : output += \"########## LUPI Relevance bounds\\n\" output += ( f \"{i:7}: [{self.interval_[i, 0]:1.1f} -- {self.interval_[i, 1]:1.1f}],\" ) output += f \" {self.relevance_classes_string_[i]}\\n\" return output","title":"Notes"},{"location":"reference/fri/main/#ancestors-in-mro","text":"sklearn.base.BaseEstimator sklearn.feature_selection._base.SelectorMixin sklearn.base.TransformerMixin","title":"Ancestors (in MRO)"},{"location":"reference/fri/main/#descendants","text":"fri.FRI","title":"Descendants"},{"location":"reference/fri/main/#methods","text":"","title":"Methods"},{"location":"reference/fri/main/#constrained_intervals","text":"def constrained_intervals ( self , preset : dict ) Method to return relevance intervals which are constrained using preset ranges or values.","title":"constrained_intervals"},{"location":"reference/fri/main/#parameters","text":"preset : dict like, {i:float} or {i:[float,float]} Keys denote feature index, values represent a fixed single value (float) or a range of allowed values (lower and upper bound). Example : To set feature 0 to a fixed value use >>> preset = { 0 : 0.1 } or to use the minimum relevance bound >>> preset [ 1 ] = self . interval_ [ 1 , 0 ]","title":"Parameters"},{"location":"reference/fri/main/#returns","text":"array like Relevance bounds with user constraints View Source def constrained_intervals ( self , preset : dict ): \"\"\" Method to return relevance intervals which are constrained using preset ranges or values. Parameters ---------- preset : dict like, {i:float} or {i:[float,float]} Keys denote feature index, values represent a fixed single value (float) or a range of allowed values (lower and upper bound). Example: To set feature 0 to a fixed value use >>> preset = {0: 0.1} or to use the minimum relevance bound >>> preset[1] = self.interval_[1, 0] Returns ------- array like Relevance bounds with user constraints \"\"\" # Do we have intervals ? check_is_fitted ( self , \"interval_\" ) return self . _relevance_bounds_computer . compute_multi_preset_relevance_bounds ( preset = preset , lupi_features = self . lupi_features_ )","title":"Returns"},{"location":"reference/fri/main/#fit","text":"def fit ( self , X , y , lupi_features = 0 , ** kwargs ) Method to fit model on data.","title":"fit"},{"location":"reference/fri/main/#parameters_1","text":"X : numpy.ndarray y : numpy.ndarray lupi_features : int Amount of features which are considered privileged information in X . The data is expected to be structured in a way that all lupi features are at the end of the set. For example lupi_features=1 would denote the last column of X to be privileged. kwargs : dict Dictionary of additional keyword arguments depending on the model .","title":"Parameters"},{"location":"reference/fri/main/#returns_1","text":"FRIBase View Source def fit ( self , X , y , lupi_features = 0 , ** kwargs ): \"\"\" Method to fit model on data. Parameters ---------- X : numpy.ndarray y : numpy.ndarray lupi_features : int Amount of features which are considered privileged information in `X`. The data is expected to be structured in a way that all lupi features are at the end of the set. For example `lupi_features=1` would denote the last column of `X` to be privileged. kwargs : dict Dictionary of additional keyword arguments depending on the `model`. Returns ------- `FRIBase` \"\"\" self . lupi_features_ = lupi_features self . n_samples_ = X . shape [ 0 ] self . n_features_ = X . shape [ 1 ] - lupi_features self . optim_model_ , best_score = self . _fit_baseline ( X , y , lupi_features , ** kwargs ) data = self . problem_type_ . preprocessing (( X , y ), lupi_features = lupi_features ) self . _relevance_bounds_computer = RelevanceBoundsIntervals ( data , self . problem_type_ , self . optim_model_ , self . random_state , self . n_probe_features , self . n_jobs , self . verbose , normalize = self . normalize , ) if lupi_features == 0 : ( self . interval_ , feature_classes , ) = self . _relevance_bounds_computer . get_normalized_intervals () else : ( self . interval_ , feature_classes , ) = self . _relevance_bounds_computer . get_normalized_lupi_intervals ( lupi_features = lupi_features ) self . _get_relevance_mask ( feature_classes ) # Return the classifier return self","title":"Returns"},{"location":"reference/fri/main/#fit_transform","text":"def fit_transform ( self , X , y = None , ** fit_params ) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.","title":"fit_transform"},{"location":"reference/fri/main/#parameters_2","text":"X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters.","title":"Parameters"},{"location":"reference/fri/main/#returns_2","text":"X_new : numpy array of shape [n_samples, n_features_new] Transformed array. View Source def fit_transform ( self , X , y = None , ** fit_params ) : \"\"\" Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters ---------- X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns ------- X_new : numpy array of shape [n_samples, n_features_new] Transformed array. \"\"\" # non - optimized default implementation ; override when a better # method is possible for a given clustering algorithm if y is None : # fit method of arity 1 ( unsupervised transformation ) return self . fit ( X , ** fit_params ). transform ( X ) else : # fit method of arity 2 ( supervised transformation ) return self . fit ( X , y , ** fit_params ). transform ( X )","title":"Returns"},{"location":"reference/fri/main/#get_params","text":"def get_params ( self , deep = True ) Get parameters for this estimator.","title":"get_params"},{"location":"reference/fri/main/#parameters_3","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"reference/fri/main/#returns_3","text":"params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out","title":"Returns"},{"location":"reference/fri/main/#get_support","text":"def get_support ( self , indices = False ) Get a mask, or integer index, of the features selected","title":"get_support"},{"location":"reference/fri/main/#parameters_4","text":"indices : boolean (default False) If True, the return value will be an array of integers, rather than a boolean mask.","title":"Parameters"},{"location":"reference/fri/main/#returns_4","text":"support : array An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. View Source def get_support ( self , indices = False ): \"\"\" Get a mask, or integer index, of the features selected Parameters ---------- indices : boolean (default False) If True, the return value will be an array of integers, rather than a boolean mask. Returns ------- support : array An index that selects the retained features from a feature vector. If `indices` is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If `indices` is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. \"\"\" mask = self . _get_support_mask () return mask if not indices else np . where ( mask )[ 0 ]","title":"Returns"},{"location":"reference/fri/main/#inverse_transform","text":"def inverse_transform ( self , X ) Reverse the transformation operation","title":"inverse_transform"},{"location":"reference/fri/main/#parameters_5","text":"X : array of shape [n_samples, n_selected_features] The input samples.","title":"Parameters"},{"location":"reference/fri/main/#returns_5","text":"X_r : array of shape [n_samples, n_original_features] X with columns of zeros inserted where features would have been removed by :meth: transform . View Source def inverse_transform ( self , X ): \"\"\" Reverse the transformation operation Parameters ---------- X : array of shape [n_samples, n_selected_features] The input samples. Returns ------- X_r : array of shape [n_samples, n_original_features] `X` with columns of zeros inserted where features would have been removed by :meth:`transform`. \"\"\" if issparse ( X ): X = X . tocsc () # insert additional entries in indptr: # e.g. if transform changed indptr from [0 2 6 7] to [0 2 3] # col_nonzeros here will be [2 0 1] so indptr becomes [0 2 2 3] it = self . inverse_transform ( np . diff ( X . indptr ). reshape ( 1 , - 1 )) col_nonzeros = it . ravel () indptr = np . concatenate ([[ 0 ], np . cumsum ( col_nonzeros )]) Xt = csc_matrix (( X . data , X . indices , indptr ), shape = ( X . shape [ 0 ], len ( indptr ) - 1 ), dtype = X . dtype ) return Xt support = self . get_support () X = check_array ( X , dtype = None ) if support . sum () != X . shape [ 1 ]: raise ValueError ( \"X has a different shape than during fitting.\" ) if X . ndim == 1 : X = X [ None , :] Xt = np . zeros (( X . shape [ 0 ], support . size ), dtype = X . dtype ) Xt [:, support ] = X return Xt","title":"Returns"},{"location":"reference/fri/main/#print_interval_with_class","text":"def print_interval_with_class ( self ) Pretty print the relevance intervals and determined feature relevance class View Source def print_interval_with_class ( self ) : \"\"\" Pretty print the relevance intervals and determined feature relevance class \"\"\" output = \"\" if self . interval_ is None : output += \"Model is not fitted.\" output += \"############## Relevance bounds ##############\\n\" output += \"feature: [LB -- UB], relevance class\\n\" for i in range ( self . n_features_ + self . lupi_features_ ) : if i == self . n_features_ : output += \"########## LUPI Relevance bounds\\n\" output += ( f \"{i:7}: [{self.interval_[i, 0]:1.1f} -- {self.interval_[i, 1]:1.1f}],\" ) output += f \" {self.relevance_classes_string_[i]}\\n\" return output","title":"print_interval_with_class"},{"location":"reference/fri/main/#score","text":"def score ( self , X , y ) Using fitted model predict points for X and compare to truth y .","title":"score"},{"location":"reference/fri/main/#parameters_6","text":"X : numpy.ndarray y : numpy.ndarray","title":"Parameters"},{"location":"reference/fri/main/#returns_6","text":"Model specific score (0 is worst, 1 is best) View Source def score ( self , X , y ): \"\"\" Using fitted model predict points for `X` and compare to truth `y`. Parameters ---------- X : numpy.ndarray y : numpy.ndarray Returns ------- Model specific score (0 is worst, 1 is best) \"\"\" if self . optim_model_ : return self . optim_model_ . score ( X , y ) else : raise NotFittedError ()","title":"Returns"},{"location":"reference/fri/main/#set_params","text":"def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params"},{"location":"reference/fri/main/#parameters_7","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"reference/fri/main/#returns_7","text":"self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Returns"},{"location":"reference/fri/main/#transform","text":"def transform ( self , X ) Reduce X to the selected features.","title":"transform"},{"location":"reference/fri/main/#parameters_8","text":"X : array of shape [n_samples, n_features] The input samples.","title":"Parameters"},{"location":"reference/fri/main/#returns_8","text":"X_r : array of shape [n_samples, n_selected_features] The input samples with only the selected features. View Source def transform ( self , X ): \"\"\"Reduce X to the selected features. Parameters ---------- X : array of shape [n_samples, n_features] The input samples. Returns ------- X_r : array of shape [n_samples, n_selected_features] The input samples with only the selected features. \"\"\" tags = self . _get_tags () X = check_array ( X , dtype = None , accept_sparse = 'csr' , force_all_finite = not tags . get ( 'allow_nan' , True )) mask = self . get_support () if not mask . any (): warn ( \"No features were selected: either the data is\" \" too noisy or the selection test too strict.\" , UserWarning ) return np . empty ( 0 ). reshape (( X . shape [ 0 ], 0 )) if len ( mask ) != X . shape [ 1 ]: raise ValueError ( \"X has a different shape than during fitting.\" ) return X [:, safe_mask ( X , mask )]","title":"Returns"},{"location":"reference/fri/main/#notfeasibleforparameters","text":"class NotFeasibleForParameters ( / , * args , ** kwargs ) Problem was infeasible with the current parameter set. View Source class NotFeasibleForParameters ( Exception ): \"\"\" Problem was infeasible with the current parameter set. \"\"\"","title":"NotFeasibleForParameters"},{"location":"reference/fri/main/#ancestors-in-mro_1","text":"builtins.Exception builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"reference/fri/main/#class-variables","text":"args","title":"Class variables"},{"location":"reference/fri/main/#methods_1","text":"","title":"Methods"},{"location":"reference/fri/main/#with_traceback","text":"def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"with_traceback"},{"location":"reference/fri/parameter_searcher/","text":"Module fri.parameter_searcher In this class we use hyperparameter search to find parameters needed in our model. Depending on the input model we sample parameters from a random distribution. The sampling rate can be increased. The model with the best internally defined accuracy is picked. To increase robustness we use cross validation. View Source \"\"\" In this class we use hyperparameter search to find parameters needed in our model. Depending on the input model we sample parameters from a random distribution. The sampling rate can be increased. The model with the best internally defined accuracy is picked. To increase robustness we use cross validation. \"\"\" import warnings from sklearn.exceptions import FitFailedWarning warnings . filterwarnings ( action = \"ignore\" , category = FitFailedWarning ) from pprint import pprint from typing import Tuple import numpy as np from sklearn.model_selection import RandomizedSearchCV from fri.model.base_initmodel import InitModel def find_best_model ( model_template : InitModel , hyperparameters : dict , data : Tuple [ np . ndarray , np . ndarray ], random_state : np . random . RandomState , n_iter : int , n_jobs : int , verbose : int = 0 , lupi_features = None , kwargs : dict = None , ) -> Tuple [ InitModel , float ]: \"\"\" Search function which wraps `sklearns` `RandomizedSearchCV` function. We use distributions and parameters defined in the `model_template`. Parameters ---------- model_template : InitModel A model template which is used to fit data. hyperparameters : dict Dictionary of hyperparameters. data : tuple Tuple of data (X,y) random_state : RandomState numpy RandomState object n_iter : int Amount of search samples. n_jobs : int Allows multiprocessing with `n_jobs` threads. verbose : int Allows verbose output when `verbose>0`. lupi_features : int Amount of lupi_features kwargs : dict Placeholder, dict to pass into fit functions. \"\"\" if lupi_features > 0 : model = model_template ( lupi_features = lupi_features ) else : model = model_template () scorer , metric = model . make_scorer () if scorer is None : refit = True else : refit = metric searcher = RandomizedSearchCV ( model , hyperparameters , scoring = scorer , random_state = random_state , refit = refit , cv = 3 , n_iter = n_iter , n_jobs = n_jobs , error_score = np . nan , verbose = verbose , ) X , y = data # Ignore warnings for extremely bad model_state (when precision=0) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) searcher . fit ( X , y ) best_model : InitModel = searcher . best_estimator_ best_score = best_model . score ( X , y ) if verbose > 0 : print ( \"*\" * 20 , \"Best found baseline model\" , \"*\" * 20 ) pprint ( best_model ) print ( \"score: \" , best_score ) for k , v in best_model . constraints . items (): pprint (( f \" {k} : {v} \" )) for k , v in best_model . model_state . items (): if hasattr ( v , \"shape\" ): pprint (( f \" {k} : shape {v.shape} \" )) else : if \"slack\" in k : continue pprint (( f \" {k} : {v} \" )) print ( \"*\" * 30 ) return best_model , best_score Functions find_best_model def find_best_model ( model_template : fri . model . base_initmodel . InitModel , hyperparameters : dict , data : Tuple [ numpy . ndarray , numpy . ndarray ], random_state : numpy . random . mtrand . RandomState , n_iter : int , n_jobs : int , verbose : int = 0 , lupi_features = None , kwargs : dict = None ) -> Tuple [ fri . model . base_initmodel . InitModel , float ] Search function which wraps sklearns RandomizedSearchCV function. We use distributions and parameters defined in the model_template . Parameters model_template : InitModel A model template which is used to fit data. hyperparameters : dict Dictionary of hyperparameters. data : tuple Tuple of data (X,y) random_state : RandomState numpy RandomState object n_iter : int Amount of search samples. n_jobs : int Allows multiprocessing with n_jobs threads. verbose : int Allows verbose output when verbose>0 . lupi_features : int Amount of lupi_features kwargs : dict Placeholder, dict to pass into fit functions. View Source def find_best_model ( model_template : InitModel , hyperparameters : dict , data : Tuple [ np . ndarray , np . ndarray ], random_state : np . random . RandomState , n_iter : int , n_jobs : int , verbose : int = 0 , lupi_features = None , kwargs : dict = None , ) -> Tuple [ InitModel , float ]: \"\"\" Search function which wraps `sklearns` `RandomizedSearchCV` function. We use distributions and parameters defined in the `model_template`. Parameters ---------- model_template : InitModel A model template which is used to fit data. hyperparameters : dict Dictionary of hyperparameters. data : tuple Tuple of data (X,y) random_state : RandomState numpy RandomState object n_iter : int Amount of search samples. n_jobs : int Allows multiprocessing with `n_jobs` threads. verbose : int Allows verbose output when `verbose>0`. lupi_features : int Amount of lupi_features kwargs : dict Placeholder, dict to pass into fit functions. \"\"\" if lupi_features > 0 : model = model_template ( lupi_features = lupi_features ) else : model = model_template () scorer , metric = model . make_scorer () if scorer is None : refit = True else : refit = metric searcher = RandomizedSearchCV ( model , hyperparameters , scoring = scorer , random_state = random_state , refit = refit , cv = 3 , n_iter = n_iter , n_jobs = n_jobs , error_score = np . nan , verbose = verbose , ) X , y = data # Ignore warnings for extremely bad model_state (when precision=0) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) searcher . fit ( X , y ) best_model : InitModel = searcher . best_estimator_ best_score = best_model . score ( X , y ) if verbose > 0 : print ( \"*\" * 20 , \"Best found baseline model\" , \"*\" * 20 ) pprint ( best_model ) print ( \"score: \" , best_score ) for k , v in best_model . constraints . items (): pprint (( f \"{k}: {v}\" )) for k , v in best_model . model_state . items (): if hasattr ( v , \"shape\" ): pprint (( f \"{k}: shape {v.shape}\" )) else : if \"slack\" in k : continue pprint (( f \"{k}: {v}\" )) print ( \"*\" * 30 ) return best_model , best_score","title":"Parameter Searcher"},{"location":"reference/fri/parameter_searcher/#module-friparameter_searcher","text":"In this class we use hyperparameter search to find parameters needed in our model. Depending on the input model we sample parameters from a random distribution. The sampling rate can be increased. The model with the best internally defined accuracy is picked. To increase robustness we use cross validation. View Source \"\"\" In this class we use hyperparameter search to find parameters needed in our model. Depending on the input model we sample parameters from a random distribution. The sampling rate can be increased. The model with the best internally defined accuracy is picked. To increase robustness we use cross validation. \"\"\" import warnings from sklearn.exceptions import FitFailedWarning warnings . filterwarnings ( action = \"ignore\" , category = FitFailedWarning ) from pprint import pprint from typing import Tuple import numpy as np from sklearn.model_selection import RandomizedSearchCV from fri.model.base_initmodel import InitModel def find_best_model ( model_template : InitModel , hyperparameters : dict , data : Tuple [ np . ndarray , np . ndarray ], random_state : np . random . RandomState , n_iter : int , n_jobs : int , verbose : int = 0 , lupi_features = None , kwargs : dict = None , ) -> Tuple [ InitModel , float ]: \"\"\" Search function which wraps `sklearns` `RandomizedSearchCV` function. We use distributions and parameters defined in the `model_template`. Parameters ---------- model_template : InitModel A model template which is used to fit data. hyperparameters : dict Dictionary of hyperparameters. data : tuple Tuple of data (X,y) random_state : RandomState numpy RandomState object n_iter : int Amount of search samples. n_jobs : int Allows multiprocessing with `n_jobs` threads. verbose : int Allows verbose output when `verbose>0`. lupi_features : int Amount of lupi_features kwargs : dict Placeholder, dict to pass into fit functions. \"\"\" if lupi_features > 0 : model = model_template ( lupi_features = lupi_features ) else : model = model_template () scorer , metric = model . make_scorer () if scorer is None : refit = True else : refit = metric searcher = RandomizedSearchCV ( model , hyperparameters , scoring = scorer , random_state = random_state , refit = refit , cv = 3 , n_iter = n_iter , n_jobs = n_jobs , error_score = np . nan , verbose = verbose , ) X , y = data # Ignore warnings for extremely bad model_state (when precision=0) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) searcher . fit ( X , y ) best_model : InitModel = searcher . best_estimator_ best_score = best_model . score ( X , y ) if verbose > 0 : print ( \"*\" * 20 , \"Best found baseline model\" , \"*\" * 20 ) pprint ( best_model ) print ( \"score: \" , best_score ) for k , v in best_model . constraints . items (): pprint (( f \" {k} : {v} \" )) for k , v in best_model . model_state . items (): if hasattr ( v , \"shape\" ): pprint (( f \" {k} : shape {v.shape} \" )) else : if \"slack\" in k : continue pprint (( f \" {k} : {v} \" )) print ( \"*\" * 30 ) return best_model , best_score","title":"Module fri.parameter_searcher"},{"location":"reference/fri/parameter_searcher/#functions","text":"","title":"Functions"},{"location":"reference/fri/parameter_searcher/#find_best_model","text":"def find_best_model ( model_template : fri . model . base_initmodel . InitModel , hyperparameters : dict , data : Tuple [ numpy . ndarray , numpy . ndarray ], random_state : numpy . random . mtrand . RandomState , n_iter : int , n_jobs : int , verbose : int = 0 , lupi_features = None , kwargs : dict = None ) -> Tuple [ fri . model . base_initmodel . InitModel , float ] Search function which wraps sklearns RandomizedSearchCV function. We use distributions and parameters defined in the model_template .","title":"find_best_model"},{"location":"reference/fri/parameter_searcher/#parameters","text":"model_template : InitModel A model template which is used to fit data. hyperparameters : dict Dictionary of hyperparameters. data : tuple Tuple of data (X,y) random_state : RandomState numpy RandomState object n_iter : int Amount of search samples. n_jobs : int Allows multiprocessing with n_jobs threads. verbose : int Allows verbose output when verbose>0 . lupi_features : int Amount of lupi_features kwargs : dict Placeholder, dict to pass into fit functions. View Source def find_best_model ( model_template : InitModel , hyperparameters : dict , data : Tuple [ np . ndarray , np . ndarray ], random_state : np . random . RandomState , n_iter : int , n_jobs : int , verbose : int = 0 , lupi_features = None , kwargs : dict = None , ) -> Tuple [ InitModel , float ]: \"\"\" Search function which wraps `sklearns` `RandomizedSearchCV` function. We use distributions and parameters defined in the `model_template`. Parameters ---------- model_template : InitModel A model template which is used to fit data. hyperparameters : dict Dictionary of hyperparameters. data : tuple Tuple of data (X,y) random_state : RandomState numpy RandomState object n_iter : int Amount of search samples. n_jobs : int Allows multiprocessing with `n_jobs` threads. verbose : int Allows verbose output when `verbose>0`. lupi_features : int Amount of lupi_features kwargs : dict Placeholder, dict to pass into fit functions. \"\"\" if lupi_features > 0 : model = model_template ( lupi_features = lupi_features ) else : model = model_template () scorer , metric = model . make_scorer () if scorer is None : refit = True else : refit = metric searcher = RandomizedSearchCV ( model , hyperparameters , scoring = scorer , random_state = random_state , refit = refit , cv = 3 , n_iter = n_iter , n_jobs = n_jobs , error_score = np . nan , verbose = verbose , ) X , y = data # Ignore warnings for extremely bad model_state (when precision=0) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) searcher . fit ( X , y ) best_model : InitModel = searcher . best_estimator_ best_score = best_model . score ( X , y ) if verbose > 0 : print ( \"*\" * 20 , \"Best found baseline model\" , \"*\" * 20 ) pprint ( best_model ) print ( \"score: \" , best_score ) for k , v in best_model . constraints . items (): pprint (( f \"{k}: {v}\" )) for k , v in best_model . model_state . items (): if hasattr ( v , \"shape\" ): pprint (( f \"{k}: shape {v.shape}\" )) else : if \"slack\" in k : continue pprint (( f \"{k}: {v}\" )) print ( \"*\" * 30 ) return best_model , best_score","title":"Parameters"},{"location":"reference/fri/plot/","text":"Module fri.plot View Source import matplotlib matplotlib . use ( \"TkAgg\" ) import matplotlib.patches as mpatches import matplotlib.pyplot as plt import numpy as np from scipy.cluster.hierarchy import dendrogram import matplotlib.cm as cm # Get a color for each relevance type color_palette_3 = cm . Set1 ([ 0 , 1 , 2 ], alpha = 0.8 ) def plot_relevance_bars ( ax , ranges , ticklabels = None , classes = None , numbering = True , tick_rotation = 30 ): \"\"\" Parameters ---------- ax: axis which the bars get drawn on ranges: the 2d array of floating values determining the lower and upper bounds of the bars ticklabels: (optional) labels for each feature classes: (optional) relevance class for each feature, determines color numbering: bool Add feature index when using ticklabels tick_rotation: int Amonut of rotation of ticklabels for easier readability. \"\"\" N = len ( ranges ) # Ticklabels if ticklabels is None : ticks = np . arange ( N ) + 1 else : ticks = list ( ticklabels ) if numbering : for i in range ( N ): ticks [ i ] += \" - {} \" . format ( i + 1 ) # Interval sizes ind = np . arange ( N ) + 1 width = 0.6 upper_vals = ranges [:, 1 ] lower_vals = ranges [:, 0 ] height = upper_vals - lower_vals # Minimal height to make very small intervals visible height [ height < 0.001 ] = 0.001 # Bar colors if classes is None : new_classes = np . zeros ( N ) . astype ( int ) color = [ color_palette_3 [ c . astype ( int )] for c in new_classes ] else : color = [ color_palette_3 [ c . astype ( int )] for c in classes ] # Plot the bars bars = ax . bar ( ind , height , width , bottom = lower_vals , tick_label = ticks , align = \"center\" , edgecolor = [ \"black\" ] * N , linewidth = 1.3 , color = color , ) ax . set_xticklabels ( ticks ) if ticklabels is not None : ax . set_xticklabels ( ax . get_xticklabels (), rotation = tick_rotation , ha = \"right\" ) # ax.tick_params(rotation=\"auto\") # Limit the y range to 0,1 or 0,L1 ax . set_ylim ([ 0 , max ( ranges [:, 1 ]) * 1.1 ]) ax . set_ylabel ( \"relevance\" ) ax . set_xlabel ( \"feature\" ) if classes is not None : relevance_classes = [ \"Irrelevant\" , \"Weakly relevant\" , \"Strongly relevant\" ] patches = [] for i , rc in enumerate ( relevance_classes ): patch = mpatches . Patch ( color = color_palette_3 [ i ], label = rc ) patches . append ( patch ) ax . legend ( handles = patches ) return bars def plotIntervals ( ranges , ticklabels = None , invert = False , classes = None ): # Figure Parameters fig = plt . figure () ax = fig . add_subplot ( 111 ) out = plot_relevance_bars ( ax , ranges , ticklabels = ticklabels , classes = classes ) fig . autofmt_xdate () # Invert the xaxis for cases in which the comparison with other tools if invert : plt . gca () . invert_xaxis () return fig def plot_dendrogram_and_intervals ( intervals , linkage , figsize = ( 13 , 7 ), ticklabels = None , classes = None , ** kwargs ): fig , ( ax2 , ax ) = plt . subplots ( 2 , 1 , figsize = figsize ) # Top dendrogram plot d = dendrogram ( linkage , color_threshold = 0 , leaf_rotation = 0.0 , # rotates the x axis labels leaf_font_size = 12.0 , # font size for the x axis labels ax = ax2 , ) # Get index determined through linkage method and dendrogram rearranged_index = d [ \"leaves\" ] ranges = intervals [ rearranged_index ] if ticklabels is None : ticks = np . array ( rearranged_index ) ticks += 1 # Index starting at 1 else : ticks = list ( ticklabels [ rearranged_index ]) for i in range ( len ( intervals )): ticks [ i ] += \" - {} \" . format ( rearranged_index [ i ] + 1 ) plot_relevance_bars ( ax , ranges , ticklabels = ticks , classes = classes [ rearranged_index ] if classes is not None else None , numbering = False , ** kwargs , ) fig . subplots_adjust ( hspace = 0 ) ax . margins ( x = 0 ) ax2 . set_xticks ([]) ax2 . margins ( x = 0 ) plt . tight_layout () return fig def plot_intervals ( model , ticklabels = None ): \"\"\"Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) \"\"\" if model . interval_ is not None : plotIntervals ( model . interval_ , ticklabels = ticklabels , classes = model . relevance_classes_ ) else : print ( \"Intervals not computed. Try running fit() function first.\" ) def plot_lupi_intervals ( model , ticklabels = None , lupi_ticklabels = None ): \"\"\"Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) lupi_ticklabels : list of str, optional Strs for lupi ticklabels on x-axis (lupi features) \"\"\" n_features = model . interval_ . shape [ 0 ] - model . lupi_features_ data_interval_ = model . interval_ [ 0 : n_features , :] lupi_interval_ = model . interval_ [ n_features :, :] data_relevance_classes_ = model . relevance_classes_ [ 0 : n_features ] lupi_relevance_classes_ = model . relevance_classes_ [ n_features :] if model . interval_ is not None : plotIntervals ( data_interval_ , ticklabels = ticklabels , classes = data_relevance_classes_ ) plotIntervals ( lupi_interval_ , ticklabels = lupi_ticklabels , classes = lupi_relevance_classes_ ) else : print ( \"Intervals not computed. Try running fit() function first.\" ) # # def interactive_scatter_embed(embedding, mode=\"markers\", txt=None): # # TODO: extend method # import plotly.graph_objs as go # from plotly.offline import init_notebook_mode, iplot # init_notebook_mode(connected=True) # # Create a trace # trace = go.Scatter( # x=embedding[:, 0], # y=embedding[:, 1], # mode=mode, # text=txt if mode is \"text\" else None # ) # # data = [trace] # # # Plot and embed in ipython notebook! # iplot(data) Variables color_palette_3 Functions plotIntervals def plotIntervals ( ranges , ticklabels = None , invert = False , classes = None ) View Source def plotIntervals ( ranges , ticklabels = None , invert = False , classes = None ): # Figure Parameters fig = plt . figure () ax = fig . add_subplot ( 111 ) out = plot_relevance_bars ( ax , ranges , ticklabels = ticklabels , classes = classes ) fig . autofmt_xdate () # Invert the xaxis for cases in which the comparison with other tools if invert : plt . gca (). invert_xaxis () return fig plot_dendrogram_and_intervals def plot_dendrogram_and_intervals ( intervals , linkage , figsize = ( 13 , 7 ), ticklabels = None , classes = None , ** kwargs ) View Source def plot_dendrogram_and_intervals ( intervals , linkage , figsize = ( 13 , 7 ), ticklabels = None , classes = None , ** kwargs ) : fig , ( ax2 , ax ) = plt . subplots ( 2 , 1 , figsize = figsize ) # Top dendrogram plot d = dendrogram ( linkage , color_threshold = 0 , leaf_rotation = 0.0 , # rotates the x axis labels leaf_font_size = 12.0 , # font size for the x axis labels ax = ax2 , ) # Get index determined through linkage method and dendrogram rearranged_index = d [ \"leaves\" ] ranges = intervals [ rearranged_index ] if ticklabels is None : ticks = np . array ( rearranged_index ) ticks += 1 # Index starting at 1 else : ticks = list ( ticklabels [ rearranged_index ] ) for i in range ( len ( intervals )) : ticks [ i ] += \" - {}\" . format ( rearranged_index [ i ] + 1 ) plot_relevance_bars ( ax , ranges , ticklabels = ticks , classes = classes [ rearranged_index ] if classes is not None else None , numbering = False , ** kwargs , ) fig . subplots_adjust ( hspace = 0 ) ax . margins ( x = 0 ) ax2 . set_xticks ( [] ) ax2 . margins ( x = 0 ) plt . tight_layout () return fig plot_intervals def plot_intervals ( model , ticklabels = None ) Plot the relevance intervals. Parameters model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) View Source def plot_intervals ( model , ticklabels = None ): \"\"\"Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) \"\"\" if model . interval_ is not None : plotIntervals ( model . interval_ , ticklabels = ticklabels , classes = model . relevance_classes_ ) else : print ( \"Intervals not computed. Try running fit() function first.\" ) plot_lupi_intervals def plot_lupi_intervals ( model , ticklabels = None , lupi_ticklabels = None ) Plot the relevance intervals. Parameters model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) lupi_ticklabels : list of str, optional Strs for lupi ticklabels on x-axis (lupi features) View Source def plot_lupi_intervals ( model , ticklabels = None , lupi_ticklabels = None ): \"\"\"Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) lupi_ticklabels : list of str, optional Strs for lupi ticklabels on x-axis (lupi features) \"\"\" n_features = model . interval_ . shape [ 0 ] - model . lupi_features_ data_interval_ = model . interval_ [ 0 : n_features , :] lupi_interval_ = model . interval_ [ n_features :, :] data_relevance_classes_ = model . relevance_classes_ [ 0 : n_features ] lupi_relevance_classes_ = model . relevance_classes_ [ n_features :] if model . interval_ is not None : plotIntervals ( data_interval_ , ticklabels = ticklabels , classes = data_relevance_classes_ ) plotIntervals ( lupi_interval_ , ticklabels = lupi_ticklabels , classes = lupi_relevance_classes_ ) else : print ( \"Intervals not computed. Try running fit() function first.\" ) plot_relevance_bars def plot_relevance_bars ( ax , ranges , ticklabels = None , classes = None , numbering = True , tick_rotation = 30 ) Parameters ax: axis which the bars get drawn on ranges: the 2d array of floating values determining the lower and upper bounds of the bars ticklabels: (optional) labels for each feature classes: (optional) relevance class for each feature, determines color numbering: bool Add feature index when using ticklabels tick_rotation: int Amonut of rotation of ticklabels for easier readability. View Source def plot_relevance_bars ( ax , ranges , ticklabels = None , classes = None , numbering = True , tick_rotation = 30 ) : \"\"\" Parameters ---------- ax: axis which the bars get drawn on ranges: the 2d array of floating values determining the lower and upper bounds of the bars ticklabels: (optional) labels for each feature classes: (optional) relevance class for each feature, determines color numbering: bool Add feature index when using ticklabels tick_rotation: int Amonut of rotation of ticklabels for easier readability. \"\"\" N = len ( ranges ) # Ticklabels if ticklabels is None : ticks = np . arange ( N ) + 1 else : ticks = list ( ticklabels ) if numbering : for i in range ( N ) : ticks [ i ] += \" - {}\" . format ( i + 1 ) # Interval sizes ind = np . arange ( N ) + 1 width = 0.6 upper_vals = ranges [ :, 1 ] lower_vals = ranges [ :, 0 ] height = upper_vals - lower_vals # Minimal height to make very small intervals visible height [ height < 0.001 ] = 0.001 # Bar colors if classes is None : new_classes = np . zeros ( N ). astype ( int ) color = [ color_palette_3[c.astype(int) ] for c in new_classes ] else : color = [ color_palette_3[c.astype(int) ] for c in classes ] # Plot the bars bars = ax . bar ( ind , height , width , bottom = lower_vals , tick_label = ticks , align = \"center\" , edgecolor =[ \"black\" ] * N , linewidth = 1.3 , color = color , ) ax . set_xticklabels ( ticks ) if ticklabels is not None : ax . set_xticklabels ( ax . get_xticklabels (), rotation = tick_rotation , ha = \"right\" ) # ax . tick_params ( rotation = \"auto\" ) # Limit the y range to 0 , 1 or 0 , L1 ax . set_ylim ( [ 0, max(ranges[:, 1 ] ) * 1.1 ] ) ax . set_ylabel ( \"relevance\" ) ax . set_xlabel ( \"feature\" ) if classes is not None : relevance_classes = [ \"Irrelevant\", \"Weakly relevant\", \"Strongly relevant\" ] patches = [] for i , rc in enumerate ( relevance_classes ) : patch = mpatches . Patch ( color = color_palette_3 [ i ] , label = rc ) patches . append ( patch ) ax . legend ( handles = patches ) return bars","title":"Plot"},{"location":"reference/fri/plot/#module-friplot","text":"View Source import matplotlib matplotlib . use ( \"TkAgg\" ) import matplotlib.patches as mpatches import matplotlib.pyplot as plt import numpy as np from scipy.cluster.hierarchy import dendrogram import matplotlib.cm as cm # Get a color for each relevance type color_palette_3 = cm . Set1 ([ 0 , 1 , 2 ], alpha = 0.8 ) def plot_relevance_bars ( ax , ranges , ticklabels = None , classes = None , numbering = True , tick_rotation = 30 ): \"\"\" Parameters ---------- ax: axis which the bars get drawn on ranges: the 2d array of floating values determining the lower and upper bounds of the bars ticklabels: (optional) labels for each feature classes: (optional) relevance class for each feature, determines color numbering: bool Add feature index when using ticklabels tick_rotation: int Amonut of rotation of ticklabels for easier readability. \"\"\" N = len ( ranges ) # Ticklabels if ticklabels is None : ticks = np . arange ( N ) + 1 else : ticks = list ( ticklabels ) if numbering : for i in range ( N ): ticks [ i ] += \" - {} \" . format ( i + 1 ) # Interval sizes ind = np . arange ( N ) + 1 width = 0.6 upper_vals = ranges [:, 1 ] lower_vals = ranges [:, 0 ] height = upper_vals - lower_vals # Minimal height to make very small intervals visible height [ height < 0.001 ] = 0.001 # Bar colors if classes is None : new_classes = np . zeros ( N ) . astype ( int ) color = [ color_palette_3 [ c . astype ( int )] for c in new_classes ] else : color = [ color_palette_3 [ c . astype ( int )] for c in classes ] # Plot the bars bars = ax . bar ( ind , height , width , bottom = lower_vals , tick_label = ticks , align = \"center\" , edgecolor = [ \"black\" ] * N , linewidth = 1.3 , color = color , ) ax . set_xticklabels ( ticks ) if ticklabels is not None : ax . set_xticklabels ( ax . get_xticklabels (), rotation = tick_rotation , ha = \"right\" ) # ax.tick_params(rotation=\"auto\") # Limit the y range to 0,1 or 0,L1 ax . set_ylim ([ 0 , max ( ranges [:, 1 ]) * 1.1 ]) ax . set_ylabel ( \"relevance\" ) ax . set_xlabel ( \"feature\" ) if classes is not None : relevance_classes = [ \"Irrelevant\" , \"Weakly relevant\" , \"Strongly relevant\" ] patches = [] for i , rc in enumerate ( relevance_classes ): patch = mpatches . Patch ( color = color_palette_3 [ i ], label = rc ) patches . append ( patch ) ax . legend ( handles = patches ) return bars def plotIntervals ( ranges , ticklabels = None , invert = False , classes = None ): # Figure Parameters fig = plt . figure () ax = fig . add_subplot ( 111 ) out = plot_relevance_bars ( ax , ranges , ticklabels = ticklabels , classes = classes ) fig . autofmt_xdate () # Invert the xaxis for cases in which the comparison with other tools if invert : plt . gca () . invert_xaxis () return fig def plot_dendrogram_and_intervals ( intervals , linkage , figsize = ( 13 , 7 ), ticklabels = None , classes = None , ** kwargs ): fig , ( ax2 , ax ) = plt . subplots ( 2 , 1 , figsize = figsize ) # Top dendrogram plot d = dendrogram ( linkage , color_threshold = 0 , leaf_rotation = 0.0 , # rotates the x axis labels leaf_font_size = 12.0 , # font size for the x axis labels ax = ax2 , ) # Get index determined through linkage method and dendrogram rearranged_index = d [ \"leaves\" ] ranges = intervals [ rearranged_index ] if ticklabels is None : ticks = np . array ( rearranged_index ) ticks += 1 # Index starting at 1 else : ticks = list ( ticklabels [ rearranged_index ]) for i in range ( len ( intervals )): ticks [ i ] += \" - {} \" . format ( rearranged_index [ i ] + 1 ) plot_relevance_bars ( ax , ranges , ticklabels = ticks , classes = classes [ rearranged_index ] if classes is not None else None , numbering = False , ** kwargs , ) fig . subplots_adjust ( hspace = 0 ) ax . margins ( x = 0 ) ax2 . set_xticks ([]) ax2 . margins ( x = 0 ) plt . tight_layout () return fig def plot_intervals ( model , ticklabels = None ): \"\"\"Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) \"\"\" if model . interval_ is not None : plotIntervals ( model . interval_ , ticklabels = ticklabels , classes = model . relevance_classes_ ) else : print ( \"Intervals not computed. Try running fit() function first.\" ) def plot_lupi_intervals ( model , ticklabels = None , lupi_ticklabels = None ): \"\"\"Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) lupi_ticklabels : list of str, optional Strs for lupi ticklabels on x-axis (lupi features) \"\"\" n_features = model . interval_ . shape [ 0 ] - model . lupi_features_ data_interval_ = model . interval_ [ 0 : n_features , :] lupi_interval_ = model . interval_ [ n_features :, :] data_relevance_classes_ = model . relevance_classes_ [ 0 : n_features ] lupi_relevance_classes_ = model . relevance_classes_ [ n_features :] if model . interval_ is not None : plotIntervals ( data_interval_ , ticklabels = ticklabels , classes = data_relevance_classes_ ) plotIntervals ( lupi_interval_ , ticklabels = lupi_ticklabels , classes = lupi_relevance_classes_ ) else : print ( \"Intervals not computed. Try running fit() function first.\" ) # # def interactive_scatter_embed(embedding, mode=\"markers\", txt=None): # # TODO: extend method # import plotly.graph_objs as go # from plotly.offline import init_notebook_mode, iplot # init_notebook_mode(connected=True) # # Create a trace # trace = go.Scatter( # x=embedding[:, 0], # y=embedding[:, 1], # mode=mode, # text=txt if mode is \"text\" else None # ) # # data = [trace] # # # Plot and embed in ipython notebook! # iplot(data)","title":"Module fri.plot"},{"location":"reference/fri/plot/#variables","text":"color_palette_3","title":"Variables"},{"location":"reference/fri/plot/#functions","text":"","title":"Functions"},{"location":"reference/fri/plot/#plotintervals","text":"def plotIntervals ( ranges , ticklabels = None , invert = False , classes = None ) View Source def plotIntervals ( ranges , ticklabels = None , invert = False , classes = None ): # Figure Parameters fig = plt . figure () ax = fig . add_subplot ( 111 ) out = plot_relevance_bars ( ax , ranges , ticklabels = ticklabels , classes = classes ) fig . autofmt_xdate () # Invert the xaxis for cases in which the comparison with other tools if invert : plt . gca (). invert_xaxis () return fig","title":"plotIntervals"},{"location":"reference/fri/plot/#plot_dendrogram_and_intervals","text":"def plot_dendrogram_and_intervals ( intervals , linkage , figsize = ( 13 , 7 ), ticklabels = None , classes = None , ** kwargs ) View Source def plot_dendrogram_and_intervals ( intervals , linkage , figsize = ( 13 , 7 ), ticklabels = None , classes = None , ** kwargs ) : fig , ( ax2 , ax ) = plt . subplots ( 2 , 1 , figsize = figsize ) # Top dendrogram plot d = dendrogram ( linkage , color_threshold = 0 , leaf_rotation = 0.0 , # rotates the x axis labels leaf_font_size = 12.0 , # font size for the x axis labels ax = ax2 , ) # Get index determined through linkage method and dendrogram rearranged_index = d [ \"leaves\" ] ranges = intervals [ rearranged_index ] if ticklabels is None : ticks = np . array ( rearranged_index ) ticks += 1 # Index starting at 1 else : ticks = list ( ticklabels [ rearranged_index ] ) for i in range ( len ( intervals )) : ticks [ i ] += \" - {}\" . format ( rearranged_index [ i ] + 1 ) plot_relevance_bars ( ax , ranges , ticklabels = ticks , classes = classes [ rearranged_index ] if classes is not None else None , numbering = False , ** kwargs , ) fig . subplots_adjust ( hspace = 0 ) ax . margins ( x = 0 ) ax2 . set_xticks ( [] ) ax2 . margins ( x = 0 ) plt . tight_layout () return fig","title":"plot_dendrogram_and_intervals"},{"location":"reference/fri/plot/#plot_intervals","text":"def plot_intervals ( model , ticklabels = None ) Plot the relevance intervals.","title":"plot_intervals"},{"location":"reference/fri/plot/#parameters","text":"model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) View Source def plot_intervals ( model , ticklabels = None ): \"\"\"Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) \"\"\" if model . interval_ is not None : plotIntervals ( model . interval_ , ticklabels = ticklabels , classes = model . relevance_classes_ ) else : print ( \"Intervals not computed. Try running fit() function first.\" )","title":"Parameters"},{"location":"reference/fri/plot/#plot_lupi_intervals","text":"def plot_lupi_intervals ( model , ticklabels = None , lupi_ticklabels = None ) Plot the relevance intervals.","title":"plot_lupi_intervals"},{"location":"reference/fri/plot/#parameters_1","text":"model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) lupi_ticklabels : list of str, optional Strs for lupi ticklabels on x-axis (lupi features) View Source def plot_lupi_intervals ( model , ticklabels = None , lupi_ticklabels = None ): \"\"\"Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) lupi_ticklabels : list of str, optional Strs for lupi ticklabels on x-axis (lupi features) \"\"\" n_features = model . interval_ . shape [ 0 ] - model . lupi_features_ data_interval_ = model . interval_ [ 0 : n_features , :] lupi_interval_ = model . interval_ [ n_features :, :] data_relevance_classes_ = model . relevance_classes_ [ 0 : n_features ] lupi_relevance_classes_ = model . relevance_classes_ [ n_features :] if model . interval_ is not None : plotIntervals ( data_interval_ , ticklabels = ticklabels , classes = data_relevance_classes_ ) plotIntervals ( lupi_interval_ , ticklabels = lupi_ticklabels , classes = lupi_relevance_classes_ ) else : print ( \"Intervals not computed. Try running fit() function first.\" )","title":"Parameters"},{"location":"reference/fri/plot/#plot_relevance_bars","text":"def plot_relevance_bars ( ax , ranges , ticklabels = None , classes = None , numbering = True , tick_rotation = 30 )","title":"plot_relevance_bars"},{"location":"reference/fri/plot/#parameters_2","text":"ax: axis which the bars get drawn on ranges: the 2d array of floating values determining the lower and upper bounds of the bars ticklabels: (optional) labels for each feature classes: (optional) relevance class for each feature, determines color numbering: bool Add feature index when using ticklabels tick_rotation: int Amonut of rotation of ticklabels for easier readability. View Source def plot_relevance_bars ( ax , ranges , ticklabels = None , classes = None , numbering = True , tick_rotation = 30 ) : \"\"\" Parameters ---------- ax: axis which the bars get drawn on ranges: the 2d array of floating values determining the lower and upper bounds of the bars ticklabels: (optional) labels for each feature classes: (optional) relevance class for each feature, determines color numbering: bool Add feature index when using ticklabels tick_rotation: int Amonut of rotation of ticklabels for easier readability. \"\"\" N = len ( ranges ) # Ticklabels if ticklabels is None : ticks = np . arange ( N ) + 1 else : ticks = list ( ticklabels ) if numbering : for i in range ( N ) : ticks [ i ] += \" - {}\" . format ( i + 1 ) # Interval sizes ind = np . arange ( N ) + 1 width = 0.6 upper_vals = ranges [ :, 1 ] lower_vals = ranges [ :, 0 ] height = upper_vals - lower_vals # Minimal height to make very small intervals visible height [ height < 0.001 ] = 0.001 # Bar colors if classes is None : new_classes = np . zeros ( N ). astype ( int ) color = [ color_palette_3[c.astype(int) ] for c in new_classes ] else : color = [ color_palette_3[c.astype(int) ] for c in classes ] # Plot the bars bars = ax . bar ( ind , height , width , bottom = lower_vals , tick_label = ticks , align = \"center\" , edgecolor =[ \"black\" ] * N , linewidth = 1.3 , color = color , ) ax . set_xticklabels ( ticks ) if ticklabels is not None : ax . set_xticklabels ( ax . get_xticklabels (), rotation = tick_rotation , ha = \"right\" ) # ax . tick_params ( rotation = \"auto\" ) # Limit the y range to 0 , 1 or 0 , L1 ax . set_ylim ( [ 0, max(ranges[:, 1 ] ) * 1.1 ] ) ax . set_ylabel ( \"relevance\" ) ax . set_xlabel ( \"feature\" ) if classes is not None : relevance_classes = [ \"Irrelevant\", \"Weakly relevant\", \"Strongly relevant\" ] patches = [] for i , rc in enumerate ( relevance_classes ) : patch = mpatches . Patch ( color = color_palette_3 [ i ] , label = rc ) patches . append ( patch ) ax . legend ( handles = patches ) return bars","title":"Parameters"},{"location":"reference/fri/utils/","text":"Module fri.utils View Source import numpy as np def distance ( u , v ): \"\"\" Distance measure custom made for feature comparison. Parameters ---------- u: first feature v: second feature Returns ------- \"\"\" u = np . asarray ( u ) v = np . asarray ( v ) # Euclidean differences diff = ( u - v ) ** 2 # Nullify pairwise contribution diff [ u == 0 ] = 0 diff [ v == 0 ] = 0 return np . sqrt ( np . sum ( diff )) def permutate_feature_in_data ( data , feature_i , random_state ): X , y = data X_copy = np . copy ( X ) # Permute selected feature permutated_feature = random_state . permutation ( X_copy [:, feature_i ]) # Add permutation back to dataset X_copy [:, feature_i ] = permutated_feature return X_copy , y Functions distance def distance ( u , v ) Distance measure custom made for feature comparison. Parameters u: first feature v: second feature Returns View Source def distance ( u , v ): \"\"\" Distance measure custom made for feature comparison. Parameters ---------- u: first feature v: second feature Returns ------- \"\"\" u = np . asarray ( u ) v = np . asarray ( v ) # Euclidean differences diff = ( u - v ) ** 2 # Nullify pairwise contribution diff [ u == 0 ] = 0 diff [ v == 0 ] = 0 return np . sqrt ( np . sum ( diff )) permutate_feature_in_data def permutate_feature_in_data ( data , feature_i , random_state ) View Source def permutate_feature_in_data ( data , feature_i , random_state ): X , y = data X_copy = np . copy ( X ) # Permute selected feature permutated_feature = random_state . permutation ( X_copy [:, feature_i ]) # Add permutation back to dataset X_copy [:, feature_i ] = permutated_feature return X_copy , y","title":"Utils"},{"location":"reference/fri/utils/#module-friutils","text":"View Source import numpy as np def distance ( u , v ): \"\"\" Distance measure custom made for feature comparison. Parameters ---------- u: first feature v: second feature Returns ------- \"\"\" u = np . asarray ( u ) v = np . asarray ( v ) # Euclidean differences diff = ( u - v ) ** 2 # Nullify pairwise contribution diff [ u == 0 ] = 0 diff [ v == 0 ] = 0 return np . sqrt ( np . sum ( diff )) def permutate_feature_in_data ( data , feature_i , random_state ): X , y = data X_copy = np . copy ( X ) # Permute selected feature permutated_feature = random_state . permutation ( X_copy [:, feature_i ]) # Add permutation back to dataset X_copy [:, feature_i ] = permutated_feature return X_copy , y","title":"Module fri.utils"},{"location":"reference/fri/utils/#functions","text":"","title":"Functions"},{"location":"reference/fri/utils/#distance","text":"def distance ( u , v ) Distance measure custom made for feature comparison.","title":"distance"},{"location":"reference/fri/utils/#parameters","text":"u: first feature v: second feature","title":"Parameters"},{"location":"reference/fri/utils/#returns","text":"View Source def distance ( u , v ): \"\"\" Distance measure custom made for feature comparison. Parameters ---------- u: first feature v: second feature Returns ------- \"\"\" u = np . asarray ( u ) v = np . asarray ( v ) # Euclidean differences diff = ( u - v ) ** 2 # Nullify pairwise contribution diff [ u == 0 ] = 0 diff [ v == 0 ] = 0 return np . sqrt ( np . sum ( diff ))","title":"Returns"},{"location":"reference/fri/utils/#permutate_feature_in_data","text":"def permutate_feature_in_data ( data , feature_i , random_state ) View Source def permutate_feature_in_data ( data , feature_i , random_state ): X , y = data X_copy = np . copy ( X ) # Permute selected feature permutated_feature = random_state . permutation ( X_copy [:, feature_i ]) # Add permutation back to dataset X_copy [:, feature_i ] = permutated_feature return X_copy , y","title":"permutate_feature_in_data"},{"location":"reference/fri/model/","text":"Module fri.model View Source from .classification import Classification from .lupi_classification import LUPI_Classification from .lupi_ordinal_regression import LUPI_OrdinalRegression from .lupi_regression import LUPI_Regression from .ordinal_regression import OrdinalRegression from .regression import Regression __all__ = [ \"Classification\" , \"Regression\" , \"OrdinalRegression\" , \"LUPI_Classification\" , \"LUPI_Regression\" , \"LUPI_OrdinalRegression\" , ] Sub-modules fri.model.base_cvxproblem fri.model.base_initmodel fri.model.base_lupi fri.model.base_type fri.model.classification fri.model.lupi_classification fri.model.lupi_ordinal_regression fri.model.lupi_regression fri.model.ordinal_regression fri.model.regression Classes Classification class Classification ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Classification ( ProblemType ) : @classmethod def parameters ( cls ) : return [ \"C\" ] @property def get_initmodel_template ( cls ) : return Classification_SVM @property def get_cvxproblem_template ( cls ) : return Classification_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 return X , y Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods parameters def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\" ] Instance variables get_cvxproblem_template get_initmodel_template Methods get_all_parameters def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } get_all_relax_factors def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } get_chosen_parameter def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) get_chosen_relax_factors def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor get_relaxed_constraints def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } postprocessing def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds preprocessing def preprocessing ( self , data , ** kwargs ) View Source def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 return X , y relax_constraint def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key )) relax_factors def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] LUPI_Classification class LUPI_Classification ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Classification ( ProblemType ) : def __init__ ( self , ** kwargs ) : super (). __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ) : return self . _lupi_features @classmethod def parameters ( cls ) : return [ \"C\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ] @property def get_initmodel_template ( cls ) : return LUPI_Classification_SVM @property def get_cvxproblem_template ( cls ) : return LUPI_Classification_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 return X , y Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods parameters def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ] Instance variables get_cvxproblem_template get_initmodel_template lupi_features Methods get_all_parameters def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } get_all_relax_factors def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } get_chosen_parameter def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) get_chosen_relax_factors def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor get_relaxed_constraints def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } postprocessing def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds preprocessing def preprocessing ( self , data , lupi_features = None ) View Source def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 return X , y relax_constraint def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key )) relax_factors def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] LUPI_OrdinalRegression class LUPI_OrdinalRegression ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_OrdinalRegression ( ProblemType ) : def __init__ ( self , ** kwargs ) : super (). __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ) : return self . _lupi_features @classmethod def parameters ( cls ) : return [ \"C\", \"scaling_lupi_w\" ] @property def get_initmodel_template ( cls ) : return LUPI_OrdinalRegression_SVM @property def get_cvxproblem_template ( cls ) : return LUPI_OrdinalRegression_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods parameters def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\", \"scaling_lupi_w\" ] Instance variables get_cvxproblem_template get_initmodel_template lupi_features Methods get_all_parameters def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } get_all_relax_factors def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } get_chosen_parameter def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) get_chosen_relax_factors def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor get_relaxed_constraints def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } postprocessing def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds preprocessing def preprocessing ( self , data , lupi_features = None ) View Source def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y relax_constraint def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key )) relax_factors def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] LUPI_Regression class LUPI_Regression ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Regression ( ProblemType ) : def __init__ ( self , ** kwargs ) : super (). __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ) : return self . _lupi_features @classmethod def parameters ( cls ) : return [ \"C\", \"epsilon\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ] @property def get_initmodel_template ( cls ) : return LUPI_Regression_SVM @property def get_cvxproblem_template ( cls ) : return LUPI_Regression_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods parameters def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\", \"epsilon\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ] Instance variables get_cvxproblem_template get_initmodel_template lupi_features Methods get_all_parameters def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } get_all_relax_factors def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } get_chosen_parameter def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) get_chosen_relax_factors def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor get_relaxed_constraints def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } postprocessing def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds preprocessing def preprocessing ( self , data , lupi_features = None ) View Source def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y relax_constraint def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key )) relax_factors def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] OrdinalRegression class OrdinalRegression ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class OrdinalRegression ( ProblemType ) : @classmethod def parameters ( cls ) : return [ \"C\" ] @property def get_initmodel_template ( cls ) : return OrdinalRegression_SVM @property def get_cvxproblem_template ( cls ) : return OrdinalRegression_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods parameters def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\" ] Instance variables get_cvxproblem_template get_initmodel_template Methods get_all_parameters def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } get_all_relax_factors def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } get_chosen_parameter def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) get_chosen_relax_factors def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor get_relaxed_constraints def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } postprocessing def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds preprocessing def preprocessing ( self , data , ** kwargs ) View Source def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y relax_constraint def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key )) relax_factors def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] Regression class Regression ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Regression ( ProblemType ) : @classmethod def parameters ( cls ) : return [ \"C\", \"epsilon\" ] @property def get_initmodel_template ( cls ) : return Regression_SVR @property def get_cvxproblem_template ( cls ) : return Regression_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods parameters def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\", \"epsilon\" ] Instance variables get_cvxproblem_template get_initmodel_template Methods get_all_parameters def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } get_all_relax_factors def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } get_chosen_parameter def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) get_chosen_relax_factors def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor get_relaxed_constraints def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } postprocessing def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds preprocessing def preprocessing ( self , data , ** kwargs ) View Source def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y relax_constraint def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key )) relax_factors def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ]","title":"Index"},{"location":"reference/fri/model/#module-frimodel","text":"View Source from .classification import Classification from .lupi_classification import LUPI_Classification from .lupi_ordinal_regression import LUPI_OrdinalRegression from .lupi_regression import LUPI_Regression from .ordinal_regression import OrdinalRegression from .regression import Regression __all__ = [ \"Classification\" , \"Regression\" , \"OrdinalRegression\" , \"LUPI_Classification\" , \"LUPI_Regression\" , \"LUPI_OrdinalRegression\" , ]","title":"Module fri.model"},{"location":"reference/fri/model/#sub-modules","text":"fri.model.base_cvxproblem fri.model.base_initmodel fri.model.base_lupi fri.model.base_type fri.model.classification fri.model.lupi_classification fri.model.lupi_ordinal_regression fri.model.lupi_regression fri.model.ordinal_regression fri.model.regression","title":"Sub-modules"},{"location":"reference/fri/model/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/#classification","text":"class Classification ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Classification ( ProblemType ) : @classmethod def parameters ( cls ) : return [ \"C\" ] @property def get_initmodel_template ( cls ) : return Classification_SVM @property def get_cvxproblem_template ( cls ) : return Classification_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 return X , y","title":"Classification"},{"location":"reference/fri/model/#ancestors-in-mro","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/#static-methods","text":"","title":"Static methods"},{"location":"reference/fri/model/#parameters","text":"def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\" ]","title":"parameters"},{"location":"reference/fri/model/#instance-variables","text":"get_cvxproblem_template get_initmodel_template","title":"Instance variables"},{"location":"reference/fri/model/#methods","text":"","title":"Methods"},{"location":"reference/fri/model/#get_all_parameters","text":"def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () }","title":"get_all_parameters"},{"location":"reference/fri/model/#get_all_relax_factors","text":"def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () }","title":"get_all_relax_factors"},{"location":"reference/fri/model/#get_chosen_parameter","text":"def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 )","title":"get_chosen_parameter"},{"location":"reference/fri/model/#get_chosen_relax_factors","text":"def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor","title":"get_chosen_relax_factors"},{"location":"reference/fri/model/#get_relaxed_constraints","text":"def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () }","title":"get_relaxed_constraints"},{"location":"reference/fri/model/#postprocessing","text":"def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds","title":"postprocessing"},{"location":"reference/fri/model/#preprocessing","text":"def preprocessing ( self , data , ** kwargs ) View Source def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 return X , y","title":"preprocessing"},{"location":"reference/fri/model/#relax_constraint","text":"def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"relax_constraint"},{"location":"reference/fri/model/#relax_factors","text":"def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ]","title":"relax_factors"},{"location":"reference/fri/model/#lupi_classification","text":"class LUPI_Classification ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Classification ( ProblemType ) : def __init__ ( self , ** kwargs ) : super (). __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ) : return self . _lupi_features @classmethod def parameters ( cls ) : return [ \"C\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ] @property def get_initmodel_template ( cls ) : return LUPI_Classification_SVM @property def get_cvxproblem_template ( cls ) : return LUPI_Classification_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 return X , y","title":"LUPI_Classification"},{"location":"reference/fri/model/#ancestors-in-mro_1","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/fri/model/#parameters_1","text":"def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ]","title":"parameters"},{"location":"reference/fri/model/#instance-variables_1","text":"get_cvxproblem_template get_initmodel_template lupi_features","title":"Instance variables"},{"location":"reference/fri/model/#methods_1","text":"","title":"Methods"},{"location":"reference/fri/model/#get_all_parameters_1","text":"def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () }","title":"get_all_parameters"},{"location":"reference/fri/model/#get_all_relax_factors_1","text":"def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () }","title":"get_all_relax_factors"},{"location":"reference/fri/model/#get_chosen_parameter_1","text":"def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 )","title":"get_chosen_parameter"},{"location":"reference/fri/model/#get_chosen_relax_factors_1","text":"def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor","title":"get_chosen_relax_factors"},{"location":"reference/fri/model/#get_relaxed_constraints_1","text":"def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () }","title":"get_relaxed_constraints"},{"location":"reference/fri/model/#postprocessing_1","text":"def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds","title":"postprocessing"},{"location":"reference/fri/model/#preprocessing_1","text":"def preprocessing ( self , data , lupi_features = None ) View Source def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 return X , y","title":"preprocessing"},{"location":"reference/fri/model/#relax_constraint_1","text":"def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"relax_constraint"},{"location":"reference/fri/model/#relax_factors_1","text":"def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ]","title":"relax_factors"},{"location":"reference/fri/model/#lupi_ordinalregression","text":"class LUPI_OrdinalRegression ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_OrdinalRegression ( ProblemType ) : def __init__ ( self , ** kwargs ) : super (). __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ) : return self . _lupi_features @classmethod def parameters ( cls ) : return [ \"C\", \"scaling_lupi_w\" ] @property def get_initmodel_template ( cls ) : return LUPI_OrdinalRegression_SVM @property def get_cvxproblem_template ( cls ) : return LUPI_OrdinalRegression_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y","title":"LUPI_OrdinalRegression"},{"location":"reference/fri/model/#ancestors-in-mro_2","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/fri/model/#parameters_2","text":"def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\", \"scaling_lupi_w\" ]","title":"parameters"},{"location":"reference/fri/model/#instance-variables_2","text":"get_cvxproblem_template get_initmodel_template lupi_features","title":"Instance variables"},{"location":"reference/fri/model/#methods_2","text":"","title":"Methods"},{"location":"reference/fri/model/#get_all_parameters_2","text":"def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () }","title":"get_all_parameters"},{"location":"reference/fri/model/#get_all_relax_factors_2","text":"def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () }","title":"get_all_relax_factors"},{"location":"reference/fri/model/#get_chosen_parameter_2","text":"def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 )","title":"get_chosen_parameter"},{"location":"reference/fri/model/#get_chosen_relax_factors_2","text":"def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor","title":"get_chosen_relax_factors"},{"location":"reference/fri/model/#get_relaxed_constraints_2","text":"def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () }","title":"get_relaxed_constraints"},{"location":"reference/fri/model/#postprocessing_2","text":"def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds","title":"postprocessing"},{"location":"reference/fri/model/#preprocessing_2","text":"def preprocessing ( self , data , lupi_features = None ) View Source def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y","title":"preprocessing"},{"location":"reference/fri/model/#relax_constraint_2","text":"def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"relax_constraint"},{"location":"reference/fri/model/#relax_factors_2","text":"def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ]","title":"relax_factors"},{"location":"reference/fri/model/#lupi_regression","text":"class LUPI_Regression ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Regression ( ProblemType ) : def __init__ ( self , ** kwargs ) : super (). __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ) : return self . _lupi_features @classmethod def parameters ( cls ) : return [ \"C\", \"epsilon\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ] @property def get_initmodel_template ( cls ) : return LUPI_Regression_SVM @property def get_cvxproblem_template ( cls ) : return LUPI_Regression_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y","title":"LUPI_Regression"},{"location":"reference/fri/model/#ancestors-in-mro_3","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/fri/model/#parameters_3","text":"def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\", \"epsilon\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ]","title":"parameters"},{"location":"reference/fri/model/#instance-variables_3","text":"get_cvxproblem_template get_initmodel_template lupi_features","title":"Instance variables"},{"location":"reference/fri/model/#methods_3","text":"","title":"Methods"},{"location":"reference/fri/model/#get_all_parameters_3","text":"def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () }","title":"get_all_parameters"},{"location":"reference/fri/model/#get_all_relax_factors_3","text":"def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () }","title":"get_all_relax_factors"},{"location":"reference/fri/model/#get_chosen_parameter_3","text":"def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 )","title":"get_chosen_parameter"},{"location":"reference/fri/model/#get_chosen_relax_factors_3","text":"def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor","title":"get_chosen_relax_factors"},{"location":"reference/fri/model/#get_relaxed_constraints_3","text":"def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () }","title":"get_relaxed_constraints"},{"location":"reference/fri/model/#postprocessing_3","text":"def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds","title":"postprocessing"},{"location":"reference/fri/model/#preprocessing_3","text":"def preprocessing ( self , data , lupi_features = None ) View Source def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y","title":"preprocessing"},{"location":"reference/fri/model/#relax_constraint_3","text":"def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"relax_constraint"},{"location":"reference/fri/model/#relax_factors_3","text":"def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ]","title":"relax_factors"},{"location":"reference/fri/model/#ordinalregression","text":"class OrdinalRegression ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class OrdinalRegression ( ProblemType ) : @classmethod def parameters ( cls ) : return [ \"C\" ] @property def get_initmodel_template ( cls ) : return OrdinalRegression_SVM @property def get_cvxproblem_template ( cls ) : return OrdinalRegression_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y","title":"OrdinalRegression"},{"location":"reference/fri/model/#ancestors-in-mro_4","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/fri/model/#parameters_4","text":"def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\" ]","title":"parameters"},{"location":"reference/fri/model/#instance-variables_4","text":"get_cvxproblem_template get_initmodel_template","title":"Instance variables"},{"location":"reference/fri/model/#methods_4","text":"","title":"Methods"},{"location":"reference/fri/model/#get_all_parameters_4","text":"def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () }","title":"get_all_parameters"},{"location":"reference/fri/model/#get_all_relax_factors_4","text":"def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () }","title":"get_all_relax_factors"},{"location":"reference/fri/model/#get_chosen_parameter_4","text":"def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 )","title":"get_chosen_parameter"},{"location":"reference/fri/model/#get_chosen_relax_factors_4","text":"def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor","title":"get_chosen_relax_factors"},{"location":"reference/fri/model/#get_relaxed_constraints_4","text":"def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () }","title":"get_relaxed_constraints"},{"location":"reference/fri/model/#postprocessing_4","text":"def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds","title":"postprocessing"},{"location":"reference/fri/model/#preprocessing_4","text":"def preprocessing ( self , data , ** kwargs ) View Source def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y","title":"preprocessing"},{"location":"reference/fri/model/#relax_constraint_4","text":"def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"relax_constraint"},{"location":"reference/fri/model/#relax_factors_4","text":"def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ]","title":"relax_factors"},{"location":"reference/fri/model/#regression","text":"class Regression ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Regression ( ProblemType ) : @classmethod def parameters ( cls ) : return [ \"C\", \"epsilon\" ] @property def get_initmodel_template ( cls ) : return Regression_SVR @property def get_cvxproblem_template ( cls ) : return Regression_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y","title":"Regression"},{"location":"reference/fri/model/#ancestors-in-mro_5","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/#static-methods_5","text":"","title":"Static methods"},{"location":"reference/fri/model/#parameters_5","text":"def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\", \"epsilon\" ]","title":"parameters"},{"location":"reference/fri/model/#instance-variables_5","text":"get_cvxproblem_template get_initmodel_template","title":"Instance variables"},{"location":"reference/fri/model/#methods_5","text":"","title":"Methods"},{"location":"reference/fri/model/#get_all_parameters_5","text":"def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () }","title":"get_all_parameters"},{"location":"reference/fri/model/#get_all_relax_factors_5","text":"def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () }","title":"get_all_relax_factors"},{"location":"reference/fri/model/#get_chosen_parameter_5","text":"def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 )","title":"get_chosen_parameter"},{"location":"reference/fri/model/#get_chosen_relax_factors_5","text":"def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor","title":"get_chosen_relax_factors"},{"location":"reference/fri/model/#get_relaxed_constraints_5","text":"def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () }","title":"get_relaxed_constraints"},{"location":"reference/fri/model/#postprocessing_5","text":"def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds","title":"postprocessing"},{"location":"reference/fri/model/#preprocessing_5","text":"def preprocessing ( self , data , ** kwargs ) View Source def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y","title":"preprocessing"},{"location":"reference/fri/model/#relax_constraint_5","text":"def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"relax_constraint"},{"location":"reference/fri/model/#relax_factors_5","text":"def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ]","title":"relax_factors"},{"location":"reference/fri/model/base_cvxproblem/","text":"Module fri.model.base_cvxproblem View Source from abc import ABC , abstractmethod import cvxpy as cvx import numpy as np from cvxpy import SolverError class Relevance_CVXProblem ( ABC ): def __repr__ ( self ) -> str : if self . isLowerBound : lower = \"Lower\" else : lower = \"Upper\" name = f \" {lower} _ {self.current_feature} _ {self.__class__.__name__} \" state = \"\" for s in self . init_hyperparameters . items (): state += f \" {s[0]} : {s[1]} , \" for s in self . init_model_constraints . items (): state += f \" {s[0]} : {s[1]} , \" state = \"(\" + state [: - 2 ] + \")\" if self . isProbe : prefix = f \"Probe_ {self.probeID} \" else : prefix = \"\" return prefix + name + state def __init__ ( self , current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs , ) -> None : self . _probeID = probeID self . _feature_relevance = None self . isLowerBound = None # General data self . current_feature = current_feature self . preset_model = preset_model self . best_model_state = best_model_state self . preprocessing_data ( data , best_model_state ) # Initialize constraints self . _constraints = [] self . _objective = None self . w = None self . _init_constraints ( hyperparameters , best_model_constraints ) if self . preset_model is not None : self . _add_preset_constraints ( self . preset_model , best_model_constraints ) self . init_hyperparameters = hyperparameters self . init_model_constraints = best_model_constraints def preprocessing_data ( self , data , best_model_state ): X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y ) @property def constraints ( self ): return self . _constraints def add_constraint ( self , new ): self . _constraints . append ( new ) @property def objective ( self ): return self . _objective @property def solved_relevance ( self ): if self . is_solved : return self . objective . value else : raise Exception ( \"Problem not solved. No feature relevance computed.\" ) @property def probeID ( self ): return self . _probeID @property def isProbe ( self ): return self . probeID >= 0 @abstractmethod def _init_constraints ( self , parameters , init_model_constraints ): pass @abstractmethod def init_objective_UB ( self , ** kwargs ): pass @abstractmethod def init_objective_LB ( self , ** kwargs ): pass @property def cvx_problem ( self ): return self . _cvx_problem @property def is_solved ( self ): if self . _solver_status in self . accepted_status : try : val = self . objective . value except ValueError : return False return True else : return False @property def accepted_status ( self ): return [ \"optimal\" , \"optimal_inaccurate\" ] def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here, worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print(\"Solve\", self) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors, which are common with our framework: # We solve multiple problems per bound and choose a feasible solution later (see '_create_interval') pass self . _solver_status = self . _cvx_problem . status # self._cvx_problem = None return self def _retrieve_result ( self ): return self . current_feature , self . objective @property def solver_kwargs ( self ): return { \"verbose\" : False , \"solver\" : \"ECOS\" , \"max_iters\" : 300 } def _add_preset_constraints ( self , preset_model : dict , best_model_constraints ): for feature , current_preset in preset_model . items (): # Skip current feature if feature == self . current_feature : continue # Skip unset values if all ( np . isnan ( current_preset )): continue # a weight bigger than the optimal model L1 makes no sense assert abs ( current_preset [ 0 ]) <= best_model_constraints [ \"w_l1\" ] assert abs ( current_preset [ 1 ]) <= best_model_constraints [ \"w_l1\" ] # We add a pair of constraints depending on sign of known coefficient # this makes it possible to solve this as a convex problem if current_preset [ 0 ] >= 0 : self . add_constraint ( self . w [ feature ] >= current_preset [ 0 ]) self . add_constraint ( self . w [ feature ] <= current_preset [ 1 ]) else : self . add_constraint ( self . w [ feature ] <= current_preset [ 0 ]) self . add_constraint ( self . w [ feature ] >= current_preset [ 1 ]) @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): for sign in [ - 1 , 1 ]: problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ): vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ): vals = [ candidate . solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value Classes Relevance_CVXProblem class Relevance_CVXProblem ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Relevance_CVXProblem ( ABC ) : def __ repr__ ( self ) -> str : if self . isLowerBound: lower = \"Lower\" else : lower = \"Upper\" name = f \"{lower}_{self.current_feature}_{self.__class__.__name__}\" state = \"\" for s in self . init_hyperparameters . items () : state += f \"{s[0]}:{s[1]}, \" for s in self . init_model_constraints . items () : state += f \"{s[0]}:{s[1]}, \" state = \"(\" + state [:- 2 ] + \")\" if self . isProbe: prefix = f \"Probe_{self.probeID}\" else : prefix = \"\" return prefix + name + state def __ init__ ( self , current_feature: int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID=- 1 , **kwargs , ) -> None : self . _ probeID = probeID self . _ feature_relevance = None self . isLowerBound = None # General data self . current_feature = current_feature self . preset_model = preset_model self . best_model_state = best_model_state self . preprocessing_data ( data , best_model_state ) # Initialize constraints self . _ constraints = [] self . _ objective = None self . w = None self . _ init_constraints ( hyperparameters , best_model_constraints ) if self . preset_model is not None : self . _ add_preset_constraints ( self . preset_model , best_model_constraints ) self . init_hyperparameters = hyperparameters self . init_model_constraints = best_model_constraints def preprocessing_data ( self , data , best_model_state ) : X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y ) @property def constraints ( self ) : return self . _ constraints def add_constraint ( self , new ) : self . _ constraints . append ( new ) @property def objective ( self ) : return self . _ objective @property def solved_relevance ( self ) : if self . is_solved: return self . objective . value else : raise Exception ( \"Problem not solved. No feature relevance computed.\" ) @property def probeID ( self ) : return self . _ probeID @property def isProbe ( self ) : return self . probeID >= 0 @abstractmethod def _ init_constraints ( self , parameters , init_model_constraints ) : pass @abstractmethod def init_objective_UB ( self , **kwargs ) : pass @abstractmethod def init_objective_LB ( self , **kwargs ) : pass @property def cvx_problem ( self ) : return self . _ cvx_problem @property def is_solved ( self ) : if self . _ solver_status in self . accepted_status: try : val = self . objective . value except ValueError : return False return True else : return False @property def accepted_status ( self ) : return [ \"optimal\" , \"optimal_inaccurate\" ] def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _ cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _ cvx_problem . solve ( **self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _ solver_status = self . _ cvx_problem . status # self . _ cvx_problem = None return self def _ retrieve_result ( self ) : return self . current_feature , self . objective @property def solver_kwargs ( self ) : return { \"verbose\" : False , \"solver\" : \"ECOS\" , \"max_iters\" : 300 } def _ add_preset_constraints ( self , preset_model: dict , best_model_constraints ) : for feature , current_preset in preset_model . items () : # Skip current feature if feature == self . current_feature: continue # Skip unset values if all ( np . isnan ( current_preset )) : continue # a weight bigger than the optimal model L1 makes no sense assert abs ( current_preset [ 0 ]) <= best_model_constraints [ \"w_l1\" ] assert abs ( current_preset [ 1 ]) <= best_model_constraints [ \"w_l1\" ] # We add a pair of constraints depending on sign of known coefficient # this makes it possible to solve this as a convex problem if current_preset [ 0 ] >= 0 : self . add_constraint ( self . w [ feature ] >= current_preset [ 0 ]) self . add_constraint ( self . w [ feature ] <= current_preset [ 1 ]) else : self . add_constraint ( self . w [ feature ] <= current_preset [ 0 ]) self . add_constraint ( self . w [ feature ] >= current_preset [ 1 ]) @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID=- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID=- 1 , ) : for sign in [ - 1 , 1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate . solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value Ancestors (in MRO) abc.ABC Descendants fri.model.classification.Classification_Relevance_Bound fri.model.base_lupi.LUPI_Relevance_CVXProblem fri.model.ordinal_regression.OrdinalRegression_Relevance_Bound fri.model.regression.Regression_Relevance_Bound Static methods aggregate_max_candidates def aggregate_max_candidates ( max_problems_candidates ) View Source @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value aggregate_min_candidates def aggregate_min_candidates ( min_problems_candidates ) View Source @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value generate_lower_bound_problem def generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem generate_upper_bound_problem def generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : for sign in [ -1, 1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem Instance variables accepted_status constraints cvx_problem isProbe is_solved objective probeID solved_relevance solver_kwargs Methods add_constraint def add_constraint ( self , new ) View Source def add_constraint ( self , new ): self . _constraints . append ( new ) init_objective_LB def init_objective_LB ( self , ** kwargs ) View Source @abstractmethod def init_objective_LB ( self , ** kwargs ) : pass init_objective_UB def init_objective_UB ( self , ** kwargs ) View Source @abstractmethod def init_objective_UB ( self , ** kwargs ) : pass preprocessing_data def preprocessing_data ( self , data , best_model_state ) View Source def preprocessing_data ( self , data , best_model_state ): X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y ) solve def solve ( self ) -> object View Source def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self","title":"Base Cvxproblem"},{"location":"reference/fri/model/base_cvxproblem/#module-frimodelbase_cvxproblem","text":"View Source from abc import ABC , abstractmethod import cvxpy as cvx import numpy as np from cvxpy import SolverError class Relevance_CVXProblem ( ABC ): def __repr__ ( self ) -> str : if self . isLowerBound : lower = \"Lower\" else : lower = \"Upper\" name = f \" {lower} _ {self.current_feature} _ {self.__class__.__name__} \" state = \"\" for s in self . init_hyperparameters . items (): state += f \" {s[0]} : {s[1]} , \" for s in self . init_model_constraints . items (): state += f \" {s[0]} : {s[1]} , \" state = \"(\" + state [: - 2 ] + \")\" if self . isProbe : prefix = f \"Probe_ {self.probeID} \" else : prefix = \"\" return prefix + name + state def __init__ ( self , current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs , ) -> None : self . _probeID = probeID self . _feature_relevance = None self . isLowerBound = None # General data self . current_feature = current_feature self . preset_model = preset_model self . best_model_state = best_model_state self . preprocessing_data ( data , best_model_state ) # Initialize constraints self . _constraints = [] self . _objective = None self . w = None self . _init_constraints ( hyperparameters , best_model_constraints ) if self . preset_model is not None : self . _add_preset_constraints ( self . preset_model , best_model_constraints ) self . init_hyperparameters = hyperparameters self . init_model_constraints = best_model_constraints def preprocessing_data ( self , data , best_model_state ): X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y ) @property def constraints ( self ): return self . _constraints def add_constraint ( self , new ): self . _constraints . append ( new ) @property def objective ( self ): return self . _objective @property def solved_relevance ( self ): if self . is_solved : return self . objective . value else : raise Exception ( \"Problem not solved. No feature relevance computed.\" ) @property def probeID ( self ): return self . _probeID @property def isProbe ( self ): return self . probeID >= 0 @abstractmethod def _init_constraints ( self , parameters , init_model_constraints ): pass @abstractmethod def init_objective_UB ( self , ** kwargs ): pass @abstractmethod def init_objective_LB ( self , ** kwargs ): pass @property def cvx_problem ( self ): return self . _cvx_problem @property def is_solved ( self ): if self . _solver_status in self . accepted_status : try : val = self . objective . value except ValueError : return False return True else : return False @property def accepted_status ( self ): return [ \"optimal\" , \"optimal_inaccurate\" ] def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here, worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print(\"Solve\", self) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors, which are common with our framework: # We solve multiple problems per bound and choose a feasible solution later (see '_create_interval') pass self . _solver_status = self . _cvx_problem . status # self._cvx_problem = None return self def _retrieve_result ( self ): return self . current_feature , self . objective @property def solver_kwargs ( self ): return { \"verbose\" : False , \"solver\" : \"ECOS\" , \"max_iters\" : 300 } def _add_preset_constraints ( self , preset_model : dict , best_model_constraints ): for feature , current_preset in preset_model . items (): # Skip current feature if feature == self . current_feature : continue # Skip unset values if all ( np . isnan ( current_preset )): continue # a weight bigger than the optimal model L1 makes no sense assert abs ( current_preset [ 0 ]) <= best_model_constraints [ \"w_l1\" ] assert abs ( current_preset [ 1 ]) <= best_model_constraints [ \"w_l1\" ] # We add a pair of constraints depending on sign of known coefficient # this makes it possible to solve this as a convex problem if current_preset [ 0 ] >= 0 : self . add_constraint ( self . w [ feature ] >= current_preset [ 0 ]) self . add_constraint ( self . w [ feature ] <= current_preset [ 1 ]) else : self . add_constraint ( self . w [ feature ] <= current_preset [ 0 ]) self . add_constraint ( self . w [ feature ] >= current_preset [ 1 ]) @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): for sign in [ - 1 , 1 ]: problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ): vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ): vals = [ candidate . solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value","title":"Module fri.model.base_cvxproblem"},{"location":"reference/fri/model/base_cvxproblem/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/base_cvxproblem/#relevance_cvxproblem","text":"class Relevance_CVXProblem ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Relevance_CVXProblem ( ABC ) : def __ repr__ ( self ) -> str : if self . isLowerBound: lower = \"Lower\" else : lower = \"Upper\" name = f \"{lower}_{self.current_feature}_{self.__class__.__name__}\" state = \"\" for s in self . init_hyperparameters . items () : state += f \"{s[0]}:{s[1]}, \" for s in self . init_model_constraints . items () : state += f \"{s[0]}:{s[1]}, \" state = \"(\" + state [:- 2 ] + \")\" if self . isProbe: prefix = f \"Probe_{self.probeID}\" else : prefix = \"\" return prefix + name + state def __ init__ ( self , current_feature: int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID=- 1 , **kwargs , ) -> None : self . _ probeID = probeID self . _ feature_relevance = None self . isLowerBound = None # General data self . current_feature = current_feature self . preset_model = preset_model self . best_model_state = best_model_state self . preprocessing_data ( data , best_model_state ) # Initialize constraints self . _ constraints = [] self . _ objective = None self . w = None self . _ init_constraints ( hyperparameters , best_model_constraints ) if self . preset_model is not None : self . _ add_preset_constraints ( self . preset_model , best_model_constraints ) self . init_hyperparameters = hyperparameters self . init_model_constraints = best_model_constraints def preprocessing_data ( self , data , best_model_state ) : X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y ) @property def constraints ( self ) : return self . _ constraints def add_constraint ( self , new ) : self . _ constraints . append ( new ) @property def objective ( self ) : return self . _ objective @property def solved_relevance ( self ) : if self . is_solved: return self . objective . value else : raise Exception ( \"Problem not solved. No feature relevance computed.\" ) @property def probeID ( self ) : return self . _ probeID @property def isProbe ( self ) : return self . probeID >= 0 @abstractmethod def _ init_constraints ( self , parameters , init_model_constraints ) : pass @abstractmethod def init_objective_UB ( self , **kwargs ) : pass @abstractmethod def init_objective_LB ( self , **kwargs ) : pass @property def cvx_problem ( self ) : return self . _ cvx_problem @property def is_solved ( self ) : if self . _ solver_status in self . accepted_status: try : val = self . objective . value except ValueError : return False return True else : return False @property def accepted_status ( self ) : return [ \"optimal\" , \"optimal_inaccurate\" ] def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _ cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _ cvx_problem . solve ( **self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _ solver_status = self . _ cvx_problem . status # self . _ cvx_problem = None return self def _ retrieve_result ( self ) : return self . current_feature , self . objective @property def solver_kwargs ( self ) : return { \"verbose\" : False , \"solver\" : \"ECOS\" , \"max_iters\" : 300 } def _ add_preset_constraints ( self , preset_model: dict , best_model_constraints ) : for feature , current_preset in preset_model . items () : # Skip current feature if feature == self . current_feature: continue # Skip unset values if all ( np . isnan ( current_preset )) : continue # a weight bigger than the optimal model L1 makes no sense assert abs ( current_preset [ 0 ]) <= best_model_constraints [ \"w_l1\" ] assert abs ( current_preset [ 1 ]) <= best_model_constraints [ \"w_l1\" ] # We add a pair of constraints depending on sign of known coefficient # this makes it possible to solve this as a convex problem if current_preset [ 0 ] >= 0 : self . add_constraint ( self . w [ feature ] >= current_preset [ 0 ]) self . add_constraint ( self . w [ feature ] <= current_preset [ 1 ]) else : self . add_constraint ( self . w [ feature ] <= current_preset [ 0 ]) self . add_constraint ( self . w [ feature ] >= current_preset [ 1 ]) @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID=- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID=- 1 , ) : for sign in [ - 1 , 1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate . solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value","title":"Relevance_CVXProblem"},{"location":"reference/fri/model/base_cvxproblem/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/base_cvxproblem/#descendants","text":"fri.model.classification.Classification_Relevance_Bound fri.model.base_lupi.LUPI_Relevance_CVXProblem fri.model.ordinal_regression.OrdinalRegression_Relevance_Bound fri.model.regression.Regression_Relevance_Bound","title":"Descendants"},{"location":"reference/fri/model/base_cvxproblem/#static-methods","text":"","title":"Static methods"},{"location":"reference/fri/model/base_cvxproblem/#aggregate_max_candidates","text":"def aggregate_max_candidates ( max_problems_candidates ) View Source @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value","title":"aggregate_max_candidates"},{"location":"reference/fri/model/base_cvxproblem/#aggregate_min_candidates","text":"def aggregate_min_candidates ( min_problems_candidates ) View Source @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value","title":"aggregate_min_candidates"},{"location":"reference/fri/model/base_cvxproblem/#generate_lower_bound_problem","text":"def generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem","title":"generate_lower_bound_problem"},{"location":"reference/fri/model/base_cvxproblem/#generate_upper_bound_problem","text":"def generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : for sign in [ -1, 1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem","title":"generate_upper_bound_problem"},{"location":"reference/fri/model/base_cvxproblem/#instance-variables","text":"accepted_status constraints cvx_problem isProbe is_solved objective probeID solved_relevance solver_kwargs","title":"Instance variables"},{"location":"reference/fri/model/base_cvxproblem/#methods","text":"","title":"Methods"},{"location":"reference/fri/model/base_cvxproblem/#add_constraint","text":"def add_constraint ( self , new ) View Source def add_constraint ( self , new ): self . _constraints . append ( new )","title":"add_constraint"},{"location":"reference/fri/model/base_cvxproblem/#init_objective_lb","text":"def init_objective_LB ( self , ** kwargs ) View Source @abstractmethod def init_objective_LB ( self , ** kwargs ) : pass","title":"init_objective_LB"},{"location":"reference/fri/model/base_cvxproblem/#init_objective_ub","text":"def init_objective_UB ( self , ** kwargs ) View Source @abstractmethod def init_objective_UB ( self , ** kwargs ) : pass","title":"init_objective_UB"},{"location":"reference/fri/model/base_cvxproblem/#preprocessing_data","text":"def preprocessing_data ( self , data , best_model_state ) View Source def preprocessing_data ( self , data , best_model_state ): X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y )","title":"preprocessing_data"},{"location":"reference/fri/model/base_cvxproblem/#solve","text":"def solve ( self ) -> object View Source def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self","title":"solve"},{"location":"reference/fri/model/base_initmodel/","text":"Module fri.model.base_initmodel Base class for our initial baseline models which are used in Gridsearch. They store the constant parameters needed in the model and the dynamic instance attributes when fitted. View Source \"\"\" Base class for our initial baseline models which are used in Gridsearch. They store the constant parameters needed in the model and the dynamic instance attributes when fitted. \"\"\" from abc import ABC , abstractmethod from sklearn.base import BaseEstimator class InitModel ( ABC , BaseEstimator ): HYPERPARAMETER = {} SOLVER_PARAMS = { \"solver\" : \"ECOS\" } def __init__ ( self , ** parameters ): self . model_state = {} self . constraints = {} @abstractmethod def fit ( self , X , y , ** kwargs ): pass @abstractmethod def predict ( self , X ): pass @abstractmethod def score ( self , X , y , ** kwargs ): pass def make_scorer ( self ): return None , None @property def L1_factor ( self ): try : return self . constraints [ \"w_l1\" ] except : raise NotImplementedError ( \"Baseline model does not provide (L1) normalization constant. Expected l1 norm of model weights (e.g. w).\" ) class LUPI_InitModel ( InitModel ): @property def L1_factor_priv ( self ): try : return self . constraints [ \"w_priv_l1\" ] except : raise NotImplementedError ( \"Baseline model does not provide LUPI (L1) normalization constant. Expected l1 norm of LUPI model weights (e.g. w_priv).\" ) Classes InitModel class InitModel ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance. View Source class InitModel ( ABC , BaseEstimator ) : HYPERPARAMETER = {} SOLVER_PARAMS = { \"solver\" : \"ECOS\" } def __init__ ( self , ** parameters ) : self . model_state = {} self . constraints = {} @abstractmethod def fit ( self , X , y , ** kwargs ) : pass @abstractmethod def predict ( self , X ) : pass @abstractmethod def score ( self , X , y , ** kwargs ) : pass def make_scorer ( self ) : return None , None @property def L1_factor ( self ) : try : return self . constraints [ \"w_l1\" ] except : raise NotImplementedError ( \"Baseline model does not provide (L1) normalization constant. Expected l1 norm of model weights (e.g. w).\" ) Ancestors (in MRO) abc.ABC sklearn.base.BaseEstimator Descendants fri.model.base_initmodel.LUPI_InitModel fri.model.classification.Classification_SVM fri.model.lupi_classification.LUPI_Classification_SVM fri.model.ordinal_regression.OrdinalRegression_SVM fri.model.regression.Regression_SVR Class variables HYPERPARAMETER SOLVER_PARAMS Instance variables L1_factor Methods fit def fit ( self , X , y , ** kwargs ) View Source @abstractmethod def fit ( self , X , y , ** kwargs ) : pass get_params def get_params ( self , deep = True ) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out make_scorer def make_scorer ( self ) View Source def make_scorer ( self ): return None , None predict def predict ( self , X ) View Source @abstractmethod def predict ( self , X ) : pass score def score ( self , X , y , ** kwargs ) View Source @abstractmethod def score ( self , X , y , ** kwargs ) : pass set_params def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self LUPI_InitModel class LUPI_InitModel ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_InitModel ( InitModel ) : @property def L1_factor_priv ( self ) : try : return self . constraints [ \"w_priv_l1\" ] except : raise NotImplementedError ( \"Baseline model does not provide LUPI (L1) normalization constant. Expected l1 norm of LUPI model weights (e.g. w_priv).\" ) Ancestors (in MRO) fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator Descendants fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression_SVM fri.model.lupi_regression.LUPI_Regression_SVM Class variables HYPERPARAMETER SOLVER_PARAMS Instance variables L1_factor L1_factor_priv Methods fit def fit ( self , X , y , ** kwargs ) View Source @abstractmethod def fit ( self , X , y , ** kwargs ) : pass get_params def get_params ( self , deep = True ) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out make_scorer def make_scorer ( self ) View Source def make_scorer ( self ): return None , None predict def predict ( self , X ) View Source @abstractmethod def predict ( self , X ) : pass score def score ( self , X , y , ** kwargs ) View Source @abstractmethod def score ( self , X , y , ** kwargs ) : pass set_params def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Base Initmodel"},{"location":"reference/fri/model/base_initmodel/#module-frimodelbase_initmodel","text":"Base class for our initial baseline models which are used in Gridsearch. They store the constant parameters needed in the model and the dynamic instance attributes when fitted. View Source \"\"\" Base class for our initial baseline models which are used in Gridsearch. They store the constant parameters needed in the model and the dynamic instance attributes when fitted. \"\"\" from abc import ABC , abstractmethod from sklearn.base import BaseEstimator class InitModel ( ABC , BaseEstimator ): HYPERPARAMETER = {} SOLVER_PARAMS = { \"solver\" : \"ECOS\" } def __init__ ( self , ** parameters ): self . model_state = {} self . constraints = {} @abstractmethod def fit ( self , X , y , ** kwargs ): pass @abstractmethod def predict ( self , X ): pass @abstractmethod def score ( self , X , y , ** kwargs ): pass def make_scorer ( self ): return None , None @property def L1_factor ( self ): try : return self . constraints [ \"w_l1\" ] except : raise NotImplementedError ( \"Baseline model does not provide (L1) normalization constant. Expected l1 norm of model weights (e.g. w).\" ) class LUPI_InitModel ( InitModel ): @property def L1_factor_priv ( self ): try : return self . constraints [ \"w_priv_l1\" ] except : raise NotImplementedError ( \"Baseline model does not provide LUPI (L1) normalization constant. Expected l1 norm of LUPI model weights (e.g. w_priv).\" )","title":"Module fri.model.base_initmodel"},{"location":"reference/fri/model/base_initmodel/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/base_initmodel/#initmodel","text":"class InitModel ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance. View Source class InitModel ( ABC , BaseEstimator ) : HYPERPARAMETER = {} SOLVER_PARAMS = { \"solver\" : \"ECOS\" } def __init__ ( self , ** parameters ) : self . model_state = {} self . constraints = {} @abstractmethod def fit ( self , X , y , ** kwargs ) : pass @abstractmethod def predict ( self , X ) : pass @abstractmethod def score ( self , X , y , ** kwargs ) : pass def make_scorer ( self ) : return None , None @property def L1_factor ( self ) : try : return self . constraints [ \"w_l1\" ] except : raise NotImplementedError ( \"Baseline model does not provide (L1) normalization constant. Expected l1 norm of model weights (e.g. w).\" )","title":"InitModel"},{"location":"reference/fri/model/base_initmodel/#ancestors-in-mro","text":"abc.ABC sklearn.base.BaseEstimator","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/base_initmodel/#descendants","text":"fri.model.base_initmodel.LUPI_InitModel fri.model.classification.Classification_SVM fri.model.lupi_classification.LUPI_Classification_SVM fri.model.ordinal_regression.OrdinalRegression_SVM fri.model.regression.Regression_SVR","title":"Descendants"},{"location":"reference/fri/model/base_initmodel/#class-variables","text":"HYPERPARAMETER SOLVER_PARAMS","title":"Class variables"},{"location":"reference/fri/model/base_initmodel/#instance-variables","text":"L1_factor","title":"Instance variables"},{"location":"reference/fri/model/base_initmodel/#methods","text":"","title":"Methods"},{"location":"reference/fri/model/base_initmodel/#fit","text":"def fit ( self , X , y , ** kwargs ) View Source @abstractmethod def fit ( self , X , y , ** kwargs ) : pass","title":"fit"},{"location":"reference/fri/model/base_initmodel/#get_params","text":"def get_params ( self , deep = True ) Get parameters for this estimator.","title":"get_params"},{"location":"reference/fri/model/base_initmodel/#parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"reference/fri/model/base_initmodel/#returns","text":"params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out","title":"Returns"},{"location":"reference/fri/model/base_initmodel/#make_scorer","text":"def make_scorer ( self ) View Source def make_scorer ( self ): return None , None","title":"make_scorer"},{"location":"reference/fri/model/base_initmodel/#predict","text":"def predict ( self , X ) View Source @abstractmethod def predict ( self , X ) : pass","title":"predict"},{"location":"reference/fri/model/base_initmodel/#score","text":"def score ( self , X , y , ** kwargs ) View Source @abstractmethod def score ( self , X , y , ** kwargs ) : pass","title":"score"},{"location":"reference/fri/model/base_initmodel/#set_params","text":"def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params"},{"location":"reference/fri/model/base_initmodel/#parameters_1","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"reference/fri/model/base_initmodel/#returns_1","text":"self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Returns"},{"location":"reference/fri/model/base_initmodel/#lupi_initmodel","text":"class LUPI_InitModel ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_InitModel ( InitModel ) : @property def L1_factor_priv ( self ) : try : return self . constraints [ \"w_priv_l1\" ] except : raise NotImplementedError ( \"Baseline model does not provide LUPI (L1) normalization constant. Expected l1 norm of LUPI model weights (e.g. w_priv).\" )","title":"LUPI_InitModel"},{"location":"reference/fri/model/base_initmodel/#ancestors-in-mro_1","text":"fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/base_initmodel/#descendants_1","text":"fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression_SVM fri.model.lupi_regression.LUPI_Regression_SVM","title":"Descendants"},{"location":"reference/fri/model/base_initmodel/#class-variables_1","text":"HYPERPARAMETER SOLVER_PARAMS","title":"Class variables"},{"location":"reference/fri/model/base_initmodel/#instance-variables_1","text":"L1_factor L1_factor_priv","title":"Instance variables"},{"location":"reference/fri/model/base_initmodel/#methods_1","text":"","title":"Methods"},{"location":"reference/fri/model/base_initmodel/#fit_1","text":"def fit ( self , X , y , ** kwargs ) View Source @abstractmethod def fit ( self , X , y , ** kwargs ) : pass","title":"fit"},{"location":"reference/fri/model/base_initmodel/#get_params_1","text":"def get_params ( self , deep = True ) Get parameters for this estimator.","title":"get_params"},{"location":"reference/fri/model/base_initmodel/#parameters_2","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"reference/fri/model/base_initmodel/#returns_2","text":"params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out","title":"Returns"},{"location":"reference/fri/model/base_initmodel/#make_scorer_1","text":"def make_scorer ( self ) View Source def make_scorer ( self ): return None , None","title":"make_scorer"},{"location":"reference/fri/model/base_initmodel/#predict_1","text":"def predict ( self , X ) View Source @abstractmethod def predict ( self , X ) : pass","title":"predict"},{"location":"reference/fri/model/base_initmodel/#score_1","text":"def score ( self , X , y , ** kwargs ) View Source @abstractmethod def score ( self , X , y , ** kwargs ) : pass","title":"score"},{"location":"reference/fri/model/base_initmodel/#set_params_1","text":"def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params"},{"location":"reference/fri/model/base_initmodel/#parameters_3","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"reference/fri/model/base_initmodel/#returns_3","text":"self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Returns"},{"location":"reference/fri/model/base_lupi/","text":"Module fri.model.base_lupi View Source from abc import abstractmethod from . base_cvxproblem import Relevance_CVXProblem class LUPI_Relevance_CVXProblem ( Relevance_CVXProblem ) : def __ init__ ( self , current_feature: int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID=- 1 , ) -> None : super (). __ init__ ( current_feature , data , hyperparameters , best_model_constraints , preset_model , best_model_state , probeID , ) def preprocessing_data ( self , data , best_model_state ) : lupi_features = best_model_state [ \"lupi_features\" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super (). preprocessing_data (( X , y ), best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False def init_objective_UB ( self , **kwargs ) : # We have two models basically with different indexes if self . isPriv: self . _ init_objective_UB_LUPI ( **kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_UB ( **kwargs ) def init_objective_LB ( self , **kwargs ) : # We have two models basically with different indexes if self . isPriv: self . _ init_objective_LB_LUPI ( **kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_LB ( **kwargs ) @abstractmethod def _ init_objective_LB_LUPI ( self , **kwargs ) : pass @abstractmethod def _ init_objective_UB_LUPI ( self , **kwargs ) : pass def split_dataset ( X_combined , lupi_features ) : assert X_combined . shape [ 1 ] > lupi_features X = X_combined [ : , :- lupi_features ] X_priv = X_combined [ : , - lupi_features: ] return X , X_priv def is_lupi_feature ( di , data , best_model_state ) : lupi_features = best_model_state [ \"lupi_features\" ] X_combined , _ = data d = X_combined . shape [ 1 ] - lupi_features lupi_index = di - d return lupi_index >= 0 Functions is_lupi_feature def is_lupi_feature ( di , data , best_model_state ) View Source def is_lupi_feature ( di , data , best_model_state ): lupi_features = best_model_state [ \"lupi_features\" ] X_combined , _ = data d = X_combined . shape [ 1 ] - lupi_features lupi_index = di - d return lupi_index >= 0 split_dataset def split_dataset ( X_combined , lupi_features ) View Source def split_dataset ( X_combined , lupi_features ) : assert X_combined . shape [ 1 ] > lupi_features X = X_combined [ : , :- lupi_features ] X_priv = X_combined [ : , - lupi_features: ] return X , X_priv Classes LUPI_Relevance_CVXProblem class LUPI_Relevance_CVXProblem ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Relevance_CVXProblem ( Relevance_CVXProblem ) : def __init__ ( self , current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ) -> None : super (). __init__ ( current_feature , data , hyperparameters , best_model_constraints , preset_model , best_model_state , probeID , ) def preprocessing_data ( self , data , best_model_state ) : lupi_features = best_model_state [ \"lupi_features\" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super (). preprocessing_data (( X , y ), best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False def init_objective_UB ( self , ** kwargs ) : # We have two models basically with different indexes if self . isPriv : self . _init_objective_UB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_UB ( ** kwargs ) def init_objective_LB ( self , ** kwargs ) : # We have two models basically with different indexes if self . isPriv : self . _init_objective_LB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_LB ( ** kwargs ) @abstractmethod def _init_objective_LB_LUPI ( self , ** kwargs ) : pass @abstractmethod def _init_objective_UB_LUPI ( self , ** kwargs ) : pass Ancestors (in MRO) fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC Descendants fri.model.lupi_classification.LUPI_Classification_Relevance_Bound fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression_Relevance_Bound fri.model.lupi_regression.LUPI_Regression_Relevance_Bound Static methods aggregate_max_candidates def aggregate_max_candidates ( max_problems_candidates ) View Source @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value aggregate_min_candidates def aggregate_min_candidates ( min_problems_candidates ) View Source @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value generate_lower_bound_problem def generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem generate_upper_bound_problem def generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : for sign in [ -1, 1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem Instance variables accepted_status constraints cvx_problem isProbe is_solved objective probeID solved_relevance solver_kwargs Methods add_constraint def add_constraint ( self , new ) View Source def add_constraint ( self , new ): self . _constraints . append ( new ) init_objective_LB def init_objective_LB ( self , ** kwargs ) View Source def init_objective_LB ( self , ** kwargs ): # We have two models basically with different indexes if self . isPriv : self . _init_objective_LB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_LB ( ** kwargs ) init_objective_UB def init_objective_UB ( self , ** kwargs ) View Source def init_objective_UB ( self , ** kwargs ): # We have two models basically with different indexes if self . isPriv : self . _init_objective_UB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_UB ( ** kwargs ) preprocessing_data def preprocessing_data ( self , data , best_model_state ) View Source def preprocessing_data ( self , data , best_model_state ): lupi_features = best_model_state [ \"lupi_features\" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super (). preprocessing_data (( X , y ), best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False solve def solve ( self ) -> object View Source def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self","title":"Base Lupi"},{"location":"reference/fri/model/base_lupi/#module-frimodelbase_lupi","text":"View Source from abc import abstractmethod from . base_cvxproblem import Relevance_CVXProblem class LUPI_Relevance_CVXProblem ( Relevance_CVXProblem ) : def __ init__ ( self , current_feature: int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID=- 1 , ) -> None : super (). __ init__ ( current_feature , data , hyperparameters , best_model_constraints , preset_model , best_model_state , probeID , ) def preprocessing_data ( self , data , best_model_state ) : lupi_features = best_model_state [ \"lupi_features\" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super (). preprocessing_data (( X , y ), best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False def init_objective_UB ( self , **kwargs ) : # We have two models basically with different indexes if self . isPriv: self . _ init_objective_UB_LUPI ( **kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_UB ( **kwargs ) def init_objective_LB ( self , **kwargs ) : # We have two models basically with different indexes if self . isPriv: self . _ init_objective_LB_LUPI ( **kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_LB ( **kwargs ) @abstractmethod def _ init_objective_LB_LUPI ( self , **kwargs ) : pass @abstractmethod def _ init_objective_UB_LUPI ( self , **kwargs ) : pass def split_dataset ( X_combined , lupi_features ) : assert X_combined . shape [ 1 ] > lupi_features X = X_combined [ : , :- lupi_features ] X_priv = X_combined [ : , - lupi_features: ] return X , X_priv def is_lupi_feature ( di , data , best_model_state ) : lupi_features = best_model_state [ \"lupi_features\" ] X_combined , _ = data d = X_combined . shape [ 1 ] - lupi_features lupi_index = di - d return lupi_index >= 0","title":"Module fri.model.base_lupi"},{"location":"reference/fri/model/base_lupi/#functions","text":"","title":"Functions"},{"location":"reference/fri/model/base_lupi/#is_lupi_feature","text":"def is_lupi_feature ( di , data , best_model_state ) View Source def is_lupi_feature ( di , data , best_model_state ): lupi_features = best_model_state [ \"lupi_features\" ] X_combined , _ = data d = X_combined . shape [ 1 ] - lupi_features lupi_index = di - d return lupi_index >= 0","title":"is_lupi_feature"},{"location":"reference/fri/model/base_lupi/#split_dataset","text":"def split_dataset ( X_combined , lupi_features ) View Source def split_dataset ( X_combined , lupi_features ) : assert X_combined . shape [ 1 ] > lupi_features X = X_combined [ : , :- lupi_features ] X_priv = X_combined [ : , - lupi_features: ] return X , X_priv","title":"split_dataset"},{"location":"reference/fri/model/base_lupi/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/base_lupi/#lupi_relevance_cvxproblem","text":"class LUPI_Relevance_CVXProblem ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Relevance_CVXProblem ( Relevance_CVXProblem ) : def __init__ ( self , current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ) -> None : super (). __init__ ( current_feature , data , hyperparameters , best_model_constraints , preset_model , best_model_state , probeID , ) def preprocessing_data ( self , data , best_model_state ) : lupi_features = best_model_state [ \"lupi_features\" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super (). preprocessing_data (( X , y ), best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False def init_objective_UB ( self , ** kwargs ) : # We have two models basically with different indexes if self . isPriv : self . _init_objective_UB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_UB ( ** kwargs ) def init_objective_LB ( self , ** kwargs ) : # We have two models basically with different indexes if self . isPriv : self . _init_objective_LB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_LB ( ** kwargs ) @abstractmethod def _init_objective_LB_LUPI ( self , ** kwargs ) : pass @abstractmethod def _init_objective_UB_LUPI ( self , ** kwargs ) : pass","title":"LUPI_Relevance_CVXProblem"},{"location":"reference/fri/model/base_lupi/#ancestors-in-mro","text":"fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/base_lupi/#descendants","text":"fri.model.lupi_classification.LUPI_Classification_Relevance_Bound fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression_Relevance_Bound fri.model.lupi_regression.LUPI_Regression_Relevance_Bound","title":"Descendants"},{"location":"reference/fri/model/base_lupi/#static-methods","text":"","title":"Static methods"},{"location":"reference/fri/model/base_lupi/#aggregate_max_candidates","text":"def aggregate_max_candidates ( max_problems_candidates ) View Source @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value","title":"aggregate_max_candidates"},{"location":"reference/fri/model/base_lupi/#aggregate_min_candidates","text":"def aggregate_min_candidates ( min_problems_candidates ) View Source @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value","title":"aggregate_min_candidates"},{"location":"reference/fri/model/base_lupi/#generate_lower_bound_problem","text":"def generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem","title":"generate_lower_bound_problem"},{"location":"reference/fri/model/base_lupi/#generate_upper_bound_problem","text":"def generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : for sign in [ -1, 1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem","title":"generate_upper_bound_problem"},{"location":"reference/fri/model/base_lupi/#instance-variables","text":"accepted_status constraints cvx_problem isProbe is_solved objective probeID solved_relevance solver_kwargs","title":"Instance variables"},{"location":"reference/fri/model/base_lupi/#methods","text":"","title":"Methods"},{"location":"reference/fri/model/base_lupi/#add_constraint","text":"def add_constraint ( self , new ) View Source def add_constraint ( self , new ): self . _constraints . append ( new )","title":"add_constraint"},{"location":"reference/fri/model/base_lupi/#init_objective_lb","text":"def init_objective_LB ( self , ** kwargs ) View Source def init_objective_LB ( self , ** kwargs ): # We have two models basically with different indexes if self . isPriv : self . _init_objective_LB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_LB ( ** kwargs )","title":"init_objective_LB"},{"location":"reference/fri/model/base_lupi/#init_objective_ub","text":"def init_objective_UB ( self , ** kwargs ) View Source def init_objective_UB ( self , ** kwargs ): # We have two models basically with different indexes if self . isPriv : self . _init_objective_UB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_UB ( ** kwargs )","title":"init_objective_UB"},{"location":"reference/fri/model/base_lupi/#preprocessing_data","text":"def preprocessing_data ( self , data , best_model_state ) View Source def preprocessing_data ( self , data , best_model_state ): lupi_features = best_model_state [ \"lupi_features\" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super (). preprocessing_data (( X , y ), best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False","title":"preprocessing_data"},{"location":"reference/fri/model/base_lupi/#solve","text":"def solve ( self ) -> object View Source def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self","title":"solve"},{"location":"reference/fri/model/base_type/","text":"Module fri.model.base_type View Source from abc import ABC , abstractmethod import scipy.stats class ProblemType ( ABC ): def __init__ ( self , ** kwargs ): self . chosen_parameters_ = {} for p in self . parameters (): if p in kwargs : if kwargs [ p ] is not None : self . chosen_parameters_ [ p ] = kwargs [ p ] self . relax_factors_ = {} for p in self . relax_factors (): if p in kwargs : if kwargs [ p ] is not None : self . relax_factors_ [ p ] = kwargs [ p ] @classmethod @abstractmethod def parameters ( cls ): raise NotImplementedError def get_chosen_parameter ( self , p ): try : return [ self . chosen_parameters_ [ p ] ] # We return list for param search function except : # # TODO: rewrite the parameter logic # # TODO: move this to subclass if p == \"scaling_lupi_w\" : # return [0.1, 1, 10, 100, 1000] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\": # # value 0>p<1 causes standard svm solution # # p>1 encourages usage of lupi function # return scipy.stats.reciprocal(a=1e-15, b=1e15) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0 , 0.001 , 0.01 , 0.1 , 1 , 10 , 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters ()} @classmethod @abstractmethod def relax_factors ( cls ): raise NotImplementedError def get_chosen_relax_factors ( self , p ): try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors ()} @property @abstractmethod def get_initmodel_template ( self ): pass @property @abstractmethod def get_cvxproblem_template ( self ): pass @abstractmethod def preprocessing ( self , data , lupi_features = None ): return data def postprocessing ( self , bounds ): return bounds def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items ()} def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key )) Classes ProblemType class ProblemType ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class ProblemType ( ABC ) : def __init__ ( self , ** kwargs ) : self . chosen_parameters_ = {} for p in self . parameters () : if p in kwargs : if kwargs [ p ] is not None : self . chosen_parameters_ [ p ] = kwargs [ p ] self . relax_factors_ = {} for p in self . relax_factors () : if p in kwargs : if kwargs [ p ] is not None : self . relax_factors_ [ p ] = kwargs [ p ] @classmethod @abstractmethod def parameters ( cls ) : raise NotImplementedError def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) def get_all_parameters ( self ) : return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } @classmethod @abstractmethod def relax_factors ( cls ) : raise NotImplementedError def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor def get_all_relax_factors ( self ) : return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } @property @abstractmethod def get_initmodel_template ( self ) : pass @property @abstractmethod def get_cvxproblem_template ( self ) : pass @abstractmethod def preprocessing ( self , data , lupi_features = None ) : return data def postprocessing ( self , bounds ) : return bounds def get_relaxed_constraints ( self , constraints ) : return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } def relax_constraint ( self , key , value ) : return value * ( 1 + self . get_chosen_relax_factors ( key )) Ancestors (in MRO) abc.ABC Descendants fri.model.classification.Classification fri.model.lupi_classification.LUPI_Classification fri.model.ordinal_regression.OrdinalRegression fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression fri.model.regression.Regression fri.model.lupi_regression.LUPI_Regression Static methods parameters def parameters ( ) View Source @classmethod @abstractmethod def parameters ( cls ) : raise NotImplementedError relax_factors def relax_factors ( ) View Source @classmethod @abstractmethod def relax_factors ( cls ) : raise NotImplementedError Instance variables get_cvxproblem_template get_initmodel_template Methods get_all_parameters def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } get_all_relax_factors def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } get_chosen_parameter def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) get_chosen_relax_factors def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor get_relaxed_constraints def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } postprocessing def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds preprocessing def preprocessing ( self , data , lupi_features = None ) View Source @abstractmethod def preprocessing ( self , data , lupi_features = None ) : return data relax_constraint def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"Base Type"},{"location":"reference/fri/model/base_type/#module-frimodelbase_type","text":"View Source from abc import ABC , abstractmethod import scipy.stats class ProblemType ( ABC ): def __init__ ( self , ** kwargs ): self . chosen_parameters_ = {} for p in self . parameters (): if p in kwargs : if kwargs [ p ] is not None : self . chosen_parameters_ [ p ] = kwargs [ p ] self . relax_factors_ = {} for p in self . relax_factors (): if p in kwargs : if kwargs [ p ] is not None : self . relax_factors_ [ p ] = kwargs [ p ] @classmethod @abstractmethod def parameters ( cls ): raise NotImplementedError def get_chosen_parameter ( self , p ): try : return [ self . chosen_parameters_ [ p ] ] # We return list for param search function except : # # TODO: rewrite the parameter logic # # TODO: move this to subclass if p == \"scaling_lupi_w\" : # return [0.1, 1, 10, 100, 1000] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\": # # value 0>p<1 causes standard svm solution # # p>1 encourages usage of lupi function # return scipy.stats.reciprocal(a=1e-15, b=1e15) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0 , 0.001 , 0.01 , 0.1 , 1 , 10 , 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters ()} @classmethod @abstractmethod def relax_factors ( cls ): raise NotImplementedError def get_chosen_relax_factors ( self , p ): try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors ()} @property @abstractmethod def get_initmodel_template ( self ): pass @property @abstractmethod def get_cvxproblem_template ( self ): pass @abstractmethod def preprocessing ( self , data , lupi_features = None ): return data def postprocessing ( self , bounds ): return bounds def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items ()} def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"Module fri.model.base_type"},{"location":"reference/fri/model/base_type/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/base_type/#problemtype","text":"class ProblemType ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class ProblemType ( ABC ) : def __init__ ( self , ** kwargs ) : self . chosen_parameters_ = {} for p in self . parameters () : if p in kwargs : if kwargs [ p ] is not None : self . chosen_parameters_ [ p ] = kwargs [ p ] self . relax_factors_ = {} for p in self . relax_factors () : if p in kwargs : if kwargs [ p ] is not None : self . relax_factors_ [ p ] = kwargs [ p ] @classmethod @abstractmethod def parameters ( cls ) : raise NotImplementedError def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) def get_all_parameters ( self ) : return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } @classmethod @abstractmethod def relax_factors ( cls ) : raise NotImplementedError def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor def get_all_relax_factors ( self ) : return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } @property @abstractmethod def get_initmodel_template ( self ) : pass @property @abstractmethod def get_cvxproblem_template ( self ) : pass @abstractmethod def preprocessing ( self , data , lupi_features = None ) : return data def postprocessing ( self , bounds ) : return bounds def get_relaxed_constraints ( self , constraints ) : return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } def relax_constraint ( self , key , value ) : return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"ProblemType"},{"location":"reference/fri/model/base_type/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/base_type/#descendants","text":"fri.model.classification.Classification fri.model.lupi_classification.LUPI_Classification fri.model.ordinal_regression.OrdinalRegression fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression fri.model.regression.Regression fri.model.lupi_regression.LUPI_Regression","title":"Descendants"},{"location":"reference/fri/model/base_type/#static-methods","text":"","title":"Static methods"},{"location":"reference/fri/model/base_type/#parameters","text":"def parameters ( ) View Source @classmethod @abstractmethod def parameters ( cls ) : raise NotImplementedError","title":"parameters"},{"location":"reference/fri/model/base_type/#relax_factors","text":"def relax_factors ( ) View Source @classmethod @abstractmethod def relax_factors ( cls ) : raise NotImplementedError","title":"relax_factors"},{"location":"reference/fri/model/base_type/#instance-variables","text":"get_cvxproblem_template get_initmodel_template","title":"Instance variables"},{"location":"reference/fri/model/base_type/#methods","text":"","title":"Methods"},{"location":"reference/fri/model/base_type/#get_all_parameters","text":"def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () }","title":"get_all_parameters"},{"location":"reference/fri/model/base_type/#get_all_relax_factors","text":"def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () }","title":"get_all_relax_factors"},{"location":"reference/fri/model/base_type/#get_chosen_parameter","text":"def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 )","title":"get_chosen_parameter"},{"location":"reference/fri/model/base_type/#get_chosen_relax_factors","text":"def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor","title":"get_chosen_relax_factors"},{"location":"reference/fri/model/base_type/#get_relaxed_constraints","text":"def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () }","title":"get_relaxed_constraints"},{"location":"reference/fri/model/base_type/#postprocessing","text":"def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds","title":"postprocessing"},{"location":"reference/fri/model/base_type/#preprocessing","text":"def preprocessing ( self , data , lupi_features = None ) View Source @abstractmethod def preprocessing ( self , data , lupi_features = None ) : return data","title":"preprocessing"},{"location":"reference/fri/model/base_type/#relax_constraint","text":"def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"relax_constraint"},{"location":"reference/fri/model/classification/","text":"Module fri.model.classification View Source import cvxpy as cvx import numpy as np from sklearn import preprocessing from sklearn.metrics import fbeta_score , classification_report from sklearn.preprocessing import LabelEncoder from sklearn.utils import check_X_y from sklearn.utils.multiclass import unique_labels from fri.model.base_cvxproblem import Relevance_CVXProblem from fri.model.base_initmodel import InitModel from .base_type import ProblemType class Classification ( ProblemType ): @classmethod def parameters ( cls ): return [ \"C\" ] @property def get_initmodel_template ( cls ): return Classification_SVM @property def get_cvxproblem_template ( cls ): return Classification_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to -1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y class Classification_SVM ( InitModel ): def __init__ ( self , C = 1 ): super () . __init__ () self . C = C def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . get_params ()[ \"C\" ] w = cvx . Variable ( shape = ( d ), name = \"w\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) b = cvx . Variable ( name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . multiply ( y . T , X * w + b ) >= 1 - slack , slack >= 0 ] # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b = b . value slack = np . asarray ( slack . value ) . flatten () self . model_state = { \"w\" : w , \"b\" : b , \"slack\" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ): w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b >= 0 y = y . astype ( int ) y [ y == 0 ] = - 1 return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) # Negative class is set to -1 for decision surface y = LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \"weighted\" ) if \"verbose\" in kwargs : return classification_report ( y , prediction ) return score class Classification_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] # New Variables self . w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) self . b = cvx . Variable ( name = \"b\" ) self . slack = cvx . Variable ( shape = ( self . n ), nonneg = True , name = \"slack\" ) # New Constraints distance_from_plane = cvx . multiply ( self . y , self . X * self . w + self . b ) self . loss = cvx . sum ( self . slack ) self . weight_norm = cvx . norm ( self . w , 1 ) self . add_constraint ( distance_from_plane >= 1 - self . slack ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Classes Classification class Classification ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Classification ( ProblemType ) : @classmethod def parameters ( cls ) : return [ \"C\" ] @property def get_initmodel_template ( cls ) : return Classification_SVM @property def get_cvxproblem_template ( cls ) : return Classification_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 return X , y Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods parameters def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\" ] Instance variables get_cvxproblem_template get_initmodel_template Methods get_all_parameters def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } get_all_relax_factors def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } get_chosen_parameter def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) get_chosen_relax_factors def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor get_relaxed_constraints def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } postprocessing def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds preprocessing def preprocessing ( self , data , ** kwargs ) View Source def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 return X , y relax_constraint def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key )) relax_factors def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] Classification_Relevance_Bound class Classification_Relevance_Bound ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Classification_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] # New Variables self . w = cvx . Variable ( shape =( self . d ), name = \"w\" ) self . b = cvx . Variable ( name = \"b\" ) self . slack = cvx . Variable ( shape =( self . n ), nonneg = True , name = \"slack\" ) # New Constraints distance_from_plane = cvx . multiply ( self . y , self . X * self . w + self . b ) self . loss = cvx . sum ( self . slack ) self . weight_norm = cvx . norm ( self . w , 1 ) self . add_constraint ( distance_from_plane >= 1 - self . slack ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Ancestors (in MRO) fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC Descendants fri.model.lupi_classification.LUPI_Classification_Relevance_Bound Static methods aggregate_max_candidates def aggregate_max_candidates ( max_problems_candidates ) View Source @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value aggregate_min_candidates def aggregate_min_candidates ( min_problems_candidates ) View Source @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value generate_lower_bound_problem def generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem generate_upper_bound_problem def generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : for sign in [ -1, 1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem Instance variables accepted_status constraints cvx_problem isProbe is_solved objective probeID solved_relevance solver_kwargs Methods add_constraint def add_constraint ( self , new ) View Source def add_constraint ( self , new ): self . _constraints . append ( new ) init_objective_LB def init_objective_LB ( self , ** kwargs ) View Source def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) init_objective_UB def init_objective_UB ( self , sign = None , ** kwargs ) View Source def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) preprocessing_data def preprocessing_data ( self , data , best_model_state ) View Source def preprocessing_data ( self , data , best_model_state ): X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y ) solve def solve ( self ) -> object View Source def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self Classification_SVM class Classification_SVM ( C = 1 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Classification_SVM ( InitModel ): def __init__ ( self , C = 1 ): super (). __init__ () self . C = C def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . get_params ()[ \"C\" ] w = cvx . Variable ( shape =( d ), name = \"w\" ) slack = cvx . Variable ( shape =( n ), name = \"slack\" ) b = cvx . Variable ( name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . multiply ( y . T , X * w + b ) >= 1 - slack , slack >= 0 ] # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve (** self . SOLVER_PARAMS ) w = w . value b = b . value slack = np . asarray ( slack . value ). flatten () self . model_state = { \"w\" : w , \"b\" : b , \"slack\" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ): w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b >= 0 y = y . astype ( int ) y [ y == 0 ] = - 1 return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) # Negative class is set to -1 for decision surface y = LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \"weighted\" ) if \"verbose\" in kwargs: return classification_report ( y , prediction ) return score Ancestors (in MRO) fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator Class variables HYPERPARAMETER SOLVER_PARAMS Instance variables L1_factor Methods fit def fit ( self , X , y , ** kwargs ) View Source def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . get_params ()[ \"C\" ] w = cvx . Variable ( shape = ( d ), name = \"w\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) b = cvx . Variable ( name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . multiply ( y . T , X * w + b ) >= 1 - slack , slack >= 0 ] # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b = b . value slack = np . asarray ( slack . value ). flatten () self . model_state = { \"w\" : w , \"b\" : b , \"slack\" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self get_params def get_params ( self , deep = True ) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out make_scorer def make_scorer ( self ) View Source def make_scorer ( self ): return None , None predict def predict ( self , X ) View Source def predict ( self , X ): w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b >= 0 y = y . astype ( int ) y [ y == 0 ] = - 1 return y score def score ( self , X , y , ** kwargs ) View Source def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) # Negative class is set to - 1 for decision surface y = LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \"weighted\" ) if \"verbose\" in kwargs : return classification_report ( y , prediction ) return score set_params def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Classification"},{"location":"reference/fri/model/classification/#module-frimodelclassification","text":"View Source import cvxpy as cvx import numpy as np from sklearn import preprocessing from sklearn.metrics import fbeta_score , classification_report from sklearn.preprocessing import LabelEncoder from sklearn.utils import check_X_y from sklearn.utils.multiclass import unique_labels from fri.model.base_cvxproblem import Relevance_CVXProblem from fri.model.base_initmodel import InitModel from .base_type import ProblemType class Classification ( ProblemType ): @classmethod def parameters ( cls ): return [ \"C\" ] @property def get_initmodel_template ( cls ): return Classification_SVM @property def get_cvxproblem_template ( cls ): return Classification_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to -1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y class Classification_SVM ( InitModel ): def __init__ ( self , C = 1 ): super () . __init__ () self . C = C def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . get_params ()[ \"C\" ] w = cvx . Variable ( shape = ( d ), name = \"w\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) b = cvx . Variable ( name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . multiply ( y . T , X * w + b ) >= 1 - slack , slack >= 0 ] # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b = b . value slack = np . asarray ( slack . value ) . flatten () self . model_state = { \"w\" : w , \"b\" : b , \"slack\" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ): w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b >= 0 y = y . astype ( int ) y [ y == 0 ] = - 1 return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) # Negative class is set to -1 for decision surface y = LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \"weighted\" ) if \"verbose\" in kwargs : return classification_report ( y , prediction ) return score class Classification_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] # New Variables self . w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) self . b = cvx . Variable ( name = \"b\" ) self . slack = cvx . Variable ( shape = ( self . n ), nonneg = True , name = \"slack\" ) # New Constraints distance_from_plane = cvx . multiply ( self . y , self . X * self . w + self . b ) self . loss = cvx . sum ( self . slack ) self . weight_norm = cvx . norm ( self . w , 1 ) self . add_constraint ( distance_from_plane >= 1 - self . slack ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"Module fri.model.classification"},{"location":"reference/fri/model/classification/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/classification/#classification","text":"class Classification ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Classification ( ProblemType ) : @classmethod def parameters ( cls ) : return [ \"C\" ] @property def get_initmodel_template ( cls ) : return Classification_SVM @property def get_cvxproblem_template ( cls ) : return Classification_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 return X , y","title":"Classification"},{"location":"reference/fri/model/classification/#ancestors-in-mro","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/classification/#static-methods","text":"","title":"Static methods"},{"location":"reference/fri/model/classification/#parameters","text":"def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\" ]","title":"parameters"},{"location":"reference/fri/model/classification/#instance-variables","text":"get_cvxproblem_template get_initmodel_template","title":"Instance variables"},{"location":"reference/fri/model/classification/#methods","text":"","title":"Methods"},{"location":"reference/fri/model/classification/#get_all_parameters","text":"def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () }","title":"get_all_parameters"},{"location":"reference/fri/model/classification/#get_all_relax_factors","text":"def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () }","title":"get_all_relax_factors"},{"location":"reference/fri/model/classification/#get_chosen_parameter","text":"def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 )","title":"get_chosen_parameter"},{"location":"reference/fri/model/classification/#get_chosen_relax_factors","text":"def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor","title":"get_chosen_relax_factors"},{"location":"reference/fri/model/classification/#get_relaxed_constraints","text":"def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () }","title":"get_relaxed_constraints"},{"location":"reference/fri/model/classification/#postprocessing","text":"def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds","title":"postprocessing"},{"location":"reference/fri/model/classification/#preprocessing","text":"def preprocessing ( self , data , ** kwargs ) View Source def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 return X , y","title":"preprocessing"},{"location":"reference/fri/model/classification/#relax_constraint","text":"def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"relax_constraint"},{"location":"reference/fri/model/classification/#relax_factors","text":"def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ]","title":"relax_factors"},{"location":"reference/fri/model/classification/#classification_relevance_bound","text":"class Classification_Relevance_Bound ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Classification_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] # New Variables self . w = cvx . Variable ( shape =( self . d ), name = \"w\" ) self . b = cvx . Variable ( name = \"b\" ) self . slack = cvx . Variable ( shape =( self . n ), nonneg = True , name = \"slack\" ) # New Constraints distance_from_plane = cvx . multiply ( self . y , self . X * self . w + self . b ) self . loss = cvx . sum ( self . slack ) self . weight_norm = cvx . norm ( self . w , 1 ) self . add_constraint ( distance_from_plane >= 1 - self . slack ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"Classification_Relevance_Bound"},{"location":"reference/fri/model/classification/#ancestors-in-mro_1","text":"fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/classification/#descendants","text":"fri.model.lupi_classification.LUPI_Classification_Relevance_Bound","title":"Descendants"},{"location":"reference/fri/model/classification/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/fri/model/classification/#aggregate_max_candidates","text":"def aggregate_max_candidates ( max_problems_candidates ) View Source @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value","title":"aggregate_max_candidates"},{"location":"reference/fri/model/classification/#aggregate_min_candidates","text":"def aggregate_min_candidates ( min_problems_candidates ) View Source @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value","title":"aggregate_min_candidates"},{"location":"reference/fri/model/classification/#generate_lower_bound_problem","text":"def generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem","title":"generate_lower_bound_problem"},{"location":"reference/fri/model/classification/#generate_upper_bound_problem","text":"def generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : for sign in [ -1, 1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem","title":"generate_upper_bound_problem"},{"location":"reference/fri/model/classification/#instance-variables_1","text":"accepted_status constraints cvx_problem isProbe is_solved objective probeID solved_relevance solver_kwargs","title":"Instance variables"},{"location":"reference/fri/model/classification/#methods_1","text":"","title":"Methods"},{"location":"reference/fri/model/classification/#add_constraint","text":"def add_constraint ( self , new ) View Source def add_constraint ( self , new ): self . _constraints . append ( new )","title":"add_constraint"},{"location":"reference/fri/model/classification/#init_objective_lb","text":"def init_objective_LB ( self , ** kwargs ) View Source def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance )","title":"init_objective_LB"},{"location":"reference/fri/model/classification/#init_objective_ub","text":"def init_objective_UB ( self , sign = None , ** kwargs ) View Source def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance )","title":"init_objective_UB"},{"location":"reference/fri/model/classification/#preprocessing_data","text":"def preprocessing_data ( self , data , best_model_state ) View Source def preprocessing_data ( self , data , best_model_state ): X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y )","title":"preprocessing_data"},{"location":"reference/fri/model/classification/#solve","text":"def solve ( self ) -> object View Source def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self","title":"solve"},{"location":"reference/fri/model/classification/#classification_svm","text":"class Classification_SVM ( C = 1 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Classification_SVM ( InitModel ): def __init__ ( self , C = 1 ): super (). __init__ () self . C = C def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . get_params ()[ \"C\" ] w = cvx . Variable ( shape =( d ), name = \"w\" ) slack = cvx . Variable ( shape =( n ), name = \"slack\" ) b = cvx . Variable ( name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . multiply ( y . T , X * w + b ) >= 1 - slack , slack >= 0 ] # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve (** self . SOLVER_PARAMS ) w = w . value b = b . value slack = np . asarray ( slack . value ). flatten () self . model_state = { \"w\" : w , \"b\" : b , \"slack\" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ): w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b >= 0 y = y . astype ( int ) y [ y == 0 ] = - 1 return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) # Negative class is set to -1 for decision surface y = LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \"weighted\" ) if \"verbose\" in kwargs: return classification_report ( y , prediction ) return score","title":"Classification_SVM"},{"location":"reference/fri/model/classification/#ancestors-in-mro_2","text":"fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/classification/#class-variables","text":"HYPERPARAMETER SOLVER_PARAMS","title":"Class variables"},{"location":"reference/fri/model/classification/#instance-variables_2","text":"L1_factor","title":"Instance variables"},{"location":"reference/fri/model/classification/#methods_2","text":"","title":"Methods"},{"location":"reference/fri/model/classification/#fit","text":"def fit ( self , X , y , ** kwargs ) View Source def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . get_params ()[ \"C\" ] w = cvx . Variable ( shape = ( d ), name = \"w\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) b = cvx . Variable ( name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . multiply ( y . T , X * w + b ) >= 1 - slack , slack >= 0 ] # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b = b . value slack = np . asarray ( slack . value ). flatten () self . model_state = { \"w\" : w , \"b\" : b , \"slack\" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self","title":"fit"},{"location":"reference/fri/model/classification/#get_params","text":"def get_params ( self , deep = True ) Get parameters for this estimator.","title":"get_params"},{"location":"reference/fri/model/classification/#parameters_1","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"reference/fri/model/classification/#returns","text":"params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out","title":"Returns"},{"location":"reference/fri/model/classification/#make_scorer","text":"def make_scorer ( self ) View Source def make_scorer ( self ): return None , None","title":"make_scorer"},{"location":"reference/fri/model/classification/#predict","text":"def predict ( self , X ) View Source def predict ( self , X ): w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b >= 0 y = y . astype ( int ) y [ y == 0 ] = - 1 return y","title":"predict"},{"location":"reference/fri/model/classification/#score","text":"def score ( self , X , y , ** kwargs ) View Source def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) # Negative class is set to - 1 for decision surface y = LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \"weighted\" ) if \"verbose\" in kwargs : return classification_report ( y , prediction ) return score","title":"score"},{"location":"reference/fri/model/classification/#set_params","text":"def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params"},{"location":"reference/fri/model/classification/#parameters_2","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"reference/fri/model/classification/#returns_1","text":"self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Returns"},{"location":"reference/fri/model/lupi_classification/","text":"Module fri.model.lupi_classification View Source import cvxpy as cvx import numpy as np from sklearn import preprocessing from sklearn.metrics import fbeta_score , classification_report from sklearn.preprocessing import LabelEncoder from sklearn.utils import check_X_y from sklearn.utils.multiclass import unique_labels from .base_initmodel import InitModel from .base_lupi import LUPI_Relevance_CVXProblem , split_dataset from .base_type import ProblemType from .classification import Classification_Relevance_Bound class LUPI_Classification ( ProblemType ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ): return self . _lupi_features @classmethod def parameters ( cls ): return [ \"C\" , \"scaling_lupi_w\" , \"scaling_lupi_loss\" ] @property def get_initmodel_template ( cls ): return LUPI_Classification_SVM @property def get_cvxproblem_template ( cls ): return LUPI_Classification_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to -1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y class LUPI_Classification_SVM ( InitModel ): HYPERPARAMETER = [ \"C\" , \"scaling_lupi_w\" , \"scaling_lupi_loss\" ] def __init__ ( self , C = 1 , scaling_lupi_w = 1 , scaling_lupi_loss = 1 , lupi_features = None ): super () . __init__ () self . lupi_features = lupi_features self . scaling_lupi_loss = scaling_lupi_loss self . scaling_lupi_w = scaling_lupi_w self . C = C def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : try : lupi_features = self . lupi_features self . lupi_features = lupi_features except : raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . get_params ()[ \"C\" ] scaling_lupi_w = self . get_params ()[ \"scaling_lupi_w\" ] scaling_lupi_loss = self . get_params ()[ \"scaling_lupi_loss\" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) w_priv = cvx . Variable ( lupi_features , name = \"w_priv\" ) b = cvx . Variable ( name = \"bias\" ) b_priv = cvx . Variable ( name = \"bias_priv\" ) # Define functions for better readability function = X * w + b priv_function = X_priv * w_priv + b_priv slack = cvx . Variable ( shape = ( n )) # Combined loss of lupi function and normal slacks, scaled by two constants loss = scaling_lupi_loss * cvx . sum ( priv_function ) + cvx . sum ( slack ) # L1 norm regularization of both functions with 1 scaling constant w_l1 = cvx . norm ( w , 1 ) w_priv_l1 = cvx . norm ( w_priv , 1 ) weight_regularization = 0.5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [ cvx . multiply ( y . T , function ) >= 1 - cvx . multiply ( y . T , priv_function ) - slack , priv_function >= 0 , slack >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value w_priv = w_priv . value b = b . value b_priv = b_priv . value self . model_state = { \"w\" : w , \"w_priv\" : w_priv , \"b\" : b , \"b_priv\" : b_priv , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO: Move this somewhere else } loss = loss . value w_l1 = w_l1 . value w_priv_l1 = w_priv_l1 . value self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 , \"w_priv_l1\" : w_priv_l1 } return self def predict ( self , X ): X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] # Simple hyperplane classification rule f = np . dot ( X , w ) + b y = f >= 0 y = y . astype ( int ) # Format binary as signed unit vector y [ y == 0 ] = - 1 return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) # Negative class is set to -1 for decision surface y = LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \"weighted\" ) if \"verbose\" in kwargs : return classification_report ( y , prediction ) return score class LUPI_Classification_Relevance_Bound ( LUPI_Relevance_CVXProblem , Classification_Relevance_Bound ): def _init_objective_UB_LUPI ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w_priv [ self . lupi_index ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_objective_LB_LUPI ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w_priv [ self . lupi_index ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from best initial model l1_w = init_model_constraints [ \"w_l1\" ] l1_priv_w = init_model_constraints [ \"w_priv_l1\" ] init_loss = init_model_constraints [ \"loss\" ] # New Variables w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) w_priv = cvx . Variable ( shape = ( self . d_priv ), name = \"w_priv\" ) b = cvx . Variable ( name = \"b\" ) b_priv = cvx . Variable ( name = \"b_priv\" ) slack = cvx . Variable ( shape = ( self . n )) # New Constraints function = cvx . multiply ( self . y . T , self . X * w + b ) priv_function = self . X_priv * w_priv + b_priv loss = cvx . sum ( priv_function ) + cvx . sum ( slack ) weight_norm = cvx . norm ( w , 1 ) weight_norm_priv = cvx . norm ( w_priv , 1 ) self . add_constraint ( function >= 1 - cvx . multiply ( self . y . T , priv_function ) - slack ) self . add_constraint ( priv_function >= 0 ) self . add_constraint ( loss <= init_loss ) self . add_constraint ( weight_norm + weight_norm_priv <= l1_w + l1_priv_w ) self . add_constraint ( slack >= 0 ) # Save values for object use later self . w = w self . w_priv = w_priv self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Classes LUPI_Classification class LUPI_Classification ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Classification ( ProblemType ) : def __init__ ( self , ** kwargs ) : super (). __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ) : return self . _lupi_features @classmethod def parameters ( cls ) : return [ \"C\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ] @property def get_initmodel_template ( cls ) : return LUPI_Classification_SVM @property def get_cvxproblem_template ( cls ) : return LUPI_Classification_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 return X , y Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods parameters def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ] Instance variables get_cvxproblem_template get_initmodel_template lupi_features Methods get_all_parameters def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } get_all_relax_factors def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } get_chosen_parameter def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) get_chosen_relax_factors def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor get_relaxed_constraints def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } postprocessing def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds preprocessing def preprocessing ( self , data , lupi_features = None ) View Source def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 return X , y relax_constraint def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key )) relax_factors def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] LUPI_Classification_Relevance_Bound class LUPI_Classification_Relevance_Bound ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Classification_Relevance_Bound ( LUPI_Relevance_CVXProblem , Classification_Relevance_Bound ): def _init_objective_UB_LUPI ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w_priv [ self . lupi_index ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_objective_LB_LUPI ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w_priv [ self . lupi_index ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from best initial model l1_w = init_model_constraints [ \"w_l1\" ] l1_priv_w = init_model_constraints [ \"w_priv_l1\" ] init_loss = init_model_constraints [ \"loss\" ] # New Variables w = cvx . Variable ( shape =( self . d ), name = \"w\" ) w_priv = cvx . Variable ( shape =( self . d_priv ), name = \"w_priv\" ) b = cvx . Variable ( name = \"b\" ) b_priv = cvx . Variable ( name = \"b_priv\" ) slack = cvx . Variable ( shape =( self . n )) # New Constraints function = cvx . multiply ( self . y . T , self . X * w + b ) priv_function = self . X_priv * w_priv + b_priv loss = cvx . sum ( priv_function ) + cvx . sum ( slack ) weight_norm = cvx . norm ( w , 1 ) weight_norm_priv = cvx . norm ( w_priv , 1 ) self . add_constraint ( function >= 1 - cvx . multiply ( self . y . T , priv_function ) - slack ) self . add_constraint ( priv_function >= 0 ) self . add_constraint ( loss <= init_loss ) self . add_constraint ( weight_norm + weight_norm_priv <= l1_w + l1_priv_w ) self . add_constraint ( slack >= 0 ) # Save values for object use later self . w = w self . w_priv = w_priv self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Ancestors (in MRO) fri.model.base_lupi.LUPI_Relevance_CVXProblem fri.model.classification.Classification_Relevance_Bound fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC Static methods aggregate_max_candidates def aggregate_max_candidates ( max_problems_candidates ) View Source @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value aggregate_min_candidates def aggregate_min_candidates ( min_problems_candidates ) View Source @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value generate_lower_bound_problem def generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem generate_upper_bound_problem def generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : for sign in [ -1, 1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem Instance variables accepted_status constraints cvx_problem isProbe is_solved objective probeID solved_relevance solver_kwargs Methods add_constraint def add_constraint ( self , new ) View Source def add_constraint ( self , new ): self . _constraints . append ( new ) init_objective_LB def init_objective_LB ( self , ** kwargs ) View Source def init_objective_LB ( self , ** kwargs ): # We have two models basically with different indexes if self . isPriv : self . _init_objective_LB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_LB ( ** kwargs ) init_objective_UB def init_objective_UB ( self , ** kwargs ) View Source def init_objective_UB ( self , ** kwargs ): # We have two models basically with different indexes if self . isPriv : self . _init_objective_UB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_UB ( ** kwargs ) preprocessing_data def preprocessing_data ( self , data , best_model_state ) View Source def preprocessing_data ( self , data , best_model_state ): lupi_features = best_model_state [ \"lupi_features\" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super (). preprocessing_data (( X , y ), best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False solve def solve ( self ) -> object View Source def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self LUPI_Classification_SVM class LUPI_Classification_SVM ( C = 1 , scaling_lupi_w = 1 , scaling_lupi_loss = 1 , lupi_features = None ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Classification_SVM ( InitModel ): HYPERPARAMETER = [ \"C\" , \"scaling_lupi_w\" , \"scaling_lupi_loss\" ] def __init__ ( self , C = 1 , scaling_lupi_w = 1 , scaling_lupi_loss = 1 , lupi_features = None ): super (). __init__ () self . lupi_features = lupi_features self . scaling_lupi_loss = scaling_lupi_loss self . scaling_lupi_w = scaling_lupi_w self . C = C def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None: try: lupi_features = self . lupi_features self . lupi_features = lupi_features except: raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . get_params ()[ \"C\" ] scaling_lupi_w = self . get_params ()[ \"scaling_lupi_w\" ] scaling_lupi_loss = self . get_params ()[ \"scaling_lupi_loss\" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape =( d ), name = \"w\" ) w_priv = cvx . Variable ( lupi_features , name = \"w_priv\" ) b = cvx . Variable ( name = \"bias\" ) b_priv = cvx . Variable ( name = \"bias_priv\" ) # Define functions for better readability function = X * w + b priv_function = X_priv * w_priv + b_priv slack = cvx . Variable ( shape =( n )) # Combined loss of lupi function and normal slacks, scaled by two constants loss = scaling_lupi_loss * cvx . sum ( priv_function ) + cvx . sum ( slack ) # L1 norm regularization of both functions with 1 scaling constant w_l1 = cvx . norm ( w , 1 ) w_priv_l1 = cvx . norm ( w_priv , 1 ) weight_regularization = 0.5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [ cvx . multiply ( y . T , function ) >= 1 - cvx . multiply ( y . T , priv_function ) - slack , priv_function >= 0 , slack >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve (** self . SOLVER_PARAMS ) w = w . value w_priv = w_priv . value b = b . value b_priv = b_priv . value self . model_state = { \"w\" : w , \"w_priv\" : w_priv , \"b\" : b , \"b_priv\" : b_priv , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO: Move this somewhere else } loss = loss . value w_l1 = w_l1 . value w_priv_l1 = w_priv_l1 . value self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 , \"w_priv_l1\" : w_priv_l1 } return self def predict ( self , X ): X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] # Simple hyperplane classification rule f = np . dot ( X , w ) + b y = f >= 0 y = y . astype ( int ) # Format binary as signed unit vector y [ y == 0 ] = - 1 return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) # Negative class is set to -1 for decision surface y = LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \"weighted\" ) if \"verbose\" in kwargs: return classification_report ( y , prediction ) return score Ancestors (in MRO) fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator Class variables HYPERPARAMETER SOLVER_PARAMS Instance variables L1_factor Methods fit def fit ( self , X_combined , y , lupi_features = None ) Parameters lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. View Source def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : try : lupi_features = self . lupi_features self . lupi_features = lupi_features except : raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . get_params ()[ \"C\" ] scaling_lupi_w = self . get_params ()[ \"scaling_lupi_w\" ] scaling_lupi_loss = self . get_params ()[ \"scaling_lupi_loss\" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) w_priv = cvx . Variable ( lupi_features , name = \"w_priv\" ) b = cvx . Variable ( name = \"bias\" ) b_priv = cvx . Variable ( name = \"bias_priv\" ) # Define functions for better readability function = X * w + b priv_function = X_priv * w_priv + b_priv slack = cvx . Variable ( shape = ( n )) # Combined loss of lupi function and normal slacks , scaled by two constants loss = scaling_lupi_loss * cvx . sum ( priv_function ) + cvx . sum ( slack ) # L1 norm regularization of both functions with 1 scaling constant w_l1 = cvx . norm ( w , 1 ) w_priv_l1 = cvx . norm ( w_priv , 1 ) weight_regularization = 0 . 5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [ cvx . multiply ( y . T , function ) >= 1 - cvx . multiply ( y . T , priv_function ) - slack , priv_function >= 0 , slack >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value w_priv = w_priv . value b = b . value b_priv = b_priv . value self . model_state = { \"w\" : w , \"w_priv\" : w_priv , \"b\" : b , \"b_priv\" : b_priv , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else } loss = loss . value w_l1 = w_l1 . value w_priv_l1 = w_priv_l1 . value self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 , \"w_priv_l1\" : w_priv_l1 } return self get_params def get_params ( self , deep = True ) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out make_scorer def make_scorer ( self ) View Source def make_scorer ( self ): return None , None predict def predict ( self , X ) View Source def predict ( self , X ): X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] # Simple hyperplane classification rule f = np . dot ( X , w ) + b y = f >= 0 y = y . astype ( int ) # Format binary as signed unit vector y [ y == 0 ] = - 1 return y score def score ( self , X , y , ** kwargs ) View Source def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) # Negative class is set to - 1 for decision surface y = LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \"weighted\" ) if \"verbose\" in kwargs : return classification_report ( y , prediction ) return score set_params def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Lupi Classification"},{"location":"reference/fri/model/lupi_classification/#module-frimodellupi_classification","text":"View Source import cvxpy as cvx import numpy as np from sklearn import preprocessing from sklearn.metrics import fbeta_score , classification_report from sklearn.preprocessing import LabelEncoder from sklearn.utils import check_X_y from sklearn.utils.multiclass import unique_labels from .base_initmodel import InitModel from .base_lupi import LUPI_Relevance_CVXProblem , split_dataset from .base_type import ProblemType from .classification import Classification_Relevance_Bound class LUPI_Classification ( ProblemType ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ): return self . _lupi_features @classmethod def parameters ( cls ): return [ \"C\" , \"scaling_lupi_w\" , \"scaling_lupi_loss\" ] @property def get_initmodel_template ( cls ): return LUPI_Classification_SVM @property def get_cvxproblem_template ( cls ): return LUPI_Classification_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to -1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y class LUPI_Classification_SVM ( InitModel ): HYPERPARAMETER = [ \"C\" , \"scaling_lupi_w\" , \"scaling_lupi_loss\" ] def __init__ ( self , C = 1 , scaling_lupi_w = 1 , scaling_lupi_loss = 1 , lupi_features = None ): super () . __init__ () self . lupi_features = lupi_features self . scaling_lupi_loss = scaling_lupi_loss self . scaling_lupi_w = scaling_lupi_w self . C = C def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : try : lupi_features = self . lupi_features self . lupi_features = lupi_features except : raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . get_params ()[ \"C\" ] scaling_lupi_w = self . get_params ()[ \"scaling_lupi_w\" ] scaling_lupi_loss = self . get_params ()[ \"scaling_lupi_loss\" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) w_priv = cvx . Variable ( lupi_features , name = \"w_priv\" ) b = cvx . Variable ( name = \"bias\" ) b_priv = cvx . Variable ( name = \"bias_priv\" ) # Define functions for better readability function = X * w + b priv_function = X_priv * w_priv + b_priv slack = cvx . Variable ( shape = ( n )) # Combined loss of lupi function and normal slacks, scaled by two constants loss = scaling_lupi_loss * cvx . sum ( priv_function ) + cvx . sum ( slack ) # L1 norm regularization of both functions with 1 scaling constant w_l1 = cvx . norm ( w , 1 ) w_priv_l1 = cvx . norm ( w_priv , 1 ) weight_regularization = 0.5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [ cvx . multiply ( y . T , function ) >= 1 - cvx . multiply ( y . T , priv_function ) - slack , priv_function >= 0 , slack >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value w_priv = w_priv . value b = b . value b_priv = b_priv . value self . model_state = { \"w\" : w , \"w_priv\" : w_priv , \"b\" : b , \"b_priv\" : b_priv , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO: Move this somewhere else } loss = loss . value w_l1 = w_l1 . value w_priv_l1 = w_priv_l1 . value self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 , \"w_priv_l1\" : w_priv_l1 } return self def predict ( self , X ): X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] # Simple hyperplane classification rule f = np . dot ( X , w ) + b y = f >= 0 y = y . astype ( int ) # Format binary as signed unit vector y [ y == 0 ] = - 1 return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) # Negative class is set to -1 for decision surface y = LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \"weighted\" ) if \"verbose\" in kwargs : return classification_report ( y , prediction ) return score class LUPI_Classification_Relevance_Bound ( LUPI_Relevance_CVXProblem , Classification_Relevance_Bound ): def _init_objective_UB_LUPI ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w_priv [ self . lupi_index ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_objective_LB_LUPI ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w_priv [ self . lupi_index ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from best initial model l1_w = init_model_constraints [ \"w_l1\" ] l1_priv_w = init_model_constraints [ \"w_priv_l1\" ] init_loss = init_model_constraints [ \"loss\" ] # New Variables w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) w_priv = cvx . Variable ( shape = ( self . d_priv ), name = \"w_priv\" ) b = cvx . Variable ( name = \"b\" ) b_priv = cvx . Variable ( name = \"b_priv\" ) slack = cvx . Variable ( shape = ( self . n )) # New Constraints function = cvx . multiply ( self . y . T , self . X * w + b ) priv_function = self . X_priv * w_priv + b_priv loss = cvx . sum ( priv_function ) + cvx . sum ( slack ) weight_norm = cvx . norm ( w , 1 ) weight_norm_priv = cvx . norm ( w_priv , 1 ) self . add_constraint ( function >= 1 - cvx . multiply ( self . y . T , priv_function ) - slack ) self . add_constraint ( priv_function >= 0 ) self . add_constraint ( loss <= init_loss ) self . add_constraint ( weight_norm + weight_norm_priv <= l1_w + l1_priv_w ) self . add_constraint ( slack >= 0 ) # Save values for object use later self . w = w self . w_priv = w_priv self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"Module fri.model.lupi_classification"},{"location":"reference/fri/model/lupi_classification/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/lupi_classification/#lupi_classification","text":"class LUPI_Classification ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Classification ( ProblemType ) : def __init__ ( self , ** kwargs ) : super (). __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ) : return self . _lupi_features @classmethod def parameters ( cls ) : return [ \"C\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ] @property def get_initmodel_template ( cls ) : return LUPI_Classification_SVM @property def get_cvxproblem_template ( cls ) : return LUPI_Classification_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 return X , y","title":"LUPI_Classification"},{"location":"reference/fri/model/lupi_classification/#ancestors-in-mro","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_classification/#static-methods","text":"","title":"Static methods"},{"location":"reference/fri/model/lupi_classification/#parameters","text":"def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ]","title":"parameters"},{"location":"reference/fri/model/lupi_classification/#instance-variables","text":"get_cvxproblem_template get_initmodel_template lupi_features","title":"Instance variables"},{"location":"reference/fri/model/lupi_classification/#methods","text":"","title":"Methods"},{"location":"reference/fri/model/lupi_classification/#get_all_parameters","text":"def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () }","title":"get_all_parameters"},{"location":"reference/fri/model/lupi_classification/#get_all_relax_factors","text":"def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () }","title":"get_all_relax_factors"},{"location":"reference/fri/model/lupi_classification/#get_chosen_parameter","text":"def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 )","title":"get_chosen_parameter"},{"location":"reference/fri/model/lupi_classification/#get_chosen_relax_factors","text":"def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor","title":"get_chosen_relax_factors"},{"location":"reference/fri/model/lupi_classification/#get_relaxed_constraints","text":"def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () }","title":"get_relaxed_constraints"},{"location":"reference/fri/model/lupi_classification/#postprocessing","text":"def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds","title":"postprocessing"},{"location":"reference/fri/model/lupi_classification/#preprocessing","text":"def preprocessing ( self , data , lupi_features = None ) View Source def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 return X , y","title":"preprocessing"},{"location":"reference/fri/model/lupi_classification/#relax_constraint","text":"def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"relax_constraint"},{"location":"reference/fri/model/lupi_classification/#relax_factors","text":"def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ]","title":"relax_factors"},{"location":"reference/fri/model/lupi_classification/#lupi_classification_relevance_bound","text":"class LUPI_Classification_Relevance_Bound ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Classification_Relevance_Bound ( LUPI_Relevance_CVXProblem , Classification_Relevance_Bound ): def _init_objective_UB_LUPI ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w_priv [ self . lupi_index ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_objective_LB_LUPI ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w_priv [ self . lupi_index ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from best initial model l1_w = init_model_constraints [ \"w_l1\" ] l1_priv_w = init_model_constraints [ \"w_priv_l1\" ] init_loss = init_model_constraints [ \"loss\" ] # New Variables w = cvx . Variable ( shape =( self . d ), name = \"w\" ) w_priv = cvx . Variable ( shape =( self . d_priv ), name = \"w_priv\" ) b = cvx . Variable ( name = \"b\" ) b_priv = cvx . Variable ( name = \"b_priv\" ) slack = cvx . Variable ( shape =( self . n )) # New Constraints function = cvx . multiply ( self . y . T , self . X * w + b ) priv_function = self . X_priv * w_priv + b_priv loss = cvx . sum ( priv_function ) + cvx . sum ( slack ) weight_norm = cvx . norm ( w , 1 ) weight_norm_priv = cvx . norm ( w_priv , 1 ) self . add_constraint ( function >= 1 - cvx . multiply ( self . y . T , priv_function ) - slack ) self . add_constraint ( priv_function >= 0 ) self . add_constraint ( loss <= init_loss ) self . add_constraint ( weight_norm + weight_norm_priv <= l1_w + l1_priv_w ) self . add_constraint ( slack >= 0 ) # Save values for object use later self . w = w self . w_priv = w_priv self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"LUPI_Classification_Relevance_Bound"},{"location":"reference/fri/model/lupi_classification/#ancestors-in-mro_1","text":"fri.model.base_lupi.LUPI_Relevance_CVXProblem fri.model.classification.Classification_Relevance_Bound fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_classification/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/fri/model/lupi_classification/#aggregate_max_candidates","text":"def aggregate_max_candidates ( max_problems_candidates ) View Source @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value","title":"aggregate_max_candidates"},{"location":"reference/fri/model/lupi_classification/#aggregate_min_candidates","text":"def aggregate_min_candidates ( min_problems_candidates ) View Source @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value","title":"aggregate_min_candidates"},{"location":"reference/fri/model/lupi_classification/#generate_lower_bound_problem","text":"def generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem","title":"generate_lower_bound_problem"},{"location":"reference/fri/model/lupi_classification/#generate_upper_bound_problem","text":"def generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : for sign in [ -1, 1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem","title":"generate_upper_bound_problem"},{"location":"reference/fri/model/lupi_classification/#instance-variables_1","text":"accepted_status constraints cvx_problem isProbe is_solved objective probeID solved_relevance solver_kwargs","title":"Instance variables"},{"location":"reference/fri/model/lupi_classification/#methods_1","text":"","title":"Methods"},{"location":"reference/fri/model/lupi_classification/#add_constraint","text":"def add_constraint ( self , new ) View Source def add_constraint ( self , new ): self . _constraints . append ( new )","title":"add_constraint"},{"location":"reference/fri/model/lupi_classification/#init_objective_lb","text":"def init_objective_LB ( self , ** kwargs ) View Source def init_objective_LB ( self , ** kwargs ): # We have two models basically with different indexes if self . isPriv : self . _init_objective_LB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_LB ( ** kwargs )","title":"init_objective_LB"},{"location":"reference/fri/model/lupi_classification/#init_objective_ub","text":"def init_objective_UB ( self , ** kwargs ) View Source def init_objective_UB ( self , ** kwargs ): # We have two models basically with different indexes if self . isPriv : self . _init_objective_UB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_UB ( ** kwargs )","title":"init_objective_UB"},{"location":"reference/fri/model/lupi_classification/#preprocessing_data","text":"def preprocessing_data ( self , data , best_model_state ) View Source def preprocessing_data ( self , data , best_model_state ): lupi_features = best_model_state [ \"lupi_features\" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super (). preprocessing_data (( X , y ), best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False","title":"preprocessing_data"},{"location":"reference/fri/model/lupi_classification/#solve","text":"def solve ( self ) -> object View Source def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self","title":"solve"},{"location":"reference/fri/model/lupi_classification/#lupi_classification_svm","text":"class LUPI_Classification_SVM ( C = 1 , scaling_lupi_w = 1 , scaling_lupi_loss = 1 , lupi_features = None ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Classification_SVM ( InitModel ): HYPERPARAMETER = [ \"C\" , \"scaling_lupi_w\" , \"scaling_lupi_loss\" ] def __init__ ( self , C = 1 , scaling_lupi_w = 1 , scaling_lupi_loss = 1 , lupi_features = None ): super (). __init__ () self . lupi_features = lupi_features self . scaling_lupi_loss = scaling_lupi_loss self . scaling_lupi_w = scaling_lupi_w self . C = C def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None: try: lupi_features = self . lupi_features self . lupi_features = lupi_features except: raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . get_params ()[ \"C\" ] scaling_lupi_w = self . get_params ()[ \"scaling_lupi_w\" ] scaling_lupi_loss = self . get_params ()[ \"scaling_lupi_loss\" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape =( d ), name = \"w\" ) w_priv = cvx . Variable ( lupi_features , name = \"w_priv\" ) b = cvx . Variable ( name = \"bias\" ) b_priv = cvx . Variable ( name = \"bias_priv\" ) # Define functions for better readability function = X * w + b priv_function = X_priv * w_priv + b_priv slack = cvx . Variable ( shape =( n )) # Combined loss of lupi function and normal slacks, scaled by two constants loss = scaling_lupi_loss * cvx . sum ( priv_function ) + cvx . sum ( slack ) # L1 norm regularization of both functions with 1 scaling constant w_l1 = cvx . norm ( w , 1 ) w_priv_l1 = cvx . norm ( w_priv , 1 ) weight_regularization = 0.5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [ cvx . multiply ( y . T , function ) >= 1 - cvx . multiply ( y . T , priv_function ) - slack , priv_function >= 0 , slack >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve (** self . SOLVER_PARAMS ) w = w . value w_priv = w_priv . value b = b . value b_priv = b_priv . value self . model_state = { \"w\" : w , \"w_priv\" : w_priv , \"b\" : b , \"b_priv\" : b_priv , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO: Move this somewhere else } loss = loss . value w_l1 = w_l1 . value w_priv_l1 = w_priv_l1 . value self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 , \"w_priv_l1\" : w_priv_l1 } return self def predict ( self , X ): X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] # Simple hyperplane classification rule f = np . dot ( X , w ) + b y = f >= 0 y = y . astype ( int ) # Format binary as signed unit vector y [ y == 0 ] = - 1 return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) # Negative class is set to -1 for decision surface y = LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \"weighted\" ) if \"verbose\" in kwargs: return classification_report ( y , prediction ) return score","title":"LUPI_Classification_SVM"},{"location":"reference/fri/model/lupi_classification/#ancestors-in-mro_2","text":"fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_classification/#class-variables","text":"HYPERPARAMETER SOLVER_PARAMS","title":"Class variables"},{"location":"reference/fri/model/lupi_classification/#instance-variables_2","text":"L1_factor","title":"Instance variables"},{"location":"reference/fri/model/lupi_classification/#methods_2","text":"","title":"Methods"},{"location":"reference/fri/model/lupi_classification/#fit","text":"def fit ( self , X_combined , y , lupi_features = None )","title":"fit"},{"location":"reference/fri/model/lupi_classification/#parameters_1","text":"lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. View Source def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : try : lupi_features = self . lupi_features self . lupi_features = lupi_features except : raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . get_params ()[ \"C\" ] scaling_lupi_w = self . get_params ()[ \"scaling_lupi_w\" ] scaling_lupi_loss = self . get_params ()[ \"scaling_lupi_loss\" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) w_priv = cvx . Variable ( lupi_features , name = \"w_priv\" ) b = cvx . Variable ( name = \"bias\" ) b_priv = cvx . Variable ( name = \"bias_priv\" ) # Define functions for better readability function = X * w + b priv_function = X_priv * w_priv + b_priv slack = cvx . Variable ( shape = ( n )) # Combined loss of lupi function and normal slacks , scaled by two constants loss = scaling_lupi_loss * cvx . sum ( priv_function ) + cvx . sum ( slack ) # L1 norm regularization of both functions with 1 scaling constant w_l1 = cvx . norm ( w , 1 ) w_priv_l1 = cvx . norm ( w_priv , 1 ) weight_regularization = 0 . 5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [ cvx . multiply ( y . T , function ) >= 1 - cvx . multiply ( y . T , priv_function ) - slack , priv_function >= 0 , slack >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value w_priv = w_priv . value b = b . value b_priv = b_priv . value self . model_state = { \"w\" : w , \"w_priv\" : w_priv , \"b\" : b , \"b_priv\" : b_priv , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else } loss = loss . value w_l1 = w_l1 . value w_priv_l1 = w_priv_l1 . value self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 , \"w_priv_l1\" : w_priv_l1 } return self","title":"Parameters"},{"location":"reference/fri/model/lupi_classification/#get_params","text":"def get_params ( self , deep = True ) Get parameters for this estimator.","title":"get_params"},{"location":"reference/fri/model/lupi_classification/#parameters_2","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"reference/fri/model/lupi_classification/#returns","text":"params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out","title":"Returns"},{"location":"reference/fri/model/lupi_classification/#make_scorer","text":"def make_scorer ( self ) View Source def make_scorer ( self ): return None , None","title":"make_scorer"},{"location":"reference/fri/model/lupi_classification/#predict","text":"def predict ( self , X ) View Source def predict ( self , X ): X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] # Simple hyperplane classification rule f = np . dot ( X , w ) + b y = f >= 0 y = y . astype ( int ) # Format binary as signed unit vector y [ y == 0 ] = - 1 return y","title":"predict"},{"location":"reference/fri/model/lupi_classification/#score","text":"def score ( self , X , y , ** kwargs ) View Source def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) # Negative class is set to - 1 for decision surface y = LabelEncoder (). fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \"weighted\" ) if \"verbose\" in kwargs : return classification_report ( y , prediction ) return score","title":"score"},{"location":"reference/fri/model/lupi_classification/#set_params","text":"def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params"},{"location":"reference/fri/model/lupi_classification/#parameters_3","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"reference/fri/model/lupi_classification/#returns_1","text":"self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Returns"},{"location":"reference/fri/model/lupi_ordinal_regression/","text":"Module fri.model.lupi_ordinal_regression View Source from itertools import product import cvxpy as cvx import numpy as np from sklearn.metrics import make_scorer from sklearn.utils import check_X_y from fri.model.base_lupi import ( LUPI_Relevance_CVXProblem , split_dataset , is_lupi_feature , ) from fri.model.ordinal_regression import ( OrdinalRegression_Relevance_Bound , ordinal_scores , ) from .base_initmodel import LUPI_InitModel from .base_type import ProblemType class LUPI_OrdinalRegression ( ProblemType ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ): return self . _lupi_features @classmethod def parameters ( cls ): return [ \"C\" , \"scaling_lupi_w\" ] @property def get_initmodel_template ( cls ): return LUPI_OrdinalRegression_SVM @property def get_cvxproblem_template ( cls ): return LUPI_OrdinalRegression_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y class LUPI_OrdinalRegression_SVM ( LUPI_InitModel ): HYPERPARAMETER = [ \"C\" , \"scaling_lupi_w\" ] def __init__ ( self , C = 1 , scaling_lupi_w = 1 , lupi_features = None ): super () . __init__ () self . scaling_lupi_w = scaling_lupi_w self . C = C self . lupi_features = lupi_features def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : try : lupi_features = self . lupi_features self . lupi_features = lupi_features except : raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape self . classes_ = np . unique ( y ) # Get parameters from CV model without any feature contstraints C = self . get_params ()[ \"C\" ] scaling_lupi_w = self . get_params ()[ \"scaling_lupi_w\" ] get_original_bin_name , n_bins = get_bin_mapping ( y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) b_s = cvx . Variable ( shape = ( n_boundaries ), name = \"bias\" ) w_priv = cvx . Variable ( shape = ( self . lupi_features , 2 ), name = \"w_priv\" ) d_priv = cvx . Variable ( shape = ( 2 ), name = \"bias_priv\" ) def priv_function ( bin , sign ): indices = np . where ( y == get_original_bin_name [ bin ]) return X_priv [ indices ] * w_priv [:, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [:, 0 ], 1 ) priv_l1_2 = cvx . norm ( w_priv [:, 1 ], 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) weight_regularization = 0.5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [] loss = 0 for left_bin in range ( 0 , n_bins - 1 ): indices = np . where ( y == get_original_bin_name [ left_bin ]) constraints . append ( X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) constraints . append ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ): indices = np . where ( y == get_original_bin_name [ right_bin ]) constraints . append ( X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) constraints . append ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ): constraints . append ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ]) objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b_s = b_s . value self . model_state = { \"w\" : w , \"b_s\" : b_s , \"w_priv\" : w_priv . value , \"d_priv\" : d_priv . value , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO: Move this somewhere else \"bin_boundaries\" : n_boundaries , } self . constraints = { \"loss\" : loss . value , \"w_l1\" : w_l1 . value , \"w_priv_l1\" : w_priv_l1 . value , } return self def predict ( self , X ): X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b_s = self . model_state [ \"b_s\" ] scores = np . dot ( X , w . T )[ np . newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] def score ( self , X , y , error_type = \"mmae\" , return_error = False , ** kwargs ): X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score def make_scorer ( self ): # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \"mze\" ) mae = make_scorer ( ordinal_scores , error_type = \"mae\" ) mmae = make_scorer ( ordinal_scores , error_type = \"mmae\" ) scorer = { \"mze\" : mze , \"mae\" : mae , \"mmae\" : mmae } return scorer , \"mmae\" def get_bin_mapping ( y ): \"\"\" Get ordered unique classes and corresponding mapping from old names Parameters ---------- y: array of discrete values (int, str) Returns ------- \"\"\" classes_ = np . unique ( y ) original_bins = sorted ( classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) return get_old_bin , n_bins class LUPI_OrdinalRegression_Relevance_Bound ( LUPI_Relevance_CVXProblem , OrdinalRegression_Relevance_Bound ): @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems? if not is_priv : yield from super () . generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign in [ 1 , - 1 ]: problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB ( sign = sign ) problem . isLowerBound = True yield problem @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems? if not is_priv : yield from super () . generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ([ 1 , - 1 ], [ 0 , 1 ]): problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ): vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] # We take the max of mins because we need the necessary contribution over all functions min_value = max ( vals ) return min_value def _init_objective_LB_LUPI ( self , sign = None , bin_index = None , ** kwargs ): self . add_constraint ( sign * self . w_priv [ self . lupi_index , :] <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_objective_UB_LUPI ( self , sign = None , pos = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w_priv [ self . lupi_index , pos ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model init_w_l1 = init_model_constraints [ \"w_l1\" ] init_w_priv_l1 = init_model_constraints [ \"w_priv_l1\" ] init_loss = init_model_constraints [ \"loss\" ] scaling_lupi_w = parameters [ \"scaling_lupi_w\" ] get_original_bin_name , n_bins = get_bin_mapping ( self . y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) b_s = cvx . Variable ( shape = ( n_boundaries ), name = \"bias\" ) w_priv = cvx . Variable ( shape = ( self . d_priv , 2 ), name = \"w_priv\" ) d_priv = cvx . Variable ( shape = ( 2 ), name = \"bias_priv\" ) def priv_function ( bin , sign ): indices = np . where ( self . y == get_original_bin_name [ bin ]) return self . X_priv [ indices ] * w_priv [:, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [:, 0 ], 1 ) priv_l1_2 = cvx . norm ( w_priv [:, 1 ], 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) loss = 0 for left_bin in range ( 0 , n_bins - 1 ): indices = np . where ( self . y == get_original_bin_name [ left_bin ]) self . add_constraint ( self . X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) self . add_constraint ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ): indices = np . where ( self . y == get_original_bin_name [ right_bin ]) self . add_constraint ( self . X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) self . add_constraint ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ): self . add_constraint ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ]) self . add_constraint ( w_l1 + scaling_lupi_w * w_priv_l1 <= init_w_l1 + scaling_lupi_w * init_w_priv_l1 ) self . add_constraint ( loss <= init_loss ) self . w = w self . w_priv = w_priv self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Functions get_bin_mapping def get_bin_mapping ( y ) Get ordered unique classes and corresponding mapping from old names Parameters y: array of discrete values (int, str) Returns View Source def get_bin_mapping ( y ): \"\"\" Get ordered unique classes and corresponding mapping from old names Parameters ---------- y: array of discrete values (int, str) Returns ------- \"\"\" classes_ = np . unique ( y ) original_bins = sorted ( classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) return get_old_bin , n_bins Classes LUPI_OrdinalRegression class LUPI_OrdinalRegression ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_OrdinalRegression ( ProblemType ) : def __init__ ( self , ** kwargs ) : super (). __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ) : return self . _lupi_features @classmethod def parameters ( cls ) : return [ \"C\", \"scaling_lupi_w\" ] @property def get_initmodel_template ( cls ) : return LUPI_OrdinalRegression_SVM @property def get_cvxproblem_template ( cls ) : return LUPI_OrdinalRegression_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods parameters def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\", \"scaling_lupi_w\" ] Instance variables get_cvxproblem_template get_initmodel_template lupi_features Methods get_all_parameters def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } get_all_relax_factors def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } get_chosen_parameter def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) get_chosen_relax_factors def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor get_relaxed_constraints def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } postprocessing def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds preprocessing def preprocessing ( self , data , lupi_features = None ) View Source def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y relax_constraint def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key )) relax_factors def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] LUPI_OrdinalRegression_Relevance_Bound class LUPI_OrdinalRegression_Relevance_Bound ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_OrdinalRegression_Relevance_Bound ( LUPI_Relevance_CVXProblem , OrdinalRegression_Relevance_Bound ) : @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super (). generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign in [ 1, -1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB ( sign = sign ) problem . isLowerBound = True yield problem @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super (). generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ( [ 1, -1 ] , [ 0, 1 ] ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] # We take the max of mins because we need the necessary contribution over all functions min_value = max ( vals ) return min_value def _init_objective_LB_LUPI ( self , sign = None , bin_index = None , ** kwargs ) : self . add_constraint ( sign * self . w_priv [ self.lupi_index, : ] <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_objective_UB_LUPI ( self , sign = None , pos = None , ** kwargs ) : self . add_constraint ( self . feature_relevance <= sign * self . w_priv [ self.lupi_index, pos ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ) : # Upper constraints from initial model init_w_l1 = init_model_constraints [ \"w_l1\" ] init_w_priv_l1 = init_model_constraints [ \"w_priv_l1\" ] init_loss = init_model_constraints [ \"loss\" ] scaling_lupi_w = parameters [ \"scaling_lupi_w\" ] get_original_bin_name , n_bins = get_bin_mapping ( self . y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) b_s = cvx . Variable ( shape = ( n_boundaries ), name = \"bias\" ) w_priv = cvx . Variable ( shape = ( self . d_priv , 2 ), name = \"w_priv\" ) d_priv = cvx . Variable ( shape = ( 2 ), name = \"bias_priv\" ) def priv_function ( bin , sign ) : indices = np . where ( self . y == get_original_bin_name [ bin ] ) return self . X_priv [ indices ] * w_priv [ :, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [ :, 0 ] , 1 ) priv_l1_2 = cvx . norm ( w_priv [ :, 1 ] , 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) loss = 0 for left_bin in range ( 0 , n_bins - 1 ) : indices = np . where ( self . y == get_original_bin_name [ left_bin ] ) self . add_constraint ( self . X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) self . add_constraint ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ) : indices = np . where ( self . y == get_original_bin_name [ right_bin ] ) self . add_constraint ( self . X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) self . add_constraint ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ) : self . add_constraint ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ] ) self . add_constraint ( w_l1 + scaling_lupi_w * w_priv_l1 <= init_w_l1 + scaling_lupi_w * init_w_priv_l1 ) self . add_constraint ( loss <= init_loss ) self . w = w self . w_priv = w_priv self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Ancestors (in MRO) fri.model.base_lupi.LUPI_Relevance_CVXProblem fri.model.ordinal_regression.OrdinalRegression_Relevance_Bound fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC Static methods aggregate_max_candidates def aggregate_max_candidates ( max_problems_candidates ) View Source @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value aggregate_min_candidates def aggregate_min_candidates ( min_problems_candidates ) View Source @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] # We take the max of mins because we need the necessary contribution over all functions min_value = max ( vals ) return min_value generate_lower_bound_problem def generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super (). generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign in [ 1, -1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB ( sign = sign ) problem . isLowerBound = True yield problem generate_upper_bound_problem def generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super (). generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ( [ 1, -1 ] , [ 0, 1 ] ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem Instance variables accepted_status constraints cvx_problem isProbe is_solved objective probeID solved_relevance solver_kwargs Methods add_constraint def add_constraint ( self , new ) View Source def add_constraint ( self , new ): self . _constraints . append ( new ) init_objective_LB def init_objective_LB ( self , ** kwargs ) View Source def init_objective_LB ( self , ** kwargs ): # We have two models basically with different indexes if self . isPriv : self . _init_objective_LB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_LB ( ** kwargs ) init_objective_UB def init_objective_UB ( self , ** kwargs ) View Source def init_objective_UB ( self , ** kwargs ): # We have two models basically with different indexes if self . isPriv : self . _init_objective_UB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_UB ( ** kwargs ) preprocessing_data def preprocessing_data ( self , data , best_model_state ) View Source def preprocessing_data ( self , data , best_model_state ): lupi_features = best_model_state [ \"lupi_features\" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super (). preprocessing_data (( X , y ), best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False solve def solve ( self ) -> object View Source def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self LUPI_OrdinalRegression_SVM class LUPI_OrdinalRegression_SVM ( C = 1 , scaling_lupi_w = 1 , lupi_features = None ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_OrdinalRegression_SVM ( LUPI_InitModel ) : HYPERPARAMETER = [ \"C\", \"scaling_lupi_w\" ] def __init__ ( self , C = 1 , scaling_lupi_w = 1 , lupi_features = None ) : super (). __init__ () self . scaling_lupi_w = scaling_lupi_w self . C = C self . lupi_features = lupi_features def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : try : lupi_features = self . lupi_features self . lupi_features = lupi_features except : raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape self . classes_ = np . unique ( y ) # Get parameters from CV model without any feature contstraints C = self . get_params () [ \"C\" ] scaling_lupi_w = self . get_params () [ \"scaling_lupi_w\" ] get_original_bin_name , n_bins = get_bin_mapping ( y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) b_s = cvx . Variable ( shape = ( n_boundaries ), name = \"bias\" ) w_priv = cvx . Variable ( shape = ( self . lupi_features , 2 ), name = \"w_priv\" ) d_priv = cvx . Variable ( shape = ( 2 ), name = \"bias_priv\" ) def priv_function ( bin , sign ) : indices = np . where ( y == get_original_bin_name [ bin ] ) return X_priv [ indices ] * w_priv [ :, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [ :, 0 ] , 1 ) priv_l1_2 = cvx . norm ( w_priv [ :, 1 ] , 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) weight_regularization = 0.5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [] loss = 0 for left_bin in range ( 0 , n_bins - 1 ) : indices = np . where ( y == get_original_bin_name [ left_bin ] ) constraints . append ( X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) constraints . append ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ) : indices = np . where ( y == get_original_bin_name [ right_bin ] ) constraints . append ( X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) constraints . append ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ) : constraints . append ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ] ) objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b_s = b_s . value self . model_state = { \"w\" : w , \"b_s\" : b_s , \"w_priv\" : w_priv . value , \"d_priv\" : d_priv . value , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else \"bin_boundaries\" : n_boundaries , } self . constraints = { \"loss\" : loss . value , \"w_l1\" : w_l1 . value , \"w_priv_l1\" : w_priv_l1 . value , } return self def predict ( self , X ) : X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b_s = self . model_state [ \"b_s\" ] scores = np . dot ( X , w . T ) [ np.newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] def score ( self , X , y , error_type = \"mmae\" , return_error = False , ** kwargs ) : X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score def make_scorer ( self ) : # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \"mze\" ) mae = make_scorer ( ordinal_scores , error_type = \"mae\" ) mmae = make_scorer ( ordinal_scores , error_type = \"mmae\" ) scorer = { \"mze\" : mze , \"mae\" : mae , \"mmae\" : mmae } return scorer , \"mmae\" Ancestors (in MRO) fri.model.base_initmodel.LUPI_InitModel fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator Class variables HYPERPARAMETER SOLVER_PARAMS Instance variables L1_factor L1_factor_priv Methods fit def fit ( self , X_combined , y , lupi_features = None ) Parameters lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. View Source def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : try : lupi_features = self . lupi_features self . lupi_features = lupi_features except : raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape self . classes_ = np . unique ( y ) # Get parameters from CV model without any feature contstraints C = self . get_params () [ \"C\" ] scaling_lupi_w = self . get_params () [ \"scaling_lupi_w\" ] get_original_bin_name , n_bins = get_bin_mapping ( y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) b_s = cvx . Variable ( shape = ( n_boundaries ), name = \"bias\" ) w_priv = cvx . Variable ( shape = ( self . lupi_features , 2 ), name = \"w_priv\" ) d_priv = cvx . Variable ( shape = ( 2 ), name = \"bias_priv\" ) def priv_function ( bin , sign ) : indices = np . where ( y == get_original_bin_name [ bin ] ) return X_priv [ indices ] * w_priv [ :, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [ :, 0 ] , 1 ) priv_l1_2 = cvx . norm ( w_priv [ :, 1 ] , 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) weight_regularization = 0.5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [] loss = 0 for left_bin in range ( 0 , n_bins - 1 ) : indices = np . where ( y == get_original_bin_name [ left_bin ] ) constraints . append ( X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) constraints . append ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ) : indices = np . where ( y == get_original_bin_name [ right_bin ] ) constraints . append ( X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) constraints . append ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ) : constraints . append ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ] ) objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b_s = b_s . value self . model_state = { \"w\" : w , \"b_s\" : b_s , \"w_priv\" : w_priv . value , \"d_priv\" : d_priv . value , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else \"bin_boundaries\" : n_boundaries , } self . constraints = { \"loss\" : loss . value , \"w_l1\" : w_l1 . value , \"w_priv_l1\" : w_priv_l1 . value , } return self get_params def get_params ( self , deep = True ) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out make_scorer def make_scorer ( self ) View Source def make_scorer ( self ): # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \"mze\" ) mae = make_scorer ( ordinal_scores , error_type = \"mae\" ) mmae = make_scorer ( ordinal_scores , error_type = \"mmae\" ) scorer = { \"mze\" : mze , \"mae\" : mae , \"mmae\" : mmae } return scorer , \"mmae\" predict def predict ( self , X ) View Source def predict ( self , X ) : X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b_s = self . model_state [ \"b_s\" ] scores = np . dot ( X , w . T ) [ np.newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] score def score ( self , X , y , error_type = 'mmae' , return_error = False , ** kwargs ) View Source def score ( self , X , y , error_type = \"mmae\" , return_error = False , ** kwargs ): X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score set_params def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Lupi Ordinal Regression"},{"location":"reference/fri/model/lupi_ordinal_regression/#module-frimodellupi_ordinal_regression","text":"View Source from itertools import product import cvxpy as cvx import numpy as np from sklearn.metrics import make_scorer from sklearn.utils import check_X_y from fri.model.base_lupi import ( LUPI_Relevance_CVXProblem , split_dataset , is_lupi_feature , ) from fri.model.ordinal_regression import ( OrdinalRegression_Relevance_Bound , ordinal_scores , ) from .base_initmodel import LUPI_InitModel from .base_type import ProblemType class LUPI_OrdinalRegression ( ProblemType ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ): return self . _lupi_features @classmethod def parameters ( cls ): return [ \"C\" , \"scaling_lupi_w\" ] @property def get_initmodel_template ( cls ): return LUPI_OrdinalRegression_SVM @property def get_cvxproblem_template ( cls ): return LUPI_OrdinalRegression_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y class LUPI_OrdinalRegression_SVM ( LUPI_InitModel ): HYPERPARAMETER = [ \"C\" , \"scaling_lupi_w\" ] def __init__ ( self , C = 1 , scaling_lupi_w = 1 , lupi_features = None ): super () . __init__ () self . scaling_lupi_w = scaling_lupi_w self . C = C self . lupi_features = lupi_features def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : try : lupi_features = self . lupi_features self . lupi_features = lupi_features except : raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape self . classes_ = np . unique ( y ) # Get parameters from CV model without any feature contstraints C = self . get_params ()[ \"C\" ] scaling_lupi_w = self . get_params ()[ \"scaling_lupi_w\" ] get_original_bin_name , n_bins = get_bin_mapping ( y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) b_s = cvx . Variable ( shape = ( n_boundaries ), name = \"bias\" ) w_priv = cvx . Variable ( shape = ( self . lupi_features , 2 ), name = \"w_priv\" ) d_priv = cvx . Variable ( shape = ( 2 ), name = \"bias_priv\" ) def priv_function ( bin , sign ): indices = np . where ( y == get_original_bin_name [ bin ]) return X_priv [ indices ] * w_priv [:, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [:, 0 ], 1 ) priv_l1_2 = cvx . norm ( w_priv [:, 1 ], 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) weight_regularization = 0.5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [] loss = 0 for left_bin in range ( 0 , n_bins - 1 ): indices = np . where ( y == get_original_bin_name [ left_bin ]) constraints . append ( X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) constraints . append ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ): indices = np . where ( y == get_original_bin_name [ right_bin ]) constraints . append ( X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) constraints . append ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ): constraints . append ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ]) objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b_s = b_s . value self . model_state = { \"w\" : w , \"b_s\" : b_s , \"w_priv\" : w_priv . value , \"d_priv\" : d_priv . value , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO: Move this somewhere else \"bin_boundaries\" : n_boundaries , } self . constraints = { \"loss\" : loss . value , \"w_l1\" : w_l1 . value , \"w_priv_l1\" : w_priv_l1 . value , } return self def predict ( self , X ): X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b_s = self . model_state [ \"b_s\" ] scores = np . dot ( X , w . T )[ np . newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] def score ( self , X , y , error_type = \"mmae\" , return_error = False , ** kwargs ): X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score def make_scorer ( self ): # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \"mze\" ) mae = make_scorer ( ordinal_scores , error_type = \"mae\" ) mmae = make_scorer ( ordinal_scores , error_type = \"mmae\" ) scorer = { \"mze\" : mze , \"mae\" : mae , \"mmae\" : mmae } return scorer , \"mmae\" def get_bin_mapping ( y ): \"\"\" Get ordered unique classes and corresponding mapping from old names Parameters ---------- y: array of discrete values (int, str) Returns ------- \"\"\" classes_ = np . unique ( y ) original_bins = sorted ( classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) return get_old_bin , n_bins class LUPI_OrdinalRegression_Relevance_Bound ( LUPI_Relevance_CVXProblem , OrdinalRegression_Relevance_Bound ): @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems? if not is_priv : yield from super () . generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign in [ 1 , - 1 ]: problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB ( sign = sign ) problem . isLowerBound = True yield problem @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems? if not is_priv : yield from super () . generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ([ 1 , - 1 ], [ 0 , 1 ]): problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ): vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] # We take the max of mins because we need the necessary contribution over all functions min_value = max ( vals ) return min_value def _init_objective_LB_LUPI ( self , sign = None , bin_index = None , ** kwargs ): self . add_constraint ( sign * self . w_priv [ self . lupi_index , :] <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_objective_UB_LUPI ( self , sign = None , pos = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w_priv [ self . lupi_index , pos ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model init_w_l1 = init_model_constraints [ \"w_l1\" ] init_w_priv_l1 = init_model_constraints [ \"w_priv_l1\" ] init_loss = init_model_constraints [ \"loss\" ] scaling_lupi_w = parameters [ \"scaling_lupi_w\" ] get_original_bin_name , n_bins = get_bin_mapping ( self . y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) b_s = cvx . Variable ( shape = ( n_boundaries ), name = \"bias\" ) w_priv = cvx . Variable ( shape = ( self . d_priv , 2 ), name = \"w_priv\" ) d_priv = cvx . Variable ( shape = ( 2 ), name = \"bias_priv\" ) def priv_function ( bin , sign ): indices = np . where ( self . y == get_original_bin_name [ bin ]) return self . X_priv [ indices ] * w_priv [:, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [:, 0 ], 1 ) priv_l1_2 = cvx . norm ( w_priv [:, 1 ], 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) loss = 0 for left_bin in range ( 0 , n_bins - 1 ): indices = np . where ( self . y == get_original_bin_name [ left_bin ]) self . add_constraint ( self . X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) self . add_constraint ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ): indices = np . where ( self . y == get_original_bin_name [ right_bin ]) self . add_constraint ( self . X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) self . add_constraint ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ): self . add_constraint ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ]) self . add_constraint ( w_l1 + scaling_lupi_w * w_priv_l1 <= init_w_l1 + scaling_lupi_w * init_w_priv_l1 ) self . add_constraint ( loss <= init_loss ) self . w = w self . w_priv = w_priv self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"Module fri.model.lupi_ordinal_regression"},{"location":"reference/fri/model/lupi_ordinal_regression/#functions","text":"","title":"Functions"},{"location":"reference/fri/model/lupi_ordinal_regression/#get_bin_mapping","text":"def get_bin_mapping ( y ) Get ordered unique classes and corresponding mapping from old names Parameters y: array of discrete values (int, str) Returns View Source def get_bin_mapping ( y ): \"\"\" Get ordered unique classes and corresponding mapping from old names Parameters ---------- y: array of discrete values (int, str) Returns ------- \"\"\" classes_ = np . unique ( y ) original_bins = sorted ( classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) return get_old_bin , n_bins","title":"get_bin_mapping"},{"location":"reference/fri/model/lupi_ordinal_regression/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/lupi_ordinal_regression/#lupi_ordinalregression","text":"class LUPI_OrdinalRegression ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_OrdinalRegression ( ProblemType ) : def __init__ ( self , ** kwargs ) : super (). __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ) : return self . _lupi_features @classmethod def parameters ( cls ) : return [ \"C\", \"scaling_lupi_w\" ] @property def get_initmodel_template ( cls ) : return LUPI_OrdinalRegression_SVM @property def get_cvxproblem_template ( cls ) : return LUPI_OrdinalRegression_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y","title":"LUPI_OrdinalRegression"},{"location":"reference/fri/model/lupi_ordinal_regression/#ancestors-in-mro","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_ordinal_regression/#static-methods","text":"","title":"Static methods"},{"location":"reference/fri/model/lupi_ordinal_regression/#parameters","text":"def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\", \"scaling_lupi_w\" ]","title":"parameters"},{"location":"reference/fri/model/lupi_ordinal_regression/#instance-variables","text":"get_cvxproblem_template get_initmodel_template lupi_features","title":"Instance variables"},{"location":"reference/fri/model/lupi_ordinal_regression/#methods","text":"","title":"Methods"},{"location":"reference/fri/model/lupi_ordinal_regression/#get_all_parameters","text":"def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () }","title":"get_all_parameters"},{"location":"reference/fri/model/lupi_ordinal_regression/#get_all_relax_factors","text":"def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () }","title":"get_all_relax_factors"},{"location":"reference/fri/model/lupi_ordinal_regression/#get_chosen_parameter","text":"def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 )","title":"get_chosen_parameter"},{"location":"reference/fri/model/lupi_ordinal_regression/#get_chosen_relax_factors","text":"def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor","title":"get_chosen_relax_factors"},{"location":"reference/fri/model/lupi_ordinal_regression/#get_relaxed_constraints","text":"def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () }","title":"get_relaxed_constraints"},{"location":"reference/fri/model/lupi_ordinal_regression/#postprocessing","text":"def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds","title":"postprocessing"},{"location":"reference/fri/model/lupi_ordinal_regression/#preprocessing","text":"def preprocessing ( self , data , lupi_features = None ) View Source def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y","title":"preprocessing"},{"location":"reference/fri/model/lupi_ordinal_regression/#relax_constraint","text":"def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"relax_constraint"},{"location":"reference/fri/model/lupi_ordinal_regression/#relax_factors","text":"def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ]","title":"relax_factors"},{"location":"reference/fri/model/lupi_ordinal_regression/#lupi_ordinalregression_relevance_bound","text":"class LUPI_OrdinalRegression_Relevance_Bound ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_OrdinalRegression_Relevance_Bound ( LUPI_Relevance_CVXProblem , OrdinalRegression_Relevance_Bound ) : @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super (). generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign in [ 1, -1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB ( sign = sign ) problem . isLowerBound = True yield problem @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super (). generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ( [ 1, -1 ] , [ 0, 1 ] ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] # We take the max of mins because we need the necessary contribution over all functions min_value = max ( vals ) return min_value def _init_objective_LB_LUPI ( self , sign = None , bin_index = None , ** kwargs ) : self . add_constraint ( sign * self . w_priv [ self.lupi_index, : ] <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_objective_UB_LUPI ( self , sign = None , pos = None , ** kwargs ) : self . add_constraint ( self . feature_relevance <= sign * self . w_priv [ self.lupi_index, pos ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ) : # Upper constraints from initial model init_w_l1 = init_model_constraints [ \"w_l1\" ] init_w_priv_l1 = init_model_constraints [ \"w_priv_l1\" ] init_loss = init_model_constraints [ \"loss\" ] scaling_lupi_w = parameters [ \"scaling_lupi_w\" ] get_original_bin_name , n_bins = get_bin_mapping ( self . y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) b_s = cvx . Variable ( shape = ( n_boundaries ), name = \"bias\" ) w_priv = cvx . Variable ( shape = ( self . d_priv , 2 ), name = \"w_priv\" ) d_priv = cvx . Variable ( shape = ( 2 ), name = \"bias_priv\" ) def priv_function ( bin , sign ) : indices = np . where ( self . y == get_original_bin_name [ bin ] ) return self . X_priv [ indices ] * w_priv [ :, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [ :, 0 ] , 1 ) priv_l1_2 = cvx . norm ( w_priv [ :, 1 ] , 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) loss = 0 for left_bin in range ( 0 , n_bins - 1 ) : indices = np . where ( self . y == get_original_bin_name [ left_bin ] ) self . add_constraint ( self . X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) self . add_constraint ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ) : indices = np . where ( self . y == get_original_bin_name [ right_bin ] ) self . add_constraint ( self . X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) self . add_constraint ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ) : self . add_constraint ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ] ) self . add_constraint ( w_l1 + scaling_lupi_w * w_priv_l1 <= init_w_l1 + scaling_lupi_w * init_w_priv_l1 ) self . add_constraint ( loss <= init_loss ) self . w = w self . w_priv = w_priv self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"LUPI_OrdinalRegression_Relevance_Bound"},{"location":"reference/fri/model/lupi_ordinal_regression/#ancestors-in-mro_1","text":"fri.model.base_lupi.LUPI_Relevance_CVXProblem fri.model.ordinal_regression.OrdinalRegression_Relevance_Bound fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_ordinal_regression/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/fri/model/lupi_ordinal_regression/#aggregate_max_candidates","text":"def aggregate_max_candidates ( max_problems_candidates ) View Source @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value","title":"aggregate_max_candidates"},{"location":"reference/fri/model/lupi_ordinal_regression/#aggregate_min_candidates","text":"def aggregate_min_candidates ( min_problems_candidates ) View Source @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] # We take the max of mins because we need the necessary contribution over all functions min_value = max ( vals ) return min_value","title":"aggregate_min_candidates"},{"location":"reference/fri/model/lupi_ordinal_regression/#generate_lower_bound_problem","text":"def generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super (). generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign in [ 1, -1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB ( sign = sign ) problem . isLowerBound = True yield problem","title":"generate_lower_bound_problem"},{"location":"reference/fri/model/lupi_ordinal_regression/#generate_upper_bound_problem","text":"def generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super (). generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ( [ 1, -1 ] , [ 0, 1 ] ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem","title":"generate_upper_bound_problem"},{"location":"reference/fri/model/lupi_ordinal_regression/#instance-variables_1","text":"accepted_status constraints cvx_problem isProbe is_solved objective probeID solved_relevance solver_kwargs","title":"Instance variables"},{"location":"reference/fri/model/lupi_ordinal_regression/#methods_1","text":"","title":"Methods"},{"location":"reference/fri/model/lupi_ordinal_regression/#add_constraint","text":"def add_constraint ( self , new ) View Source def add_constraint ( self , new ): self . _constraints . append ( new )","title":"add_constraint"},{"location":"reference/fri/model/lupi_ordinal_regression/#init_objective_lb","text":"def init_objective_LB ( self , ** kwargs ) View Source def init_objective_LB ( self , ** kwargs ): # We have two models basically with different indexes if self . isPriv : self . _init_objective_LB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_LB ( ** kwargs )","title":"init_objective_LB"},{"location":"reference/fri/model/lupi_ordinal_regression/#init_objective_ub","text":"def init_objective_UB ( self , ** kwargs ) View Source def init_objective_UB ( self , ** kwargs ): # We have two models basically with different indexes if self . isPriv : self . _init_objective_UB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_UB ( ** kwargs )","title":"init_objective_UB"},{"location":"reference/fri/model/lupi_ordinal_regression/#preprocessing_data","text":"def preprocessing_data ( self , data , best_model_state ) View Source def preprocessing_data ( self , data , best_model_state ): lupi_features = best_model_state [ \"lupi_features\" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super (). preprocessing_data (( X , y ), best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False","title":"preprocessing_data"},{"location":"reference/fri/model/lupi_ordinal_regression/#solve","text":"def solve ( self ) -> object View Source def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self","title":"solve"},{"location":"reference/fri/model/lupi_ordinal_regression/#lupi_ordinalregression_svm","text":"class LUPI_OrdinalRegression_SVM ( C = 1 , scaling_lupi_w = 1 , lupi_features = None ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_OrdinalRegression_SVM ( LUPI_InitModel ) : HYPERPARAMETER = [ \"C\", \"scaling_lupi_w\" ] def __init__ ( self , C = 1 , scaling_lupi_w = 1 , lupi_features = None ) : super (). __init__ () self . scaling_lupi_w = scaling_lupi_w self . C = C self . lupi_features = lupi_features def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : try : lupi_features = self . lupi_features self . lupi_features = lupi_features except : raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape self . classes_ = np . unique ( y ) # Get parameters from CV model without any feature contstraints C = self . get_params () [ \"C\" ] scaling_lupi_w = self . get_params () [ \"scaling_lupi_w\" ] get_original_bin_name , n_bins = get_bin_mapping ( y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) b_s = cvx . Variable ( shape = ( n_boundaries ), name = \"bias\" ) w_priv = cvx . Variable ( shape = ( self . lupi_features , 2 ), name = \"w_priv\" ) d_priv = cvx . Variable ( shape = ( 2 ), name = \"bias_priv\" ) def priv_function ( bin , sign ) : indices = np . where ( y == get_original_bin_name [ bin ] ) return X_priv [ indices ] * w_priv [ :, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [ :, 0 ] , 1 ) priv_l1_2 = cvx . norm ( w_priv [ :, 1 ] , 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) weight_regularization = 0.5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [] loss = 0 for left_bin in range ( 0 , n_bins - 1 ) : indices = np . where ( y == get_original_bin_name [ left_bin ] ) constraints . append ( X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) constraints . append ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ) : indices = np . where ( y == get_original_bin_name [ right_bin ] ) constraints . append ( X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) constraints . append ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ) : constraints . append ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ] ) objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b_s = b_s . value self . model_state = { \"w\" : w , \"b_s\" : b_s , \"w_priv\" : w_priv . value , \"d_priv\" : d_priv . value , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else \"bin_boundaries\" : n_boundaries , } self . constraints = { \"loss\" : loss . value , \"w_l1\" : w_l1 . value , \"w_priv_l1\" : w_priv_l1 . value , } return self def predict ( self , X ) : X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b_s = self . model_state [ \"b_s\" ] scores = np . dot ( X , w . T ) [ np.newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] def score ( self , X , y , error_type = \"mmae\" , return_error = False , ** kwargs ) : X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score def make_scorer ( self ) : # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \"mze\" ) mae = make_scorer ( ordinal_scores , error_type = \"mae\" ) mmae = make_scorer ( ordinal_scores , error_type = \"mmae\" ) scorer = { \"mze\" : mze , \"mae\" : mae , \"mmae\" : mmae } return scorer , \"mmae\"","title":"LUPI_OrdinalRegression_SVM"},{"location":"reference/fri/model/lupi_ordinal_regression/#ancestors-in-mro_2","text":"fri.model.base_initmodel.LUPI_InitModel fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_ordinal_regression/#class-variables","text":"HYPERPARAMETER SOLVER_PARAMS","title":"Class variables"},{"location":"reference/fri/model/lupi_ordinal_regression/#instance-variables_2","text":"L1_factor L1_factor_priv","title":"Instance variables"},{"location":"reference/fri/model/lupi_ordinal_regression/#methods_2","text":"","title":"Methods"},{"location":"reference/fri/model/lupi_ordinal_regression/#fit","text":"def fit ( self , X_combined , y , lupi_features = None )","title":"fit"},{"location":"reference/fri/model/lupi_ordinal_regression/#parameters_1","text":"lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. View Source def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : try : lupi_features = self . lupi_features self . lupi_features = lupi_features except : raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape self . classes_ = np . unique ( y ) # Get parameters from CV model without any feature contstraints C = self . get_params () [ \"C\" ] scaling_lupi_w = self . get_params () [ \"scaling_lupi_w\" ] get_original_bin_name , n_bins = get_bin_mapping ( y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) b_s = cvx . Variable ( shape = ( n_boundaries ), name = \"bias\" ) w_priv = cvx . Variable ( shape = ( self . lupi_features , 2 ), name = \"w_priv\" ) d_priv = cvx . Variable ( shape = ( 2 ), name = \"bias_priv\" ) def priv_function ( bin , sign ) : indices = np . where ( y == get_original_bin_name [ bin ] ) return X_priv [ indices ] * w_priv [ :, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [ :, 0 ] , 1 ) priv_l1_2 = cvx . norm ( w_priv [ :, 1 ] , 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) weight_regularization = 0.5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [] loss = 0 for left_bin in range ( 0 , n_bins - 1 ) : indices = np . where ( y == get_original_bin_name [ left_bin ] ) constraints . append ( X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) constraints . append ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ) : indices = np . where ( y == get_original_bin_name [ right_bin ] ) constraints . append ( X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) constraints . append ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ) : constraints . append ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ] ) objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b_s = b_s . value self . model_state = { \"w\" : w , \"b_s\" : b_s , \"w_priv\" : w_priv . value , \"d_priv\" : d_priv . value , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else \"bin_boundaries\" : n_boundaries , } self . constraints = { \"loss\" : loss . value , \"w_l1\" : w_l1 . value , \"w_priv_l1\" : w_priv_l1 . value , } return self","title":"Parameters"},{"location":"reference/fri/model/lupi_ordinal_regression/#get_params","text":"def get_params ( self , deep = True ) Get parameters for this estimator.","title":"get_params"},{"location":"reference/fri/model/lupi_ordinal_regression/#parameters_2","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"reference/fri/model/lupi_ordinal_regression/#returns","text":"params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out","title":"Returns"},{"location":"reference/fri/model/lupi_ordinal_regression/#make_scorer","text":"def make_scorer ( self ) View Source def make_scorer ( self ): # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \"mze\" ) mae = make_scorer ( ordinal_scores , error_type = \"mae\" ) mmae = make_scorer ( ordinal_scores , error_type = \"mmae\" ) scorer = { \"mze\" : mze , \"mae\" : mae , \"mmae\" : mmae } return scorer , \"mmae\"","title":"make_scorer"},{"location":"reference/fri/model/lupi_ordinal_regression/#predict","text":"def predict ( self , X ) View Source def predict ( self , X ) : X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b_s = self . model_state [ \"b_s\" ] scores = np . dot ( X , w . T ) [ np.newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ]","title":"predict"},{"location":"reference/fri/model/lupi_ordinal_regression/#score","text":"def score ( self , X , y , error_type = 'mmae' , return_error = False , ** kwargs ) View Source def score ( self , X , y , error_type = \"mmae\" , return_error = False , ** kwargs ): X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score","title":"score"},{"location":"reference/fri/model/lupi_ordinal_regression/#set_params","text":"def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params"},{"location":"reference/fri/model/lupi_ordinal_regression/#parameters_3","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"reference/fri/model/lupi_ordinal_regression/#returns_1","text":"self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Returns"},{"location":"reference/fri/model/lupi_regression/","text":"Module fri.model.lupi_regression View Source from itertools import product import cvxpy as cvx import numpy as np from sklearn.metrics import r2_score from sklearn.metrics.regression import _check_reg_targets from sklearn.utils import check_X_y from fri.model.base_lupi import ( LUPI_Relevance_CVXProblem , split_dataset , is_lupi_feature , ) from fri.model.regression import Regression_Relevance_Bound from .base_initmodel import LUPI_InitModel from .base_type import ProblemType class LUPI_Regression ( ProblemType ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ): return self . _lupi_features @classmethod def parameters ( cls ): return [ \"C\" , \"epsilon\" , \"scaling_lupi_w\" , \"scaling_lupi_loss\" ] @property def get_initmodel_template ( cls ): return LUPI_Regression_SVM @property def get_cvxproblem_template ( cls ): return LUPI_Regression_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y class LUPI_Regression_SVM ( LUPI_InitModel ): HYPERPARAMETER = [ \"C\" , \"epsilon\" , \"scaling_lupi_w\" , \"scaling_lupi_loss\" ] def __init__ ( self , C = 1 , epsilon = 0.1 , scaling_lupi_w = 1 , scaling_lupi_loss = 1 , lupi_features = None , ): super () . __init__ () self . epsilon = epsilon self . scaling_lupi_loss = scaling_lupi_loss self . scaling_lupi_w = scaling_lupi_w self . C = C self . lupi_features = lupi_features def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : try : lupi_features = self . lupi_features self . lupi_features = lupi_features except : raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . get_params ()[ \"C\" ] epsilon = self . get_params ()[ \"epsilon\" ] scaling_lupi_w = self . get_params ()[ \"scaling_lupi_w\" ] scaling_lupi_loss = self . get_params ()[ \"scaling_lupi_loss\" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) b = cvx . Variable ( name = \"bias\" ) w_priv_pos = cvx . Variable ( lupi_features , name = \"w_priv_pos\" ) b_priv_pos = cvx . Variable ( name = \"bias_priv_pos\" ) w_priv_neg = cvx . Variable ( lupi_features , name = \"w_priv_neg\" ) b_priv_neg = cvx . Variable ( name = \"bias_priv_neg\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) # Define functions for better readability priv_function_pos = X_priv * w_priv_pos + b_priv_pos priv_function_neg = X_priv * w_priv_neg + b_priv_neg # Combined loss of lupi function and normal slacks, scaled by two constants priv_loss_pos = cvx . sum ( priv_function_pos ) priv_loss_neg = cvx . sum ( priv_function_neg ) priv_loss = priv_loss_pos + priv_loss_neg slack_loss = cvx . sum ( slack ) loss = scaling_lupi_loss * priv_loss + slack_loss # L1 norm regularization of both functions with 1 scaling constant weight_regularization = 0.5 * ( cvx . norm ( w , 1 ) + scaling_lupi_w * ( 0.5 * cvx . norm ( w_priv_pos , 1 ) + 0.5 * cvx . norm ( w_priv_neg , 1 )) ) constraints = [ y - X * w - b <= epsilon + priv_function_pos + slack , X * w + b - y <= epsilon + priv_function_neg + slack , priv_function_pos >= 0 , priv_function_neg >= 0 , # priv_loss_pos >= 0, # priv_loss_neg >= 0, # slack_loss >= 0, slack >= 0 , # loss >= 0, ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) self . model_state = { \"signs_pos\" : priv_function_pos . value > 0 , \"signs_neg\" : priv_function_neg . value > 0 , \"w\" : w . value , \"w_priv_pos\" : w_priv_pos . value , \"w_priv_neg\" : w_priv_neg . value , \"b\" : b . value , \"b_priv_pos\" : b_priv_pos . value , \"b_priv_neg\" : b_priv_neg . value , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO: Move this somewhere else, } w_l1 = np . linalg . norm ( w . value , ord = 1 ) w_priv_pos_l1 = np . linalg . norm ( w_priv_pos . value , ord = 1 ) w_priv_neg_l1 = np . linalg . norm ( w_priv_neg . value , ord = 1 ) # We take the mean to combine all submodels (for priv) into a single normalization factor w_priv_l1 = w_priv_pos_l1 + w_priv_neg_l1 self . constraints = { \"priv_loss\" : priv_loss . value , \"scaling_lupi_loss\" : scaling_lupi_loss , # \"loss_slack\": slack_loss.value, \"loss\" : loss . value , \"w_l1\" : w_l1 , \"w_priv_l1\" : w_priv_l1 , \"w_priv_pos_l1\" : w_priv_pos_l1 , \"w_priv_neg_l1\" : w_priv_neg_l1 , } return self @property def SOLVER_PARAMS ( cls ): return { \"solver\" : \"ECOS\" , \"verbose\" : False } def predict ( self , X ): \"\"\" Method to predict points using svm classification rule. We use both normal and priv. features. This function is mainly used for CV purposes to find the best parameters according to score. Parameters ---------- X : numpy.ndarray \"\"\" X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) _check_reg_targets ( y , prediction , None ) score = r2_score ( y , prediction ) return score class LUPI_Regression_Relevance_Bound ( LUPI_Relevance_CVXProblem , Regression_Relevance_Bound ): @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems? if not is_priv : yield from super () . generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ([ 1 , - 1 ], [ True , False ]): problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem def _init_objective_LB_LUPI ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w_priv_pos [ self . lupi_index ]) <= self . feature_relevance ) self . add_constraint ( cvx . abs ( self . w_priv_neg [ self . lupi_index ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_objective_UB_LUPI ( self , pos = None , sign = None , ** kwargs ): if pos : self . add_constraint ( self . feature_relevance <= sign * self . w_priv_pos [ self . lupi_index ] ) else : self . add_constraint ( self . feature_relevance <= sign * self . w_priv_neg [ self . lupi_index ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from best initial model l1_w = init_model_constraints [ \"w_l1\" ] self . l1_priv_w_pos = init_model_constraints [ \"w_priv_pos_l1\" ] self . l1_priv_w_neg = init_model_constraints [ \"w_priv_neg_l1\" ] init_loss = init_model_constraints [ \"loss\" ] epsilon = parameters [ \"epsilon\" ] scaling_lupi_loss = init_model_constraints [ \"scaling_lupi_loss\" ] # New Variables w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) b = cvx . Variable ( name = \"b\" ) w_priv_pos = cvx . Variable ( self . d_priv , name = \"w_priv_pos\" ) b_priv_pos = cvx . Variable ( name = \"bias_priv_pos\" ) w_priv_neg = cvx . Variable ( self . d_priv , name = \"w_priv_neg\" ) b_priv_neg = cvx . Variable ( name = \"bias_priv_neg\" ) slack = cvx . Variable ( shape = ( self . n )) priv_function_pos = self . X_priv * w_priv_pos + b_priv_pos priv_function_neg = self . X_priv * w_priv_neg + b_priv_neg priv_loss = cvx . sum ( priv_function_pos + priv_function_neg ) loss = priv_loss + cvx . sum ( slack ) weight_norm = cvx . norm ( w , 1 ) self . weight_norm_priv_pos = cvx . norm ( w_priv_pos , 1 ) self . weight_norm_priv_neg = cvx . norm ( w_priv_neg , 1 ) self . add_constraint ( self . y - self . X * w - b <= epsilon + priv_function_pos + slack ) self . add_constraint ( self . X * w + b - self . y <= epsilon + priv_function_neg + slack ) self . add_constraint ( priv_function_pos >= 0 ) self . add_constraint ( priv_function_neg >= 0 ) self . add_constraint ( loss <= init_loss ) self . add_constraint ( slack >= 0 ) sum_norms = weight_norm + self . weight_norm_priv_pos + self . weight_norm_priv_neg self . add_constraint ( sum_norms <= l1_w ) # self.add_constraint(self.weight_norm_priv_pos <= self.l1_priv_w_pos) # self.add_constraint(self.weight_norm_priv_neg <= self.l1_priv_w_neg) # Save values for object use later self . w = w self . w_priv_pos = w_priv_pos self . w_priv_neg = w_priv_neg self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Classes LUPI_Regression class LUPI_Regression ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Regression ( ProblemType ) : def __init__ ( self , ** kwargs ) : super (). __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ) : return self . _lupi_features @classmethod def parameters ( cls ) : return [ \"C\", \"epsilon\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ] @property def get_initmodel_template ( cls ) : return LUPI_Regression_SVM @property def get_cvxproblem_template ( cls ) : return LUPI_Regression_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods parameters def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\", \"epsilon\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ] Instance variables get_cvxproblem_template get_initmodel_template lupi_features Methods get_all_parameters def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } get_all_relax_factors def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } get_chosen_parameter def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) get_chosen_relax_factors def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor get_relaxed_constraints def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } postprocessing def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds preprocessing def preprocessing ( self , data , lupi_features = None ) View Source def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y relax_constraint def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key )) relax_factors def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] LUPI_Regression_Relevance_Bound class LUPI_Regression_Relevance_Bound ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Regression_Relevance_Bound ( LUPI_Relevance_CVXProblem , Regression_Relevance_Bound ) : @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super (). generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ( [ 1, -1 ] , [ True, False ] ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem def _init_objective_LB_LUPI ( self , ** kwargs ) : self . add_constraint ( cvx . abs ( self . w_priv_pos [ self.lupi_index ] ) <= self . feature_relevance ) self . add_constraint ( cvx . abs ( self . w_priv_neg [ self.lupi_index ] ) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_objective_UB_LUPI ( self , pos = None , sign = None , ** kwargs ) : if pos : self . add_constraint ( self . feature_relevance <= sign * self . w_priv_pos [ self.lupi_index ] ) else : self . add_constraint ( self . feature_relevance <= sign * self . w_priv_neg [ self.lupi_index ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ) : # Upper constraints from best initial model l1_w = init_model_constraints [ \"w_l1\" ] self . l1_priv_w_pos = init_model_constraints [ \"w_priv_pos_l1\" ] self . l1_priv_w_neg = init_model_constraints [ \"w_priv_neg_l1\" ] init_loss = init_model_constraints [ \"loss\" ] epsilon = parameters [ \"epsilon\" ] scaling_lupi_loss = init_model_constraints [ \"scaling_lupi_loss\" ] # New Variables w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) b = cvx . Variable ( name = \"b\" ) w_priv_pos = cvx . Variable ( self . d_priv , name = \"w_priv_pos\" ) b_priv_pos = cvx . Variable ( name = \"bias_priv_pos\" ) w_priv_neg = cvx . Variable ( self . d_priv , name = \"w_priv_neg\" ) b_priv_neg = cvx . Variable ( name = \"bias_priv_neg\" ) slack = cvx . Variable ( shape = ( self . n )) priv_function_pos = self . X_priv * w_priv_pos + b_priv_pos priv_function_neg = self . X_priv * w_priv_neg + b_priv_neg priv_loss = cvx . sum ( priv_function_pos + priv_function_neg ) loss = priv_loss + cvx . sum ( slack ) weight_norm = cvx . norm ( w , 1 ) self . weight_norm_priv_pos = cvx . norm ( w_priv_pos , 1 ) self . weight_norm_priv_neg = cvx . norm ( w_priv_neg , 1 ) self . add_constraint ( self . y - self . X * w - b <= epsilon + priv_function_pos + slack ) self . add_constraint ( self . X * w + b - self . y <= epsilon + priv_function_neg + slack ) self . add_constraint ( priv_function_pos >= 0 ) self . add_constraint ( priv_function_neg >= 0 ) self . add_constraint ( loss <= init_loss ) self . add_constraint ( slack >= 0 ) sum_norms = weight_norm + self . weight_norm_priv_pos + self . weight_norm_priv_neg self . add_constraint ( sum_norms <= l1_w ) # self . add_constraint ( self . weight_norm_priv_pos <= self . l1_priv_w_pos ) # self . add_constraint ( self . weight_norm_priv_neg <= self . l1_priv_w_neg ) # Save values for object use later self . w = w self . w_priv_pos = w_priv_pos self . w_priv_neg = w_priv_neg self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Ancestors (in MRO) fri.model.base_lupi.LUPI_Relevance_CVXProblem fri.model.regression.Regression_Relevance_Bound fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC Static methods aggregate_max_candidates def aggregate_max_candidates ( max_problems_candidates ) View Source @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value aggregate_min_candidates def aggregate_min_candidates ( min_problems_candidates ) View Source @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value generate_lower_bound_problem def generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem generate_upper_bound_problem def generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super (). generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ( [ 1, -1 ] , [ True, False ] ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem Instance variables accepted_status constraints cvx_problem isProbe is_solved objective probeID solved_relevance solver_kwargs Methods add_constraint def add_constraint ( self , new ) View Source def add_constraint ( self , new ): self . _constraints . append ( new ) init_objective_LB def init_objective_LB ( self , ** kwargs ) View Source def init_objective_LB ( self , ** kwargs ): # We have two models basically with different indexes if self . isPriv : self . _init_objective_LB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_LB ( ** kwargs ) init_objective_UB def init_objective_UB ( self , ** kwargs ) View Source def init_objective_UB ( self , ** kwargs ): # We have two models basically with different indexes if self . isPriv : self . _init_objective_UB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_UB ( ** kwargs ) preprocessing_data def preprocessing_data ( self , data , best_model_state ) View Source def preprocessing_data ( self , data , best_model_state ): lupi_features = best_model_state [ \"lupi_features\" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super (). preprocessing_data (( X , y ), best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False solve def solve ( self ) -> object View Source def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self LUPI_Regression_SVM class LUPI_Regression_SVM ( C = 1 , epsilon = 0.1 , scaling_lupi_w = 1 , scaling_lupi_loss = 1 , lupi_features = None ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Regression_SVM ( LUPI_InitModel ) : HYPERPARAMETER = [ \"C\", \"epsilon\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ] def __init__ ( self , C = 1 , epsilon = 0.1 , scaling_lupi_w = 1 , scaling_lupi_loss = 1 , lupi_features = None , ) : super (). __init__ () self . epsilon = epsilon self . scaling_lupi_loss = scaling_lupi_loss self . scaling_lupi_w = scaling_lupi_w self . C = C self . lupi_features = lupi_features def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : try : lupi_features = self . lupi_features self . lupi_features = lupi_features except : raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . get_params () [ \"C\" ] epsilon = self . get_params () [ \"epsilon\" ] scaling_lupi_w = self . get_params () [ \"scaling_lupi_w\" ] scaling_lupi_loss = self . get_params () [ \"scaling_lupi_loss\" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) b = cvx . Variable ( name = \"bias\" ) w_priv_pos = cvx . Variable ( lupi_features , name = \"w_priv_pos\" ) b_priv_pos = cvx . Variable ( name = \"bias_priv_pos\" ) w_priv_neg = cvx . Variable ( lupi_features , name = \"w_priv_neg\" ) b_priv_neg = cvx . Variable ( name = \"bias_priv_neg\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) # Define functions for better readability priv_function_pos = X_priv * w_priv_pos + b_priv_pos priv_function_neg = X_priv * w_priv_neg + b_priv_neg # Combined loss of lupi function and normal slacks , scaled by two constants priv_loss_pos = cvx . sum ( priv_function_pos ) priv_loss_neg = cvx . sum ( priv_function_neg ) priv_loss = priv_loss_pos + priv_loss_neg slack_loss = cvx . sum ( slack ) loss = scaling_lupi_loss * priv_loss + slack_loss # L1 norm regularization of both functions with 1 scaling constant weight_regularization = 0.5 * ( cvx . norm ( w , 1 ) + scaling_lupi_w * ( 0.5 * cvx . norm ( w_priv_pos , 1 ) + 0.5 * cvx . norm ( w_priv_neg , 1 )) ) constraints = [ y - X * w - b <= epsilon + priv_function_pos + slack, X * w + b - y <= epsilon + priv_function_neg + slack, priv_function_pos >= 0, priv_function_neg >= 0, # priv_loss_pos >= 0, # priv_loss_neg >= 0, # slack_loss >= 0, slack >= 0, # loss >= 0, ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) self . model_state = { \"signs_pos\" : priv_function_pos . value > 0 , \"signs_neg\" : priv_function_neg . value > 0 , \"w\" : w . value , \"w_priv_pos\" : w_priv_pos . value , \"w_priv_neg\" : w_priv_neg . value , \"b\" : b . value , \"b_priv_pos\" : b_priv_pos . value , \"b_priv_neg\" : b_priv_neg . value , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else , } w_l1 = np . linalg . norm ( w . value , ord = 1 ) w_priv_pos_l1 = np . linalg . norm ( w_priv_pos . value , ord = 1 ) w_priv_neg_l1 = np . linalg . norm ( w_priv_neg . value , ord = 1 ) # We take the mean to combine all submodels ( for priv ) into a single normalization factor w_priv_l1 = w_priv_pos_l1 + w_priv_neg_l1 self . constraints = { \"priv_loss\" : priv_loss . value , \"scaling_lupi_loss\" : scaling_lupi_loss , # \"loss_slack\" : slack_loss . value , \"loss\" : loss . value , \"w_l1\" : w_l1 , \"w_priv_l1\" : w_priv_l1 , \"w_priv_pos_l1\" : w_priv_pos_l1 , \"w_priv_neg_l1\" : w_priv_neg_l1 , } return self @property def SOLVER_PARAMS ( cls ) : return { \"solver\" : \"ECOS\" , \"verbose\" : False } def predict ( self , X ) : \"\"\" Method to predict points using svm classification rule. We use both normal and priv. features. This function is mainly used for CV purposes to find the best parameters according to score. Parameters ---------- X : numpy.ndarray \"\"\" X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b return y def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) _check_reg_targets ( y , prediction , None ) score = r2_score ( y , prediction ) return score Ancestors (in MRO) fri.model.base_initmodel.LUPI_InitModel fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator Class variables HYPERPARAMETER Instance variables L1_factor L1_factor_priv SOLVER_PARAMS Methods fit def fit ( self , X_combined , y , lupi_features = None ) Parameters lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. View Source def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : try : lupi_features = self . lupi_features self . lupi_features = lupi_features except : raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . get_params ()[ \"C\" ] epsilon = self . get_params ()[ \"epsilon\" ] scaling_lupi_w = self . get_params ()[ \"scaling_lupi_w\" ] scaling_lupi_loss = self . get_params ()[ \"scaling_lupi_loss\" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) b = cvx . Variable ( name = \"bias\" ) w_priv_pos = cvx . Variable ( lupi_features , name = \"w_priv_pos\" ) b_priv_pos = cvx . Variable ( name = \"bias_priv_pos\" ) w_priv_neg = cvx . Variable ( lupi_features , name = \"w_priv_neg\" ) b_priv_neg = cvx . Variable ( name = \"bias_priv_neg\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) # Define functions for better readability priv_function_pos = X_priv * w_priv_pos + b_priv_pos priv_function_neg = X_priv * w_priv_neg + b_priv_neg # Combined loss of lupi function and normal slacks , scaled by two constants priv_loss_pos = cvx . sum ( priv_function_pos ) priv_loss_neg = cvx . sum ( priv_function_neg ) priv_loss = priv_loss_pos + priv_loss_neg slack_loss = cvx . sum ( slack ) loss = scaling_lupi_loss * priv_loss + slack_loss # L1 norm regularization of both functions with 1 scaling constant weight_regularization = 0 . 5 * ( cvx . norm ( w , 1 ) + scaling_lupi_w * ( 0 . 5 * cvx . norm ( w_priv_pos , 1 ) + 0 . 5 * cvx . norm ( w_priv_neg , 1 )) ) constraints = [ y - X * w - b <= epsilon + priv_function_pos + slack , X * w + b - y <= epsilon + priv_function_neg + slack , priv_function_pos >= 0 , priv_function_neg >= 0 , # priv_loss_pos >= 0 , # priv_loss_neg >= 0 , # slack_loss >= 0 , slack >= 0 , # loss >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) self . model_state = { \"signs_pos\" : priv_function_pos . value > 0 , \"signs_neg\" : priv_function_neg . value > 0 , \"w\" : w . value , \"w_priv_pos\" : w_priv_pos . value , \"w_priv_neg\" : w_priv_neg . value , \"b\" : b . value , \"b_priv_pos\" : b_priv_pos . value , \"b_priv_neg\" : b_priv_neg . value , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else , } w_l1 = np . linalg . norm ( w . value , ord = 1 ) w_priv_pos_l1 = np . linalg . norm ( w_priv_pos . value , ord = 1 ) w_priv_neg_l1 = np . linalg . norm ( w_priv_neg . value , ord = 1 ) # We take the mean to combine all submodels ( for priv ) into a single normalization factor w_priv_l1 = w_priv_pos_l1 + w_priv_neg_l1 self . constraints = { \"priv_loss\" : priv_loss . value , \"scaling_lupi_loss\" : scaling_lupi_loss , # \"loss_slack\" : slack_loss . value , \"loss\" : loss . value , \"w_l1\" : w_l1 , \"w_priv_l1\" : w_priv_l1 , \"w_priv_pos_l1\" : w_priv_pos_l1 , \"w_priv_neg_l1\" : w_priv_neg_l1 , } return self get_params def get_params ( self , deep = True ) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out make_scorer def make_scorer ( self ) View Source def make_scorer ( self ): return None , None predict def predict ( self , X ) Method to predict points using svm classification rule. We use both normal and priv. features. This function is mainly used for CV purposes to find the best parameters according to score. Parameters X : numpy.ndarray View Source def predict ( self , X ): \"\"\" Method to predict points using svm classification rule. We use both normal and priv. features. This function is mainly used for CV purposes to find the best parameters according to score. Parameters ---------- X : numpy.ndarray \"\"\" X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b return y score def score ( self , X , y , ** kwargs ) View Source def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) _check_reg_targets ( y , prediction , None ) score = r2_score ( y , prediction ) return score set_params def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Lupi Regression"},{"location":"reference/fri/model/lupi_regression/#module-frimodellupi_regression","text":"View Source from itertools import product import cvxpy as cvx import numpy as np from sklearn.metrics import r2_score from sklearn.metrics.regression import _check_reg_targets from sklearn.utils import check_X_y from fri.model.base_lupi import ( LUPI_Relevance_CVXProblem , split_dataset , is_lupi_feature , ) from fri.model.regression import Regression_Relevance_Bound from .base_initmodel import LUPI_InitModel from .base_type import ProblemType class LUPI_Regression ( ProblemType ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ): return self . _lupi_features @classmethod def parameters ( cls ): return [ \"C\" , \"epsilon\" , \"scaling_lupi_w\" , \"scaling_lupi_loss\" ] @property def get_initmodel_template ( cls ): return LUPI_Regression_SVM @property def get_cvxproblem_template ( cls ): return LUPI_Regression_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y class LUPI_Regression_SVM ( LUPI_InitModel ): HYPERPARAMETER = [ \"C\" , \"epsilon\" , \"scaling_lupi_w\" , \"scaling_lupi_loss\" ] def __init__ ( self , C = 1 , epsilon = 0.1 , scaling_lupi_w = 1 , scaling_lupi_loss = 1 , lupi_features = None , ): super () . __init__ () self . epsilon = epsilon self . scaling_lupi_loss = scaling_lupi_loss self . scaling_lupi_w = scaling_lupi_w self . C = C self . lupi_features = lupi_features def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : try : lupi_features = self . lupi_features self . lupi_features = lupi_features except : raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . get_params ()[ \"C\" ] epsilon = self . get_params ()[ \"epsilon\" ] scaling_lupi_w = self . get_params ()[ \"scaling_lupi_w\" ] scaling_lupi_loss = self . get_params ()[ \"scaling_lupi_loss\" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) b = cvx . Variable ( name = \"bias\" ) w_priv_pos = cvx . Variable ( lupi_features , name = \"w_priv_pos\" ) b_priv_pos = cvx . Variable ( name = \"bias_priv_pos\" ) w_priv_neg = cvx . Variable ( lupi_features , name = \"w_priv_neg\" ) b_priv_neg = cvx . Variable ( name = \"bias_priv_neg\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) # Define functions for better readability priv_function_pos = X_priv * w_priv_pos + b_priv_pos priv_function_neg = X_priv * w_priv_neg + b_priv_neg # Combined loss of lupi function and normal slacks, scaled by two constants priv_loss_pos = cvx . sum ( priv_function_pos ) priv_loss_neg = cvx . sum ( priv_function_neg ) priv_loss = priv_loss_pos + priv_loss_neg slack_loss = cvx . sum ( slack ) loss = scaling_lupi_loss * priv_loss + slack_loss # L1 norm regularization of both functions with 1 scaling constant weight_regularization = 0.5 * ( cvx . norm ( w , 1 ) + scaling_lupi_w * ( 0.5 * cvx . norm ( w_priv_pos , 1 ) + 0.5 * cvx . norm ( w_priv_neg , 1 )) ) constraints = [ y - X * w - b <= epsilon + priv_function_pos + slack , X * w + b - y <= epsilon + priv_function_neg + slack , priv_function_pos >= 0 , priv_function_neg >= 0 , # priv_loss_pos >= 0, # priv_loss_neg >= 0, # slack_loss >= 0, slack >= 0 , # loss >= 0, ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) self . model_state = { \"signs_pos\" : priv_function_pos . value > 0 , \"signs_neg\" : priv_function_neg . value > 0 , \"w\" : w . value , \"w_priv_pos\" : w_priv_pos . value , \"w_priv_neg\" : w_priv_neg . value , \"b\" : b . value , \"b_priv_pos\" : b_priv_pos . value , \"b_priv_neg\" : b_priv_neg . value , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO: Move this somewhere else, } w_l1 = np . linalg . norm ( w . value , ord = 1 ) w_priv_pos_l1 = np . linalg . norm ( w_priv_pos . value , ord = 1 ) w_priv_neg_l1 = np . linalg . norm ( w_priv_neg . value , ord = 1 ) # We take the mean to combine all submodels (for priv) into a single normalization factor w_priv_l1 = w_priv_pos_l1 + w_priv_neg_l1 self . constraints = { \"priv_loss\" : priv_loss . value , \"scaling_lupi_loss\" : scaling_lupi_loss , # \"loss_slack\": slack_loss.value, \"loss\" : loss . value , \"w_l1\" : w_l1 , \"w_priv_l1\" : w_priv_l1 , \"w_priv_pos_l1\" : w_priv_pos_l1 , \"w_priv_neg_l1\" : w_priv_neg_l1 , } return self @property def SOLVER_PARAMS ( cls ): return { \"solver\" : \"ECOS\" , \"verbose\" : False } def predict ( self , X ): \"\"\" Method to predict points using svm classification rule. We use both normal and priv. features. This function is mainly used for CV purposes to find the best parameters according to score. Parameters ---------- X : numpy.ndarray \"\"\" X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) _check_reg_targets ( y , prediction , None ) score = r2_score ( y , prediction ) return score class LUPI_Regression_Relevance_Bound ( LUPI_Relevance_CVXProblem , Regression_Relevance_Bound ): @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems? if not is_priv : yield from super () . generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ([ 1 , - 1 ], [ True , False ]): problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem def _init_objective_LB_LUPI ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w_priv_pos [ self . lupi_index ]) <= self . feature_relevance ) self . add_constraint ( cvx . abs ( self . w_priv_neg [ self . lupi_index ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_objective_UB_LUPI ( self , pos = None , sign = None , ** kwargs ): if pos : self . add_constraint ( self . feature_relevance <= sign * self . w_priv_pos [ self . lupi_index ] ) else : self . add_constraint ( self . feature_relevance <= sign * self . w_priv_neg [ self . lupi_index ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from best initial model l1_w = init_model_constraints [ \"w_l1\" ] self . l1_priv_w_pos = init_model_constraints [ \"w_priv_pos_l1\" ] self . l1_priv_w_neg = init_model_constraints [ \"w_priv_neg_l1\" ] init_loss = init_model_constraints [ \"loss\" ] epsilon = parameters [ \"epsilon\" ] scaling_lupi_loss = init_model_constraints [ \"scaling_lupi_loss\" ] # New Variables w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) b = cvx . Variable ( name = \"b\" ) w_priv_pos = cvx . Variable ( self . d_priv , name = \"w_priv_pos\" ) b_priv_pos = cvx . Variable ( name = \"bias_priv_pos\" ) w_priv_neg = cvx . Variable ( self . d_priv , name = \"w_priv_neg\" ) b_priv_neg = cvx . Variable ( name = \"bias_priv_neg\" ) slack = cvx . Variable ( shape = ( self . n )) priv_function_pos = self . X_priv * w_priv_pos + b_priv_pos priv_function_neg = self . X_priv * w_priv_neg + b_priv_neg priv_loss = cvx . sum ( priv_function_pos + priv_function_neg ) loss = priv_loss + cvx . sum ( slack ) weight_norm = cvx . norm ( w , 1 ) self . weight_norm_priv_pos = cvx . norm ( w_priv_pos , 1 ) self . weight_norm_priv_neg = cvx . norm ( w_priv_neg , 1 ) self . add_constraint ( self . y - self . X * w - b <= epsilon + priv_function_pos + slack ) self . add_constraint ( self . X * w + b - self . y <= epsilon + priv_function_neg + slack ) self . add_constraint ( priv_function_pos >= 0 ) self . add_constraint ( priv_function_neg >= 0 ) self . add_constraint ( loss <= init_loss ) self . add_constraint ( slack >= 0 ) sum_norms = weight_norm + self . weight_norm_priv_pos + self . weight_norm_priv_neg self . add_constraint ( sum_norms <= l1_w ) # self.add_constraint(self.weight_norm_priv_pos <= self.l1_priv_w_pos) # self.add_constraint(self.weight_norm_priv_neg <= self.l1_priv_w_neg) # Save values for object use later self . w = w self . w_priv_pos = w_priv_pos self . w_priv_neg = w_priv_neg self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"Module fri.model.lupi_regression"},{"location":"reference/fri/model/lupi_regression/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/lupi_regression/#lupi_regression","text":"class LUPI_Regression ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Regression ( ProblemType ) : def __init__ ( self , ** kwargs ) : super (). __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ) : return self . _lupi_features @classmethod def parameters ( cls ) : return [ \"C\", \"epsilon\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ] @property def get_initmodel_template ( cls ) : return LUPI_Regression_SVM @property def get_cvxproblem_template ( cls ) : return LUPI_Regression_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y","title":"LUPI_Regression"},{"location":"reference/fri/model/lupi_regression/#ancestors-in-mro","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_regression/#static-methods","text":"","title":"Static methods"},{"location":"reference/fri/model/lupi_regression/#parameters","text":"def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\", \"epsilon\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ]","title":"parameters"},{"location":"reference/fri/model/lupi_regression/#instance-variables","text":"get_cvxproblem_template get_initmodel_template lupi_features","title":"Instance variables"},{"location":"reference/fri/model/lupi_regression/#methods","text":"","title":"Methods"},{"location":"reference/fri/model/lupi_regression/#get_all_parameters","text":"def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () }","title":"get_all_parameters"},{"location":"reference/fri/model/lupi_regression/#get_all_relax_factors","text":"def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () }","title":"get_all_relax_factors"},{"location":"reference/fri/model/lupi_regression/#get_chosen_parameter","text":"def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 )","title":"get_chosen_parameter"},{"location":"reference/fri/model/lupi_regression/#get_chosen_relax_factors","text":"def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor","title":"get_chosen_relax_factors"},{"location":"reference/fri/model/lupi_regression/#get_relaxed_constraints","text":"def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () }","title":"get_relaxed_constraints"},{"location":"reference/fri/model/lupi_regression/#postprocessing","text":"def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds","title":"postprocessing"},{"location":"reference/fri/model/lupi_regression/#preprocessing","text":"def preprocessing ( self , data , lupi_features = None ) View Source def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y","title":"preprocessing"},{"location":"reference/fri/model/lupi_regression/#relax_constraint","text":"def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"relax_constraint"},{"location":"reference/fri/model/lupi_regression/#relax_factors","text":"def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ]","title":"relax_factors"},{"location":"reference/fri/model/lupi_regression/#lupi_regression_relevance_bound","text":"class LUPI_Regression_Relevance_Bound ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Regression_Relevance_Bound ( LUPI_Relevance_CVXProblem , Regression_Relevance_Bound ) : @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super (). generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ( [ 1, -1 ] , [ True, False ] ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem def _init_objective_LB_LUPI ( self , ** kwargs ) : self . add_constraint ( cvx . abs ( self . w_priv_pos [ self.lupi_index ] ) <= self . feature_relevance ) self . add_constraint ( cvx . abs ( self . w_priv_neg [ self.lupi_index ] ) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_objective_UB_LUPI ( self , pos = None , sign = None , ** kwargs ) : if pos : self . add_constraint ( self . feature_relevance <= sign * self . w_priv_pos [ self.lupi_index ] ) else : self . add_constraint ( self . feature_relevance <= sign * self . w_priv_neg [ self.lupi_index ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ) : # Upper constraints from best initial model l1_w = init_model_constraints [ \"w_l1\" ] self . l1_priv_w_pos = init_model_constraints [ \"w_priv_pos_l1\" ] self . l1_priv_w_neg = init_model_constraints [ \"w_priv_neg_l1\" ] init_loss = init_model_constraints [ \"loss\" ] epsilon = parameters [ \"epsilon\" ] scaling_lupi_loss = init_model_constraints [ \"scaling_lupi_loss\" ] # New Variables w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) b = cvx . Variable ( name = \"b\" ) w_priv_pos = cvx . Variable ( self . d_priv , name = \"w_priv_pos\" ) b_priv_pos = cvx . Variable ( name = \"bias_priv_pos\" ) w_priv_neg = cvx . Variable ( self . d_priv , name = \"w_priv_neg\" ) b_priv_neg = cvx . Variable ( name = \"bias_priv_neg\" ) slack = cvx . Variable ( shape = ( self . n )) priv_function_pos = self . X_priv * w_priv_pos + b_priv_pos priv_function_neg = self . X_priv * w_priv_neg + b_priv_neg priv_loss = cvx . sum ( priv_function_pos + priv_function_neg ) loss = priv_loss + cvx . sum ( slack ) weight_norm = cvx . norm ( w , 1 ) self . weight_norm_priv_pos = cvx . norm ( w_priv_pos , 1 ) self . weight_norm_priv_neg = cvx . norm ( w_priv_neg , 1 ) self . add_constraint ( self . y - self . X * w - b <= epsilon + priv_function_pos + slack ) self . add_constraint ( self . X * w + b - self . y <= epsilon + priv_function_neg + slack ) self . add_constraint ( priv_function_pos >= 0 ) self . add_constraint ( priv_function_neg >= 0 ) self . add_constraint ( loss <= init_loss ) self . add_constraint ( slack >= 0 ) sum_norms = weight_norm + self . weight_norm_priv_pos + self . weight_norm_priv_neg self . add_constraint ( sum_norms <= l1_w ) # self . add_constraint ( self . weight_norm_priv_pos <= self . l1_priv_w_pos ) # self . add_constraint ( self . weight_norm_priv_neg <= self . l1_priv_w_neg ) # Save values for object use later self . w = w self . w_priv_pos = w_priv_pos self . w_priv_neg = w_priv_neg self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"LUPI_Regression_Relevance_Bound"},{"location":"reference/fri/model/lupi_regression/#ancestors-in-mro_1","text":"fri.model.base_lupi.LUPI_Relevance_CVXProblem fri.model.regression.Regression_Relevance_Bound fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_regression/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/fri/model/lupi_regression/#aggregate_max_candidates","text":"def aggregate_max_candidates ( max_problems_candidates ) View Source @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value","title":"aggregate_max_candidates"},{"location":"reference/fri/model/lupi_regression/#aggregate_min_candidates","text":"def aggregate_min_candidates ( min_problems_candidates ) View Source @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value","title":"aggregate_min_candidates"},{"location":"reference/fri/model/lupi_regression/#generate_lower_bound_problem","text":"def generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem","title":"generate_lower_bound_problem"},{"location":"reference/fri/model/lupi_regression/#generate_upper_bound_problem","text":"def generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super (). generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ( [ 1, -1 ] , [ True, False ] ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem","title":"generate_upper_bound_problem"},{"location":"reference/fri/model/lupi_regression/#instance-variables_1","text":"accepted_status constraints cvx_problem isProbe is_solved objective probeID solved_relevance solver_kwargs","title":"Instance variables"},{"location":"reference/fri/model/lupi_regression/#methods_1","text":"","title":"Methods"},{"location":"reference/fri/model/lupi_regression/#add_constraint","text":"def add_constraint ( self , new ) View Source def add_constraint ( self , new ): self . _constraints . append ( new )","title":"add_constraint"},{"location":"reference/fri/model/lupi_regression/#init_objective_lb","text":"def init_objective_LB ( self , ** kwargs ) View Source def init_objective_LB ( self , ** kwargs ): # We have two models basically with different indexes if self . isPriv : self . _init_objective_LB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_LB ( ** kwargs )","title":"init_objective_LB"},{"location":"reference/fri/model/lupi_regression/#init_objective_ub","text":"def init_objective_UB ( self , ** kwargs ) View Source def init_objective_UB ( self , ** kwargs ): # We have two models basically with different indexes if self . isPriv : self . _init_objective_UB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_UB ( ** kwargs )","title":"init_objective_UB"},{"location":"reference/fri/model/lupi_regression/#preprocessing_data","text":"def preprocessing_data ( self , data , best_model_state ) View Source def preprocessing_data ( self , data , best_model_state ): lupi_features = best_model_state [ \"lupi_features\" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super (). preprocessing_data (( X , y ), best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False","title":"preprocessing_data"},{"location":"reference/fri/model/lupi_regression/#solve","text":"def solve ( self ) -> object View Source def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self","title":"solve"},{"location":"reference/fri/model/lupi_regression/#lupi_regression_svm","text":"class LUPI_Regression_SVM ( C = 1 , epsilon = 0.1 , scaling_lupi_w = 1 , scaling_lupi_loss = 1 , lupi_features = None ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LUPI_Regression_SVM ( LUPI_InitModel ) : HYPERPARAMETER = [ \"C\", \"epsilon\", \"scaling_lupi_w\", \"scaling_lupi_loss\" ] def __init__ ( self , C = 1 , epsilon = 0.1 , scaling_lupi_w = 1 , scaling_lupi_loss = 1 , lupi_features = None , ) : super (). __init__ () self . epsilon = epsilon self . scaling_lupi_loss = scaling_lupi_loss self . scaling_lupi_w = scaling_lupi_w self . C = C self . lupi_features = lupi_features def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : try : lupi_features = self . lupi_features self . lupi_features = lupi_features except : raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . get_params () [ \"C\" ] epsilon = self . get_params () [ \"epsilon\" ] scaling_lupi_w = self . get_params () [ \"scaling_lupi_w\" ] scaling_lupi_loss = self . get_params () [ \"scaling_lupi_loss\" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) b = cvx . Variable ( name = \"bias\" ) w_priv_pos = cvx . Variable ( lupi_features , name = \"w_priv_pos\" ) b_priv_pos = cvx . Variable ( name = \"bias_priv_pos\" ) w_priv_neg = cvx . Variable ( lupi_features , name = \"w_priv_neg\" ) b_priv_neg = cvx . Variable ( name = \"bias_priv_neg\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) # Define functions for better readability priv_function_pos = X_priv * w_priv_pos + b_priv_pos priv_function_neg = X_priv * w_priv_neg + b_priv_neg # Combined loss of lupi function and normal slacks , scaled by two constants priv_loss_pos = cvx . sum ( priv_function_pos ) priv_loss_neg = cvx . sum ( priv_function_neg ) priv_loss = priv_loss_pos + priv_loss_neg slack_loss = cvx . sum ( slack ) loss = scaling_lupi_loss * priv_loss + slack_loss # L1 norm regularization of both functions with 1 scaling constant weight_regularization = 0.5 * ( cvx . norm ( w , 1 ) + scaling_lupi_w * ( 0.5 * cvx . norm ( w_priv_pos , 1 ) + 0.5 * cvx . norm ( w_priv_neg , 1 )) ) constraints = [ y - X * w - b <= epsilon + priv_function_pos + slack, X * w + b - y <= epsilon + priv_function_neg + slack, priv_function_pos >= 0, priv_function_neg >= 0, # priv_loss_pos >= 0, # priv_loss_neg >= 0, # slack_loss >= 0, slack >= 0, # loss >= 0, ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) self . model_state = { \"signs_pos\" : priv_function_pos . value > 0 , \"signs_neg\" : priv_function_neg . value > 0 , \"w\" : w . value , \"w_priv_pos\" : w_priv_pos . value , \"w_priv_neg\" : w_priv_neg . value , \"b\" : b . value , \"b_priv_pos\" : b_priv_pos . value , \"b_priv_neg\" : b_priv_neg . value , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else , } w_l1 = np . linalg . norm ( w . value , ord = 1 ) w_priv_pos_l1 = np . linalg . norm ( w_priv_pos . value , ord = 1 ) w_priv_neg_l1 = np . linalg . norm ( w_priv_neg . value , ord = 1 ) # We take the mean to combine all submodels ( for priv ) into a single normalization factor w_priv_l1 = w_priv_pos_l1 + w_priv_neg_l1 self . constraints = { \"priv_loss\" : priv_loss . value , \"scaling_lupi_loss\" : scaling_lupi_loss , # \"loss_slack\" : slack_loss . value , \"loss\" : loss . value , \"w_l1\" : w_l1 , \"w_priv_l1\" : w_priv_l1 , \"w_priv_pos_l1\" : w_priv_pos_l1 , \"w_priv_neg_l1\" : w_priv_neg_l1 , } return self @property def SOLVER_PARAMS ( cls ) : return { \"solver\" : \"ECOS\" , \"verbose\" : False } def predict ( self , X ) : \"\"\" Method to predict points using svm classification rule. We use both normal and priv. features. This function is mainly used for CV purposes to find the best parameters according to score. Parameters ---------- X : numpy.ndarray \"\"\" X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b return y def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) _check_reg_targets ( y , prediction , None ) score = r2_score ( y , prediction ) return score","title":"LUPI_Regression_SVM"},{"location":"reference/fri/model/lupi_regression/#ancestors-in-mro_2","text":"fri.model.base_initmodel.LUPI_InitModel fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_regression/#class-variables","text":"HYPERPARAMETER","title":"Class variables"},{"location":"reference/fri/model/lupi_regression/#instance-variables_2","text":"L1_factor L1_factor_priv SOLVER_PARAMS","title":"Instance variables"},{"location":"reference/fri/model/lupi_regression/#methods_2","text":"","title":"Methods"},{"location":"reference/fri/model/lupi_regression/#fit","text":"def fit ( self , X_combined , y , lupi_features = None )","title":"fit"},{"location":"reference/fri/model/lupi_regression/#parameters_1","text":"lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. View Source def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : try : lupi_features = self . lupi_features self . lupi_features = lupi_features except : raise ValueError ( \"No amount of lupi features given.\" ) X , X_priv = split_dataset ( X_combined , self . lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . get_params ()[ \"C\" ] epsilon = self . get_params ()[ \"epsilon\" ] scaling_lupi_w = self . get_params ()[ \"scaling_lupi_w\" ] scaling_lupi_loss = self . get_params ()[ \"scaling_lupi_loss\" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) b = cvx . Variable ( name = \"bias\" ) w_priv_pos = cvx . Variable ( lupi_features , name = \"w_priv_pos\" ) b_priv_pos = cvx . Variable ( name = \"bias_priv_pos\" ) w_priv_neg = cvx . Variable ( lupi_features , name = \"w_priv_neg\" ) b_priv_neg = cvx . Variable ( name = \"bias_priv_neg\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) # Define functions for better readability priv_function_pos = X_priv * w_priv_pos + b_priv_pos priv_function_neg = X_priv * w_priv_neg + b_priv_neg # Combined loss of lupi function and normal slacks , scaled by two constants priv_loss_pos = cvx . sum ( priv_function_pos ) priv_loss_neg = cvx . sum ( priv_function_neg ) priv_loss = priv_loss_pos + priv_loss_neg slack_loss = cvx . sum ( slack ) loss = scaling_lupi_loss * priv_loss + slack_loss # L1 norm regularization of both functions with 1 scaling constant weight_regularization = 0 . 5 * ( cvx . norm ( w , 1 ) + scaling_lupi_w * ( 0 . 5 * cvx . norm ( w_priv_pos , 1 ) + 0 . 5 * cvx . norm ( w_priv_neg , 1 )) ) constraints = [ y - X * w - b <= epsilon + priv_function_pos + slack , X * w + b - y <= epsilon + priv_function_neg + slack , priv_function_pos >= 0 , priv_function_neg >= 0 , # priv_loss_pos >= 0 , # priv_loss_neg >= 0 , # slack_loss >= 0 , slack >= 0 , # loss >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) self . model_state = { \"signs_pos\" : priv_function_pos . value > 0 , \"signs_neg\" : priv_function_neg . value > 0 , \"w\" : w . value , \"w_priv_pos\" : w_priv_pos . value , \"w_priv_neg\" : w_priv_neg . value , \"b\" : b . value , \"b_priv_pos\" : b_priv_pos . value , \"b_priv_neg\" : b_priv_neg . value , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else , } w_l1 = np . linalg . norm ( w . value , ord = 1 ) w_priv_pos_l1 = np . linalg . norm ( w_priv_pos . value , ord = 1 ) w_priv_neg_l1 = np . linalg . norm ( w_priv_neg . value , ord = 1 ) # We take the mean to combine all submodels ( for priv ) into a single normalization factor w_priv_l1 = w_priv_pos_l1 + w_priv_neg_l1 self . constraints = { \"priv_loss\" : priv_loss . value , \"scaling_lupi_loss\" : scaling_lupi_loss , # \"loss_slack\" : slack_loss . value , \"loss\" : loss . value , \"w_l1\" : w_l1 , \"w_priv_l1\" : w_priv_l1 , \"w_priv_pos_l1\" : w_priv_pos_l1 , \"w_priv_neg_l1\" : w_priv_neg_l1 , } return self","title":"Parameters"},{"location":"reference/fri/model/lupi_regression/#get_params","text":"def get_params ( self , deep = True ) Get parameters for this estimator.","title":"get_params"},{"location":"reference/fri/model/lupi_regression/#parameters_2","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"reference/fri/model/lupi_regression/#returns","text":"params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out","title":"Returns"},{"location":"reference/fri/model/lupi_regression/#make_scorer","text":"def make_scorer ( self ) View Source def make_scorer ( self ): return None , None","title":"make_scorer"},{"location":"reference/fri/model/lupi_regression/#predict","text":"def predict ( self , X ) Method to predict points using svm classification rule. We use both normal and priv. features. This function is mainly used for CV purposes to find the best parameters according to score.","title":"predict"},{"location":"reference/fri/model/lupi_regression/#parameters_3","text":"X : numpy.ndarray View Source def predict ( self , X ): \"\"\" Method to predict points using svm classification rule. We use both normal and priv. features. This function is mainly used for CV purposes to find the best parameters according to score. Parameters ---------- X : numpy.ndarray \"\"\" X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b return y","title":"Parameters"},{"location":"reference/fri/model/lupi_regression/#score","text":"def score ( self , X , y , ** kwargs ) View Source def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) _check_reg_targets ( y , prediction , None ) score = r2_score ( y , prediction ) return score","title":"score"},{"location":"reference/fri/model/lupi_regression/#set_params","text":"def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params"},{"location":"reference/fri/model/lupi_regression/#parameters_4","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"reference/fri/model/lupi_regression/#returns_1","text":"self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Returns"},{"location":"reference/fri/model/ordinal_regression/","text":"Module fri.model.ordinal_regression View Source import cvxpy as cvx import numpy as np from sklearn.metrics import make_scorer from sklearn.utils import check_X_y from .base_cvxproblem import Relevance_CVXProblem from .base_initmodel import InitModel from .base_type import ProblemType class OrdinalRegression ( ProblemType ): @classmethod def parameters ( cls ): return [ \"C\" ] @property def get_initmodel_template ( cls ): return OrdinalRegression_SVM @property def get_cvxproblem_template ( cls ): return OrdinalRegression_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y class OrdinalRegression_SVM ( InitModel ): HYPERPARAMETER = [ \"C\" ] def __init__ ( self , C = 1 ): super () . __init__ () self . C = C def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . get_params ()[ \"C\" ] self . classes_ = np . unique ( y ) original_bins = sorted ( self . classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) w = cvx . Variable ( shape = ( d ), name = \"w\" ) # For ordinal regression we use two slack variables, we observe the slack in both directions slack_left = cvx . Variable ( shape = ( n ), name = \"slack_left\" ) slack_right = cvx . Variable ( shape = ( n ), name = \"slack_right\" ) # We have an offset for every bin boundary b_s = cvx . Variable ( shape = ( n_bins - 1 ), name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack_left + slack_right )) constraints = [ slack_left >= 0 , slack_right >= 0 ] # Add constraints for slack into left neighboring bins for i in range ( n_bins - 1 ): indices = np . where ( y == get_old_bin [ i ]) constraints . append ( X [ indices ] * w - slack_left [ indices ] <= b_s [ i ] - 1 ) # Add constraints for slack into right neighboring bins for i in range ( 1 , n_bins ): indices = np . where ( y == get_old_bin [ i ]) constraints . append ( X [ indices ] * w + slack_right [ indices ] >= b_s [ i - 1 ] + 1 ) # Add explicit constraint, that all bins are ascending for i in range ( n_bins - 2 ): constraints . append ( b_s [ i ] <= b_s [ i + 1 ]) # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b_s = b_s . value slack_left = np . asarray ( slack_left . value ) . flatten () slack_right = np . asarray ( slack_right . value ) . flatten () self . model_state = { \"w\" : w , \"b_s\" : b_s , \"slack\" : ( slack_left , slack_right )} loss = np . sum ( slack_left + slack_right ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ): w = self . model_state [ \"w\" ] b_s = self . model_state [ \"b_s\" ] scores = np . dot ( X , w . T )[ np . newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] def score ( self , X , y , error_type = \"mmae\" , return_error = False , ** kwargs ): X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score def make_scorer ( self ): # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \"mze\" ) mae = make_scorer ( ordinal_scores , error_type = \"mae\" ) mmae = make_scorer ( ordinal_scores , error_type = \"mmae\" ) scorer = { \"mze\" : mze , \"mae\" : mae , \"mmae\" : mmae } return scorer , \"mmae\" def ordinal_scores ( y , prediction , error_type , return_error = False ): \"\"\"Score function for ordinal problems. Parameters ---------- y : target class vector Truth vector prediction : prediction class vector Predicted classes error_type : str Error type \"mze\",\"mae\",\"mmae\" return_error : bool, optional Return error (lower is better) or score (inverted, higher is better) Returns ------- float Error or score depending on 'return_error' Raises ------ ValueError When using wrong error_type \"\"\" n = len ( y ) classes = np . unique ( y ) n_bins = len ( classes ) max_dist = n_bins - 1 # If only one class available, we dont need to average if max_dist == 0 : error_type = \"mze\" def mze ( prediction , y ): return np . sum ( prediction != y ) def mae ( prediction , y ): return np . sum ( np . abs ( prediction - y )) # Score based on mean zero-one error if error_type == \"mze\" : error = mze ( prediction , y ) / n score = 1 - error # Score based on mean absolute error elif error_type == \"mae\" : error = mae ( prediction , y ) / n score = ( max_dist - error ) / max_dist # Score based on macro-averaged mean absolute error elif error_type == \"mmae\" : sum = 0 for i in range ( n_bins ): samples = y == i n_samples = np . sum ( samples ) if n_samples > 0 : bin_error = mae ( prediction [ samples ], y [ samples ]) / n_samples sum += bin_error error = sum / n_bins score = ( max_dist - error ) / max_dist else : raise ValueError ( \"error_type {} not available!'\" . format ( error_type )) if return_error : return error else : return score class OrdinalRegression_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): n_bins = len ( np . unique ( self . y )) # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] # New Variables self . w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) # For ordinal regression we use two slack variables, we observe the slack in both directions self . slack_left = cvx . Variable ( shape = ( self . n ), name = \"slack_left\" , nonneg = True ) self . slack_right = cvx . Variable ( shape = ( self . n ), name = \"slack_right\" , nonneg = True ) # We have an offset for every bin boundary self . b_s = cvx . Variable ( shape = ( n_bins - 1 ), name = \"bias\" ) # New Constraints self . loss = cvx . sum ( self . slack_left + self . slack_right ) self . weight_norm = cvx . norm ( self . w , 1 ) for i in range ( n_bins - 1 ): indices = np . where ( self . y == i ) self . add_constraint ( self . X [ indices ] * self . w - self . slack_left [ indices ] <= self . b_s [ i ] - 1 ) for i in range ( 1 , n_bins ): indices = np . where ( self . y == i ) self . add_constraint ( self . X [ indices ] * self . w + self . slack_right [ indices ] >= self . b_s [ i - 1 ] + 1 ) for i in range ( n_bins - 2 ): self . add_constraint ( self . b_s [ i ] <= self . b_s [ i + 1 ]) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Functions ordinal_scores def ordinal_scores ( y , prediction , error_type , return_error = False ) Score function for ordinal problems. Parameters y : target class vector Truth vector prediction : prediction class vector Predicted classes error_type : str Error type \"mze\",\"mae\",\"mmae\" return_error : bool, optional Return error (lower is better) or score (inverted, higher is better) Returns float Error or score depending on 'return_error' Raises ValueError When using wrong error_type View Source def ordinal_scores ( y , prediction , error_type , return_error = False ) : \"\"\"Score function for ordinal problems. Parameters ---------- y : target class vector Truth vector prediction : prediction class vector Predicted classes error_type : str Error type \" mze \",\" mae \",\" mmae \" return_error : bool, optional Return error (lower is better) or score (inverted, higher is better) Returns ------- float Error or score depending on 'return_error' Raises ------ ValueError When using wrong error_type \"\"\" n = len ( y ) classes = np . unique ( y ) n_bins = len ( classes ) max_dist = n_bins - 1 # If only one class available , we dont need to average if max_dist == 0 : error_type = \"mze\" def mze ( prediction , y ) : return np . sum ( prediction != y ) def mae ( prediction , y ) : return np . sum ( np . abs ( prediction - y )) # Score based on mean zero - one error if error_type == \"mze\" : error = mze ( prediction , y ) / n score = 1 - error # Score based on mean absolute error elif error_type == \"mae\" : error = mae ( prediction , y ) / n score = ( max_dist - error ) / max_dist # Score based on macro - averaged mean absolute error elif error_type == \"mmae\" : sum = 0 for i in range ( n_bins ) : samples = y == i n_samples = np . sum ( samples ) if n_samples > 0 : bin_error = mae ( prediction [ samples ] , y [ samples ] ) / n_samples sum += bin_error error = sum / n_bins score = ( max_dist - error ) / max_dist else : raise ValueError ( \"error_type {} not available!'\" . format ( error_type )) if return_error : return error else : return score Classes OrdinalRegression class OrdinalRegression ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class OrdinalRegression ( ProblemType ) : @classmethod def parameters ( cls ) : return [ \"C\" ] @property def get_initmodel_template ( cls ) : return OrdinalRegression_SVM @property def get_cvxproblem_template ( cls ) : return OrdinalRegression_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods parameters def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\" ] Instance variables get_cvxproblem_template get_initmodel_template Methods get_all_parameters def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } get_all_relax_factors def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } get_chosen_parameter def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) get_chosen_relax_factors def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor get_relaxed_constraints def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } postprocessing def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds preprocessing def preprocessing ( self , data , ** kwargs ) View Source def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y relax_constraint def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key )) relax_factors def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] OrdinalRegression_Relevance_Bound class OrdinalRegression_Relevance_Bound ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class OrdinalRegression_Relevance_Bound ( Relevance_CVXProblem ) : def init_objective_UB ( self , sign = None , ** kwargs ) : self . add_constraint ( self . feature_relevance <= sign * self . w [ self.current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ) : self . add_constraint ( cvx . abs ( self . w [ self.current_feature ] ) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ) : n_bins = len ( np . unique ( self . y )) # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] # New Variables self . w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) # For ordinal regression we use two slack variables , we observe the slack in both directions self . slack_left = cvx . Variable ( shape = ( self . n ), name = \"slack_left\" , nonneg = True ) self . slack_right = cvx . Variable ( shape = ( self . n ), name = \"slack_right\" , nonneg = True ) # We have an offset for every bin boundary self . b_s = cvx . Variable ( shape = ( n_bins - 1 ), name = \"bias\" ) # New Constraints self . loss = cvx . sum ( self . slack_left + self . slack_right ) self . weight_norm = cvx . norm ( self . w , 1 ) for i in range ( n_bins - 1 ) : indices = np . where ( self . y == i ) self . add_constraint ( self . X [ indices ] * self . w - self . slack_left [ indices ] <= self . b_s [ i ] - 1 ) for i in range ( 1 , n_bins ) : indices = np . where ( self . y == i ) self . add_constraint ( self . X [ indices ] * self . w + self . slack_right [ indices ] >= self . b_s [ i - 1 ] + 1 ) for i in range ( n_bins - 2 ) : self . add_constraint ( self . b_s [ i ] <= self . b_s [ i + 1 ] ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Ancestors (in MRO) fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC Descendants fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression_Relevance_Bound Static methods aggregate_max_candidates def aggregate_max_candidates ( max_problems_candidates ) View Source @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value aggregate_min_candidates def aggregate_min_candidates ( min_problems_candidates ) View Source @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value generate_lower_bound_problem def generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem generate_upper_bound_problem def generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : for sign in [ -1, 1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem Instance variables accepted_status constraints cvx_problem isProbe is_solved objective probeID solved_relevance solver_kwargs Methods add_constraint def add_constraint ( self , new ) View Source def add_constraint ( self , new ): self . _constraints . append ( new ) init_objective_LB def init_objective_LB ( self , ** kwargs ) View Source def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) init_objective_UB def init_objective_UB ( self , sign = None , ** kwargs ) View Source def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) preprocessing_data def preprocessing_data ( self , data , best_model_state ) View Source def preprocessing_data ( self , data , best_model_state ): X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y ) solve def solve ( self ) -> object View Source def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self OrdinalRegression_SVM class OrdinalRegression_SVM ( C = 1 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class OrdinalRegression_SVM ( InitModel ) : HYPERPARAMETER = [ \"C\" ] def __init__ ( self , C = 1 ) : super (). __init__ () self . C = C def fit ( self , X , y , ** kwargs ) : ( n , d ) = X . shape C = self . get_params () [ \"C\" ] self . classes_ = np . unique ( y ) original_bins = sorted ( self . classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) w = cvx . Variable ( shape = ( d ), name = \"w\" ) # For ordinal regression we use two slack variables , we observe the slack in both directions slack_left = cvx . Variable ( shape = ( n ), name = \"slack_left\" ) slack_right = cvx . Variable ( shape = ( n ), name = \"slack_right\" ) # We have an offset for every bin boundary b_s = cvx . Variable ( shape = ( n_bins - 1 ), name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack_left + slack_right )) constraints = [ slack_left >= 0, slack_right >= 0 ] # Add constraints for slack into left neighboring bins for i in range ( n_bins - 1 ) : indices = np . where ( y == get_old_bin [ i ] ) constraints . append ( X [ indices ] * w - slack_left [ indices ] <= b_s [ i ] - 1 ) # Add constraints for slack into right neighboring bins for i in range ( 1 , n_bins ) : indices = np . where ( y == get_old_bin [ i ] ) constraints . append ( X [ indices ] * w + slack_right [ indices ] >= b_s [ i - 1 ] + 1 ) # Add explicit constraint , that all bins are ascending for i in range ( n_bins - 2 ) : constraints . append ( b_s [ i ] <= b_s [ i + 1 ] ) # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b_s = b_s . value slack_left = np . asarray ( slack_left . value ). flatten () slack_right = np . asarray ( slack_right . value ). flatten () self . model_state = { \"w\" : w , \"b_s\" : b_s , \"slack\" : ( slack_left , slack_right ) } loss = np . sum ( slack_left + slack_right ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ) : w = self . model_state [ \"w\" ] b_s = self . model_state [ \"b_s\" ] scores = np . dot ( X , w . T ) [ np.newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] def score ( self , X , y , error_type = \"mmae\" , return_error = False , ** kwargs ) : X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score def make_scorer ( self ) : # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \"mze\" ) mae = make_scorer ( ordinal_scores , error_type = \"mae\" ) mmae = make_scorer ( ordinal_scores , error_type = \"mmae\" ) scorer = { \"mze\" : mze , \"mae\" : mae , \"mmae\" : mmae } return scorer , \"mmae\" Ancestors (in MRO) fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator Class variables HYPERPARAMETER SOLVER_PARAMS Instance variables L1_factor Methods fit def fit ( self , X , y , ** kwargs ) View Source def fit ( self , X , y , ** kwargs ) : ( n , d ) = X . shape C = self . get_params () [ \"C\" ] self . classes_ = np . unique ( y ) original_bins = sorted ( self . classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) w = cvx . Variable ( shape = ( d ), name = \"w\" ) # For ordinal regression we use two slack variables , we observe the slack in both directions slack_left = cvx . Variable ( shape = ( n ), name = \"slack_left\" ) slack_right = cvx . Variable ( shape = ( n ), name = \"slack_right\" ) # We have an offset for every bin boundary b_s = cvx . Variable ( shape = ( n_bins - 1 ), name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack_left + slack_right )) constraints = [ slack_left >= 0, slack_right >= 0 ] # Add constraints for slack into left neighboring bins for i in range ( n_bins - 1 ) : indices = np . where ( y == get_old_bin [ i ] ) constraints . append ( X [ indices ] * w - slack_left [ indices ] <= b_s [ i ] - 1 ) # Add constraints for slack into right neighboring bins for i in range ( 1 , n_bins ) : indices = np . where ( y == get_old_bin [ i ] ) constraints . append ( X [ indices ] * w + slack_right [ indices ] >= b_s [ i - 1 ] + 1 ) # Add explicit constraint , that all bins are ascending for i in range ( n_bins - 2 ) : constraints . append ( b_s [ i ] <= b_s [ i + 1 ] ) # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b_s = b_s . value slack_left = np . asarray ( slack_left . value ). flatten () slack_right = np . asarray ( slack_right . value ). flatten () self . model_state = { \"w\" : w , \"b_s\" : b_s , \"slack\" : ( slack_left , slack_right ) } loss = np . sum ( slack_left + slack_right ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self get_params def get_params ( self , deep = True ) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out make_scorer def make_scorer ( self ) View Source def make_scorer ( self ): # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \"mze\" ) mae = make_scorer ( ordinal_scores , error_type = \"mae\" ) mmae = make_scorer ( ordinal_scores , error_type = \"mmae\" ) scorer = { \"mze\" : mze , \"mae\" : mae , \"mmae\" : mmae } return scorer , \"mmae\" predict def predict ( self , X ) View Source def predict ( self , X ) : w = self . model_state [ \"w\" ] b_s = self . model_state [ \"b_s\" ] scores = np . dot ( X , w . T ) [ np.newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] score def score ( self , X , y , error_type = 'mmae' , return_error = False , ** kwargs ) View Source def score ( self , X , y , error_type = \"mmae\" , return_error = False , ** kwargs ): X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score set_params def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Ordinal Regression"},{"location":"reference/fri/model/ordinal_regression/#module-frimodelordinal_regression","text":"View Source import cvxpy as cvx import numpy as np from sklearn.metrics import make_scorer from sklearn.utils import check_X_y from .base_cvxproblem import Relevance_CVXProblem from .base_initmodel import InitModel from .base_type import ProblemType class OrdinalRegression ( ProblemType ): @classmethod def parameters ( cls ): return [ \"C\" ] @property def get_initmodel_template ( cls ): return OrdinalRegression_SVM @property def get_cvxproblem_template ( cls ): return OrdinalRegression_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y class OrdinalRegression_SVM ( InitModel ): HYPERPARAMETER = [ \"C\" ] def __init__ ( self , C = 1 ): super () . __init__ () self . C = C def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . get_params ()[ \"C\" ] self . classes_ = np . unique ( y ) original_bins = sorted ( self . classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) w = cvx . Variable ( shape = ( d ), name = \"w\" ) # For ordinal regression we use two slack variables, we observe the slack in both directions slack_left = cvx . Variable ( shape = ( n ), name = \"slack_left\" ) slack_right = cvx . Variable ( shape = ( n ), name = \"slack_right\" ) # We have an offset for every bin boundary b_s = cvx . Variable ( shape = ( n_bins - 1 ), name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack_left + slack_right )) constraints = [ slack_left >= 0 , slack_right >= 0 ] # Add constraints for slack into left neighboring bins for i in range ( n_bins - 1 ): indices = np . where ( y == get_old_bin [ i ]) constraints . append ( X [ indices ] * w - slack_left [ indices ] <= b_s [ i ] - 1 ) # Add constraints for slack into right neighboring bins for i in range ( 1 , n_bins ): indices = np . where ( y == get_old_bin [ i ]) constraints . append ( X [ indices ] * w + slack_right [ indices ] >= b_s [ i - 1 ] + 1 ) # Add explicit constraint, that all bins are ascending for i in range ( n_bins - 2 ): constraints . append ( b_s [ i ] <= b_s [ i + 1 ]) # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b_s = b_s . value slack_left = np . asarray ( slack_left . value ) . flatten () slack_right = np . asarray ( slack_right . value ) . flatten () self . model_state = { \"w\" : w , \"b_s\" : b_s , \"slack\" : ( slack_left , slack_right )} loss = np . sum ( slack_left + slack_right ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ): w = self . model_state [ \"w\" ] b_s = self . model_state [ \"b_s\" ] scores = np . dot ( X , w . T )[ np . newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] def score ( self , X , y , error_type = \"mmae\" , return_error = False , ** kwargs ): X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score def make_scorer ( self ): # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \"mze\" ) mae = make_scorer ( ordinal_scores , error_type = \"mae\" ) mmae = make_scorer ( ordinal_scores , error_type = \"mmae\" ) scorer = { \"mze\" : mze , \"mae\" : mae , \"mmae\" : mmae } return scorer , \"mmae\" def ordinal_scores ( y , prediction , error_type , return_error = False ): \"\"\"Score function for ordinal problems. Parameters ---------- y : target class vector Truth vector prediction : prediction class vector Predicted classes error_type : str Error type \"mze\",\"mae\",\"mmae\" return_error : bool, optional Return error (lower is better) or score (inverted, higher is better) Returns ------- float Error or score depending on 'return_error' Raises ------ ValueError When using wrong error_type \"\"\" n = len ( y ) classes = np . unique ( y ) n_bins = len ( classes ) max_dist = n_bins - 1 # If only one class available, we dont need to average if max_dist == 0 : error_type = \"mze\" def mze ( prediction , y ): return np . sum ( prediction != y ) def mae ( prediction , y ): return np . sum ( np . abs ( prediction - y )) # Score based on mean zero-one error if error_type == \"mze\" : error = mze ( prediction , y ) / n score = 1 - error # Score based on mean absolute error elif error_type == \"mae\" : error = mae ( prediction , y ) / n score = ( max_dist - error ) / max_dist # Score based on macro-averaged mean absolute error elif error_type == \"mmae\" : sum = 0 for i in range ( n_bins ): samples = y == i n_samples = np . sum ( samples ) if n_samples > 0 : bin_error = mae ( prediction [ samples ], y [ samples ]) / n_samples sum += bin_error error = sum / n_bins score = ( max_dist - error ) / max_dist else : raise ValueError ( \"error_type {} not available!'\" . format ( error_type )) if return_error : return error else : return score class OrdinalRegression_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): n_bins = len ( np . unique ( self . y )) # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] # New Variables self . w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) # For ordinal regression we use two slack variables, we observe the slack in both directions self . slack_left = cvx . Variable ( shape = ( self . n ), name = \"slack_left\" , nonneg = True ) self . slack_right = cvx . Variable ( shape = ( self . n ), name = \"slack_right\" , nonneg = True ) # We have an offset for every bin boundary self . b_s = cvx . Variable ( shape = ( n_bins - 1 ), name = \"bias\" ) # New Constraints self . loss = cvx . sum ( self . slack_left + self . slack_right ) self . weight_norm = cvx . norm ( self . w , 1 ) for i in range ( n_bins - 1 ): indices = np . where ( self . y == i ) self . add_constraint ( self . X [ indices ] * self . w - self . slack_left [ indices ] <= self . b_s [ i ] - 1 ) for i in range ( 1 , n_bins ): indices = np . where ( self . y == i ) self . add_constraint ( self . X [ indices ] * self . w + self . slack_right [ indices ] >= self . b_s [ i - 1 ] + 1 ) for i in range ( n_bins - 2 ): self . add_constraint ( self . b_s [ i ] <= self . b_s [ i + 1 ]) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"Module fri.model.ordinal_regression"},{"location":"reference/fri/model/ordinal_regression/#functions","text":"","title":"Functions"},{"location":"reference/fri/model/ordinal_regression/#ordinal_scores","text":"def ordinal_scores ( y , prediction , error_type , return_error = False ) Score function for ordinal problems.","title":"ordinal_scores"},{"location":"reference/fri/model/ordinal_regression/#parameters","text":"y : target class vector Truth vector prediction : prediction class vector Predicted classes error_type : str Error type \"mze\",\"mae\",\"mmae\" return_error : bool, optional Return error (lower is better) or score (inverted, higher is better)","title":"Parameters"},{"location":"reference/fri/model/ordinal_regression/#returns","text":"float Error or score depending on 'return_error'","title":"Returns"},{"location":"reference/fri/model/ordinal_regression/#raises","text":"ValueError When using wrong error_type View Source def ordinal_scores ( y , prediction , error_type , return_error = False ) : \"\"\"Score function for ordinal problems. Parameters ---------- y : target class vector Truth vector prediction : prediction class vector Predicted classes error_type : str Error type \" mze \",\" mae \",\" mmae \" return_error : bool, optional Return error (lower is better) or score (inverted, higher is better) Returns ------- float Error or score depending on 'return_error' Raises ------ ValueError When using wrong error_type \"\"\" n = len ( y ) classes = np . unique ( y ) n_bins = len ( classes ) max_dist = n_bins - 1 # If only one class available , we dont need to average if max_dist == 0 : error_type = \"mze\" def mze ( prediction , y ) : return np . sum ( prediction != y ) def mae ( prediction , y ) : return np . sum ( np . abs ( prediction - y )) # Score based on mean zero - one error if error_type == \"mze\" : error = mze ( prediction , y ) / n score = 1 - error # Score based on mean absolute error elif error_type == \"mae\" : error = mae ( prediction , y ) / n score = ( max_dist - error ) / max_dist # Score based on macro - averaged mean absolute error elif error_type == \"mmae\" : sum = 0 for i in range ( n_bins ) : samples = y == i n_samples = np . sum ( samples ) if n_samples > 0 : bin_error = mae ( prediction [ samples ] , y [ samples ] ) / n_samples sum += bin_error error = sum / n_bins score = ( max_dist - error ) / max_dist else : raise ValueError ( \"error_type {} not available!'\" . format ( error_type )) if return_error : return error else : return score","title":"Raises"},{"location":"reference/fri/model/ordinal_regression/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/ordinal_regression/#ordinalregression","text":"class OrdinalRegression ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class OrdinalRegression ( ProblemType ) : @classmethod def parameters ( cls ) : return [ \"C\" ] @property def get_initmodel_template ( cls ) : return OrdinalRegression_SVM @property def get_cvxproblem_template ( cls ) : return OrdinalRegression_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y","title":"OrdinalRegression"},{"location":"reference/fri/model/ordinal_regression/#ancestors-in-mro","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/ordinal_regression/#static-methods","text":"","title":"Static methods"},{"location":"reference/fri/model/ordinal_regression/#parameters_1","text":"def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\" ]","title":"parameters"},{"location":"reference/fri/model/ordinal_regression/#instance-variables","text":"get_cvxproblem_template get_initmodel_template","title":"Instance variables"},{"location":"reference/fri/model/ordinal_regression/#methods","text":"","title":"Methods"},{"location":"reference/fri/model/ordinal_regression/#get_all_parameters","text":"def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () }","title":"get_all_parameters"},{"location":"reference/fri/model/ordinal_regression/#get_all_relax_factors","text":"def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () }","title":"get_all_relax_factors"},{"location":"reference/fri/model/ordinal_regression/#get_chosen_parameter","text":"def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 )","title":"get_chosen_parameter"},{"location":"reference/fri/model/ordinal_regression/#get_chosen_relax_factors","text":"def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor","title":"get_chosen_relax_factors"},{"location":"reference/fri/model/ordinal_regression/#get_relaxed_constraints","text":"def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () }","title":"get_relaxed_constraints"},{"location":"reference/fri/model/ordinal_regression/#postprocessing","text":"def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds","title":"postprocessing"},{"location":"reference/fri/model/ordinal_regression/#preprocessing","text":"def preprocessing ( self , data , ** kwargs ) View Source def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y","title":"preprocessing"},{"location":"reference/fri/model/ordinal_regression/#relax_constraint","text":"def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"relax_constraint"},{"location":"reference/fri/model/ordinal_regression/#relax_factors","text":"def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ]","title":"relax_factors"},{"location":"reference/fri/model/ordinal_regression/#ordinalregression_relevance_bound","text":"class OrdinalRegression_Relevance_Bound ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class OrdinalRegression_Relevance_Bound ( Relevance_CVXProblem ) : def init_objective_UB ( self , sign = None , ** kwargs ) : self . add_constraint ( self . feature_relevance <= sign * self . w [ self.current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ) : self . add_constraint ( cvx . abs ( self . w [ self.current_feature ] ) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ) : n_bins = len ( np . unique ( self . y )) # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] # New Variables self . w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) # For ordinal regression we use two slack variables , we observe the slack in both directions self . slack_left = cvx . Variable ( shape = ( self . n ), name = \"slack_left\" , nonneg = True ) self . slack_right = cvx . Variable ( shape = ( self . n ), name = \"slack_right\" , nonneg = True ) # We have an offset for every bin boundary self . b_s = cvx . Variable ( shape = ( n_bins - 1 ), name = \"bias\" ) # New Constraints self . loss = cvx . sum ( self . slack_left + self . slack_right ) self . weight_norm = cvx . norm ( self . w , 1 ) for i in range ( n_bins - 1 ) : indices = np . where ( self . y == i ) self . add_constraint ( self . X [ indices ] * self . w - self . slack_left [ indices ] <= self . b_s [ i ] - 1 ) for i in range ( 1 , n_bins ) : indices = np . where ( self . y == i ) self . add_constraint ( self . X [ indices ] * self . w + self . slack_right [ indices ] >= self . b_s [ i - 1 ] + 1 ) for i in range ( n_bins - 2 ) : self . add_constraint ( self . b_s [ i ] <= self . b_s [ i + 1 ] ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"OrdinalRegression_Relevance_Bound"},{"location":"reference/fri/model/ordinal_regression/#ancestors-in-mro_1","text":"fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/ordinal_regression/#descendants","text":"fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression_Relevance_Bound","title":"Descendants"},{"location":"reference/fri/model/ordinal_regression/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/fri/model/ordinal_regression/#aggregate_max_candidates","text":"def aggregate_max_candidates ( max_problems_candidates ) View Source @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value","title":"aggregate_max_candidates"},{"location":"reference/fri/model/ordinal_regression/#aggregate_min_candidates","text":"def aggregate_min_candidates ( min_problems_candidates ) View Source @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value","title":"aggregate_min_candidates"},{"location":"reference/fri/model/ordinal_regression/#generate_lower_bound_problem","text":"def generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem","title":"generate_lower_bound_problem"},{"location":"reference/fri/model/ordinal_regression/#generate_upper_bound_problem","text":"def generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : for sign in [ -1, 1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem","title":"generate_upper_bound_problem"},{"location":"reference/fri/model/ordinal_regression/#instance-variables_1","text":"accepted_status constraints cvx_problem isProbe is_solved objective probeID solved_relevance solver_kwargs","title":"Instance variables"},{"location":"reference/fri/model/ordinal_regression/#methods_1","text":"","title":"Methods"},{"location":"reference/fri/model/ordinal_regression/#add_constraint","text":"def add_constraint ( self , new ) View Source def add_constraint ( self , new ): self . _constraints . append ( new )","title":"add_constraint"},{"location":"reference/fri/model/ordinal_regression/#init_objective_lb","text":"def init_objective_LB ( self , ** kwargs ) View Source def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance )","title":"init_objective_LB"},{"location":"reference/fri/model/ordinal_regression/#init_objective_ub","text":"def init_objective_UB ( self , sign = None , ** kwargs ) View Source def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance )","title":"init_objective_UB"},{"location":"reference/fri/model/ordinal_regression/#preprocessing_data","text":"def preprocessing_data ( self , data , best_model_state ) View Source def preprocessing_data ( self , data , best_model_state ): X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y )","title":"preprocessing_data"},{"location":"reference/fri/model/ordinal_regression/#solve","text":"def solve ( self ) -> object View Source def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self","title":"solve"},{"location":"reference/fri/model/ordinal_regression/#ordinalregression_svm","text":"class OrdinalRegression_SVM ( C = 1 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class OrdinalRegression_SVM ( InitModel ) : HYPERPARAMETER = [ \"C\" ] def __init__ ( self , C = 1 ) : super (). __init__ () self . C = C def fit ( self , X , y , ** kwargs ) : ( n , d ) = X . shape C = self . get_params () [ \"C\" ] self . classes_ = np . unique ( y ) original_bins = sorted ( self . classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) w = cvx . Variable ( shape = ( d ), name = \"w\" ) # For ordinal regression we use two slack variables , we observe the slack in both directions slack_left = cvx . Variable ( shape = ( n ), name = \"slack_left\" ) slack_right = cvx . Variable ( shape = ( n ), name = \"slack_right\" ) # We have an offset for every bin boundary b_s = cvx . Variable ( shape = ( n_bins - 1 ), name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack_left + slack_right )) constraints = [ slack_left >= 0, slack_right >= 0 ] # Add constraints for slack into left neighboring bins for i in range ( n_bins - 1 ) : indices = np . where ( y == get_old_bin [ i ] ) constraints . append ( X [ indices ] * w - slack_left [ indices ] <= b_s [ i ] - 1 ) # Add constraints for slack into right neighboring bins for i in range ( 1 , n_bins ) : indices = np . where ( y == get_old_bin [ i ] ) constraints . append ( X [ indices ] * w + slack_right [ indices ] >= b_s [ i - 1 ] + 1 ) # Add explicit constraint , that all bins are ascending for i in range ( n_bins - 2 ) : constraints . append ( b_s [ i ] <= b_s [ i + 1 ] ) # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b_s = b_s . value slack_left = np . asarray ( slack_left . value ). flatten () slack_right = np . asarray ( slack_right . value ). flatten () self . model_state = { \"w\" : w , \"b_s\" : b_s , \"slack\" : ( slack_left , slack_right ) } loss = np . sum ( slack_left + slack_right ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ) : w = self . model_state [ \"w\" ] b_s = self . model_state [ \"b_s\" ] scores = np . dot ( X , w . T ) [ np.newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] def score ( self , X , y , error_type = \"mmae\" , return_error = False , ** kwargs ) : X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score def make_scorer ( self ) : # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \"mze\" ) mae = make_scorer ( ordinal_scores , error_type = \"mae\" ) mmae = make_scorer ( ordinal_scores , error_type = \"mmae\" ) scorer = { \"mze\" : mze , \"mae\" : mae , \"mmae\" : mmae } return scorer , \"mmae\"","title":"OrdinalRegression_SVM"},{"location":"reference/fri/model/ordinal_regression/#ancestors-in-mro_2","text":"fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/ordinal_regression/#class-variables","text":"HYPERPARAMETER SOLVER_PARAMS","title":"Class variables"},{"location":"reference/fri/model/ordinal_regression/#instance-variables_2","text":"L1_factor","title":"Instance variables"},{"location":"reference/fri/model/ordinal_regression/#methods_2","text":"","title":"Methods"},{"location":"reference/fri/model/ordinal_regression/#fit","text":"def fit ( self , X , y , ** kwargs ) View Source def fit ( self , X , y , ** kwargs ) : ( n , d ) = X . shape C = self . get_params () [ \"C\" ] self . classes_ = np . unique ( y ) original_bins = sorted ( self . classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) w = cvx . Variable ( shape = ( d ), name = \"w\" ) # For ordinal regression we use two slack variables , we observe the slack in both directions slack_left = cvx . Variable ( shape = ( n ), name = \"slack_left\" ) slack_right = cvx . Variable ( shape = ( n ), name = \"slack_right\" ) # We have an offset for every bin boundary b_s = cvx . Variable ( shape = ( n_bins - 1 ), name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack_left + slack_right )) constraints = [ slack_left >= 0, slack_right >= 0 ] # Add constraints for slack into left neighboring bins for i in range ( n_bins - 1 ) : indices = np . where ( y == get_old_bin [ i ] ) constraints . append ( X [ indices ] * w - slack_left [ indices ] <= b_s [ i ] - 1 ) # Add constraints for slack into right neighboring bins for i in range ( 1 , n_bins ) : indices = np . where ( y == get_old_bin [ i ] ) constraints . append ( X [ indices ] * w + slack_right [ indices ] >= b_s [ i - 1 ] + 1 ) # Add explicit constraint , that all bins are ascending for i in range ( n_bins - 2 ) : constraints . append ( b_s [ i ] <= b_s [ i + 1 ] ) # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b_s = b_s . value slack_left = np . asarray ( slack_left . value ). flatten () slack_right = np . asarray ( slack_right . value ). flatten () self . model_state = { \"w\" : w , \"b_s\" : b_s , \"slack\" : ( slack_left , slack_right ) } loss = np . sum ( slack_left + slack_right ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self","title":"fit"},{"location":"reference/fri/model/ordinal_regression/#get_params","text":"def get_params ( self , deep = True ) Get parameters for this estimator.","title":"get_params"},{"location":"reference/fri/model/ordinal_regression/#parameters_2","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"reference/fri/model/ordinal_regression/#returns_1","text":"params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out","title":"Returns"},{"location":"reference/fri/model/ordinal_regression/#make_scorer","text":"def make_scorer ( self ) View Source def make_scorer ( self ): # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \"mze\" ) mae = make_scorer ( ordinal_scores , error_type = \"mae\" ) mmae = make_scorer ( ordinal_scores , error_type = \"mmae\" ) scorer = { \"mze\" : mze , \"mae\" : mae , \"mmae\" : mmae } return scorer , \"mmae\"","title":"make_scorer"},{"location":"reference/fri/model/ordinal_regression/#predict","text":"def predict ( self , X ) View Source def predict ( self , X ) : w = self . model_state [ \"w\" ] b_s = self . model_state [ \"b_s\" ] scores = np . dot ( X , w . T ) [ np.newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ]","title":"predict"},{"location":"reference/fri/model/ordinal_regression/#score","text":"def score ( self , X , y , error_type = 'mmae' , return_error = False , ** kwargs ) View Source def score ( self , X , y , error_type = \"mmae\" , return_error = False , ** kwargs ): X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score","title":"score"},{"location":"reference/fri/model/ordinal_regression/#set_params","text":"def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params"},{"location":"reference/fri/model/ordinal_regression/#parameters_3","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"reference/fri/model/ordinal_regression/#returns_2","text":"self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Returns"},{"location":"reference/fri/model/regression/","text":"Module fri.model.regression View Source import cvxpy as cvx import numpy as np from sklearn.utils import check_X_y from .base_cvxproblem import Relevance_CVXProblem from .base_initmodel import InitModel from .base_type import ProblemType class Regression ( ProblemType ): @classmethod def parameters ( cls ): return [ \"C\" , \"epsilon\" ] @property def get_initmodel_template ( cls ): return Regression_SVR @property def get_cvxproblem_template ( cls ): return Regression_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y class Regression_SVR ( InitModel ): HYPERPARAMETER = [ \"C\" , \"epsilon\" ] def __init__ ( self , C = 1 , epsilon = 0.1 ): super () . __init__ () self . epsilon = epsilon self . C = C def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . get_params ()[ \"C\" ] epsilon = self . get_params ()[ \"epsilon\" ] w = cvx . Variable ( shape = ( d ), name = \"w\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) b = cvx . Variable ( name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . abs ( y - ( X * w + b )) <= epsilon + slack , slack >= 0 ] # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b = b . value slack = np . asarray ( slack . value ) . flatten () self . model_state = { \"w\" : w , \"b\" : b , \"slack\" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ): w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) from sklearn.metrics import r2_score from sklearn.metrics.regression import _check_reg_targets _check_reg_targets ( y , prediction , None ) # Using weighted f1 score to have a stable score for imbalanced datasets score = r2_score ( y , prediction ) return score class Regression_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] epsilon = parameters [ \"epsilon\" ] # New Variables self . w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) self . b = cvx . Variable ( name = \"b\" ) self . slack = cvx . Variable ( shape = ( self . n ), nonneg = True , name = \"slack\" ) # New Constraints distance_from_plane = cvx . abs ( self . y - ( self . X * self . w + self . b )) self . loss = cvx . sum ( self . slack ) self . weight_norm = cvx . norm ( self . w , 1 ) self . add_constraint ( distance_from_plane <= epsilon + self . slack ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Classes Regression class Regression ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Regression ( ProblemType ) : @classmethod def parameters ( cls ) : return [ \"C\", \"epsilon\" ] @property def get_initmodel_template ( cls ) : return Regression_SVR @property def get_cvxproblem_template ( cls ) : return Regression_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods parameters def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\", \"epsilon\" ] Instance variables get_cvxproblem_template get_initmodel_template Methods get_all_parameters def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } get_all_relax_factors def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } get_chosen_parameter def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) get_chosen_relax_factors def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor get_relaxed_constraints def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } postprocessing def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds preprocessing def preprocessing ( self , data , ** kwargs ) View Source def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y relax_constraint def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key )) relax_factors def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] Regression_Relevance_Bound class Regression_Relevance_Bound ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Regression_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] epsilon = parameters [ \"epsilon\" ] # New Variables self . w = cvx . Variable ( shape =( self . d ), name = \"w\" ) self . b = cvx . Variable ( name = \"b\" ) self . slack = cvx . Variable ( shape =( self . n ), nonneg = True , name = \"slack\" ) # New Constraints distance_from_plane = cvx . abs ( self . y - ( self . X * self . w + self . b )) self . loss = cvx . sum ( self . slack ) self . weight_norm = cvx . norm ( self . w , 1 ) self . add_constraint ( distance_from_plane <= epsilon + self . slack ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Ancestors (in MRO) fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC Descendants fri.model.lupi_regression.LUPI_Regression_Relevance_Bound Static methods aggregate_max_candidates def aggregate_max_candidates ( max_problems_candidates ) View Source @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value aggregate_min_candidates def aggregate_min_candidates ( min_problems_candidates ) View Source @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value generate_lower_bound_problem def generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem generate_upper_bound_problem def generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : for sign in [ -1, 1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem Instance variables accepted_status constraints cvx_problem isProbe is_solved objective probeID solved_relevance solver_kwargs Methods add_constraint def add_constraint ( self , new ) View Source def add_constraint ( self , new ): self . _constraints . append ( new ) init_objective_LB def init_objective_LB ( self , ** kwargs ) View Source def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) init_objective_UB def init_objective_UB ( self , sign = None , ** kwargs ) View Source def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) preprocessing_data def preprocessing_data ( self , data , best_model_state ) View Source def preprocessing_data ( self , data , best_model_state ): X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y ) solve def solve ( self ) -> object View Source def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self Regression_SVR class Regression_SVR ( C = 1 , epsilon = 0.1 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Regression_SVR ( InitModel ): HYPERPARAMETER = [ \"C\" , \"epsilon\" ] def __init__ ( self , C = 1 , epsilon = 0.1 ): super (). __init__ () self . epsilon = epsilon self . C = C def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . get_params ()[ \"C\" ] epsilon = self . get_params ()[ \"epsilon\" ] w = cvx . Variable ( shape =( d ), name = \"w\" ) slack = cvx . Variable ( shape =( n ), name = \"slack\" ) b = cvx . Variable ( name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . abs ( y - ( X * w + b )) <= epsilon + slack , slack >= 0 ] # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve (** self . SOLVER_PARAMS ) w = w . value b = b . value slack = np . asarray ( slack . value ). flatten () self . model_state = { \"w\" : w , \"b\" : b , \"slack\" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ): w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) from sklearn . metrics import r2_score from sklearn . metrics . regression import _check_reg_targets _check_reg_targets ( y , prediction , None ) # Using weighted f1 score to have a stable score for imbalanced datasets score = r2_score ( y , prediction ) return score Ancestors (in MRO) fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator Class variables HYPERPARAMETER SOLVER_PARAMS Instance variables L1_factor Methods fit def fit ( self , X , y , ** kwargs ) View Source def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . get_params ()[ \"C\" ] epsilon = self . get_params ()[ \"epsilon\" ] w = cvx . Variable ( shape = ( d ), name = \"w\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) b = cvx . Variable ( name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . abs ( y - ( X * w + b )) <= epsilon + slack , slack >= 0 ] # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b = b . value slack = np . asarray ( slack . value ). flatten () self . model_state = { \"w\" : w , \"b\" : b , \"slack\" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self get_params def get_params ( self , deep = True ) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out make_scorer def make_scorer ( self ) View Source def make_scorer ( self ): return None , None predict def predict ( self , X ) View Source def predict ( self , X ): w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b return y score def score ( self , X , y , ** kwargs ) View Source def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) from sklearn.metrics import r2_score from sklearn.metrics.regression import _check_reg_targets _check_reg_targets ( y , prediction , None ) # Using weighted f1 score to have a stable score for imbalanced datasets score = r2_score ( y , prediction ) return score set_params def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Regression"},{"location":"reference/fri/model/regression/#module-frimodelregression","text":"View Source import cvxpy as cvx import numpy as np from sklearn.utils import check_X_y from .base_cvxproblem import Relevance_CVXProblem from .base_initmodel import InitModel from .base_type import ProblemType class Regression ( ProblemType ): @classmethod def parameters ( cls ): return [ \"C\" , \"epsilon\" ] @property def get_initmodel_template ( cls ): return Regression_SVR @property def get_cvxproblem_template ( cls ): return Regression_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y class Regression_SVR ( InitModel ): HYPERPARAMETER = [ \"C\" , \"epsilon\" ] def __init__ ( self , C = 1 , epsilon = 0.1 ): super () . __init__ () self . epsilon = epsilon self . C = C def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . get_params ()[ \"C\" ] epsilon = self . get_params ()[ \"epsilon\" ] w = cvx . Variable ( shape = ( d ), name = \"w\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) b = cvx . Variable ( name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . abs ( y - ( X * w + b )) <= epsilon + slack , slack >= 0 ] # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b = b . value slack = np . asarray ( slack . value ) . flatten () self . model_state = { \"w\" : w , \"b\" : b , \"slack\" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ): w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) from sklearn.metrics import r2_score from sklearn.metrics.regression import _check_reg_targets _check_reg_targets ( y , prediction , None ) # Using weighted f1 score to have a stable score for imbalanced datasets score = r2_score ( y , prediction ) return score class Regression_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] epsilon = parameters [ \"epsilon\" ] # New Variables self . w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) self . b = cvx . Variable ( name = \"b\" ) self . slack = cvx . Variable ( shape = ( self . n ), nonneg = True , name = \"slack\" ) # New Constraints distance_from_plane = cvx . abs ( self . y - ( self . X * self . w + self . b )) self . loss = cvx . sum ( self . slack ) self . weight_norm = cvx . norm ( self . w , 1 ) self . add_constraint ( distance_from_plane <= epsilon + self . slack ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"Module fri.model.regression"},{"location":"reference/fri/model/regression/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/regression/#regression","text":"class Regression ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Regression ( ProblemType ) : @classmethod def parameters ( cls ) : return [ \"C\", \"epsilon\" ] @property def get_initmodel_template ( cls ) : return Regression_SVR @property def get_cvxproblem_template ( cls ) : return Regression_Relevance_Bound def relax_factors ( cls ) : return [ \"loss_slack\", \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y","title":"Regression"},{"location":"reference/fri/model/regression/#ancestors-in-mro","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/regression/#static-methods","text":"","title":"Static methods"},{"location":"reference/fri/model/regression/#parameters","text":"def parameters ( ) View Source @classmethod def parameters ( cls ) : return [ \"C\", \"epsilon\" ]","title":"parameters"},{"location":"reference/fri/model/regression/#instance-variables","text":"get_cvxproblem_template get_initmodel_template","title":"Instance variables"},{"location":"reference/fri/model/regression/#methods","text":"","title":"Methods"},{"location":"reference/fri/model/regression/#get_all_parameters","text":"def get_all_parameters ( self ) View Source def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters () }","title":"get_all_parameters"},{"location":"reference/fri/model/regression/#get_all_relax_factors","text":"def get_all_relax_factors ( self ) View Source def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () }","title":"get_all_relax_factors"},{"location":"reference/fri/model/regression/#get_chosen_parameter","text":"def get_chosen_parameter ( self , p ) View Source def get_chosen_parameter ( self , p ) : try : return [ self.chosen_parameters_[p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \"scaling_lupi_w\" : # return [ 0.1, 1, 10, 100, 1000 ] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1e-15 , b = 1e15 ) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0, 0.001, 0.01, 0.1, 1, 10, 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 )","title":"get_chosen_parameter"},{"location":"reference/fri/model/regression/#get_chosen_relax_factors","text":"def get_chosen_relax_factors ( self , p ) View Source def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor","title":"get_chosen_relax_factors"},{"location":"reference/fri/model/regression/#get_relaxed_constraints","text":"def get_relaxed_constraints ( self , constraints ) View Source def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () }","title":"get_relaxed_constraints"},{"location":"reference/fri/model/regression/#postprocessing","text":"def postprocessing ( self , bounds ) View Source def postprocessing ( self , bounds ): return bounds","title":"postprocessing"},{"location":"reference/fri/model/regression/#preprocessing","text":"def preprocessing ( self , data , ** kwargs ) View Source def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y","title":"preprocessing"},{"location":"reference/fri/model/regression/#relax_constraint","text":"def relax_constraint ( self , key , value ) View Source def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"relax_constraint"},{"location":"reference/fri/model/regression/#relax_factors","text":"def relax_factors ( cls ) View Source def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ]","title":"relax_factors"},{"location":"reference/fri/model/regression/#regression_relevance_bound","text":"class Regression_Relevance_Bound ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Regression_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] epsilon = parameters [ \"epsilon\" ] # New Variables self . w = cvx . Variable ( shape =( self . d ), name = \"w\" ) self . b = cvx . Variable ( name = \"b\" ) self . slack = cvx . Variable ( shape =( self . n ), nonneg = True , name = \"slack\" ) # New Constraints distance_from_plane = cvx . abs ( self . y - ( self . X * self . w + self . b )) self . loss = cvx . sum ( self . slack ) self . weight_norm = cvx . norm ( self . w , 1 ) self . add_constraint ( distance_from_plane <= epsilon + self . slack ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"Regression_Relevance_Bound"},{"location":"reference/fri/model/regression/#ancestors-in-mro_1","text":"fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/regression/#descendants","text":"fri.model.lupi_regression.LUPI_Regression_Relevance_Bound","title":"Descendants"},{"location":"reference/fri/model/regression/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/fri/model/regression/#aggregate_max_candidates","text":"def aggregate_max_candidates ( max_problems_candidates ) View Source @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value","title":"aggregate_max_candidates"},{"location":"reference/fri/model/regression/#aggregate_min_candidates","text":"def aggregate_min_candidates ( min_problems_candidates ) View Source @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate.solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value","title":"aggregate_min_candidates"},{"location":"reference/fri/model/regression/#generate_lower_bound_problem","text":"def generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem","title":"generate_lower_bound_problem"},{"location":"reference/fri/model/regression/#generate_upper_bound_problem","text":"def generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) View Source @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : for sign in [ -1, 1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem","title":"generate_upper_bound_problem"},{"location":"reference/fri/model/regression/#instance-variables_1","text":"accepted_status constraints cvx_problem isProbe is_solved objective probeID solved_relevance solver_kwargs","title":"Instance variables"},{"location":"reference/fri/model/regression/#methods_1","text":"","title":"Methods"},{"location":"reference/fri/model/regression/#add_constraint","text":"def add_constraint ( self , new ) View Source def add_constraint ( self , new ): self . _constraints . append ( new )","title":"add_constraint"},{"location":"reference/fri/model/regression/#init_objective_lb","text":"def init_objective_LB ( self , ** kwargs ) View Source def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance )","title":"init_objective_LB"},{"location":"reference/fri/model/regression/#init_objective_ub","text":"def init_objective_UB ( self , sign = None , ** kwargs ) View Source def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance )","title":"init_objective_UB"},{"location":"reference/fri/model/regression/#preprocessing_data","text":"def preprocessing_data ( self , data , best_model_state ) View Source def preprocessing_data ( self , data , best_model_state ): X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y )","title":"preprocessing_data"},{"location":"reference/fri/model/regression/#solve","text":"def solve ( self ) -> object View Source def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self","title":"solve"},{"location":"reference/fri/model/regression/#regression_svr","text":"class Regression_SVR ( C = 1 , epsilon = 0.1 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Regression_SVR ( InitModel ): HYPERPARAMETER = [ \"C\" , \"epsilon\" ] def __init__ ( self , C = 1 , epsilon = 0.1 ): super (). __init__ () self . epsilon = epsilon self . C = C def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . get_params ()[ \"C\" ] epsilon = self . get_params ()[ \"epsilon\" ] w = cvx . Variable ( shape =( d ), name = \"w\" ) slack = cvx . Variable ( shape =( n ), name = \"slack\" ) b = cvx . Variable ( name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . abs ( y - ( X * w + b )) <= epsilon + slack , slack >= 0 ] # Solve problem. problem = cvx . Problem ( objective , constraints ) problem . solve (** self . SOLVER_PARAMS ) w = w . value b = b . value slack = np . asarray ( slack . value ). flatten () self . model_state = { \"w\" : w , \"b\" : b , \"slack\" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ): w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) from sklearn . metrics import r2_score from sklearn . metrics . regression import _check_reg_targets _check_reg_targets ( y , prediction , None ) # Using weighted f1 score to have a stable score for imbalanced datasets score = r2_score ( y , prediction ) return score","title":"Regression_SVR"},{"location":"reference/fri/model/regression/#ancestors-in-mro_2","text":"fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/regression/#class-variables","text":"HYPERPARAMETER SOLVER_PARAMS","title":"Class variables"},{"location":"reference/fri/model/regression/#instance-variables_2","text":"L1_factor","title":"Instance variables"},{"location":"reference/fri/model/regression/#methods_2","text":"","title":"Methods"},{"location":"reference/fri/model/regression/#fit","text":"def fit ( self , X , y , ** kwargs ) View Source def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . get_params ()[ \"C\" ] epsilon = self . get_params ()[ \"epsilon\" ] w = cvx . Variable ( shape = ( d ), name = \"w\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) b = cvx . Variable ( name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . abs ( y - ( X * w + b )) <= epsilon + slack , slack >= 0 ] # Solve problem . problem = cvx . Problem ( objective , constraints ) problem . solve ( ** self . SOLVER_PARAMS ) w = w . value b = b . value slack = np . asarray ( slack . value ). flatten () self . model_state = { \"w\" : w , \"b\" : b , \"slack\" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self","title":"fit"},{"location":"reference/fri/model/regression/#get_params","text":"def get_params ( self , deep = True ) Get parameters for this estimator.","title":"get_params"},{"location":"reference/fri/model/regression/#parameters_1","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"reference/fri/model/regression/#returns","text":"params : mapping of string to any Parameter names mapped to their values. View Source def get_params ( self , deep = True ) : \"\"\" Get parameters for this estimator. Parameters ---------- deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns ------- params : mapping of string to any Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names () : try : value = getattr ( self , key ) except AttributeError : warnings . warn ( 'From version 0.24, get_params will raise an ' 'AttributeError if a parameter cannot be ' 'retrieved as an instance attribute. Previously ' 'it would return None.' , FutureWarning ) value = None if deep and hasattr ( value , 'get_params' ) : deep_items = value . get_params (). items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out","title":"Returns"},{"location":"reference/fri/model/regression/#make_scorer","text":"def make_scorer ( self ) View Source def make_scorer ( self ): return None , None","title":"make_scorer"},{"location":"reference/fri/model/regression/#predict","text":"def predict ( self , X ) View Source def predict ( self , X ): w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b return y","title":"predict"},{"location":"reference/fri/model/regression/#score","text":"def score ( self , X , y , ** kwargs ) View Source def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) from sklearn.metrics import r2_score from sklearn.metrics.regression import _check_reg_targets _check_reg_targets ( y , prediction , None ) # Using weighted f1 score to have a stable score for imbalanced datasets score = r2_score ( y , prediction ) return score","title":"score"},{"location":"reference/fri/model/regression/#set_params","text":"def set_params ( self , ** params ) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params"},{"location":"reference/fri/model/regression/#parameters_2","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"reference/fri/model/regression/#returns_1","text":"self : object Estimator instance. View Source def set_params ( self , ** params ) : \"\"\" Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters ---------- **params : dict Estimator parameters. Returns ------- self : object Estimator instance. \"\"\" if not params : # Simple optimization to gain speed ( inspect is slow ) return self valid_params = self . get_params ( deep = True ) nested_params = defaultdict ( dict ) # grouped by prefix for key , value in params . items () : key , delim , sub_key = key . partition ( '__' ) if key not in valid_params : raise ValueError ( 'Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.' % ( key , self )) if delim : nested_params [ key ][ sub_key ] = value else : setattr ( self , key , value ) valid_params [ key ] = value for key , sub_params in nested_params . items () : valid_params [ key ] . set_params ( ** sub_params ) return self","title":"Returns"},{"location":"reference/fri/toydata/","text":"Module fri.toydata View Source import numpy as np from fri import ProblemName from .gen_data import genRegressionData , genClassificationData , genOrdinalRegressionData from .gen_lupi import genLupiData __all__ = [ \"genRegressionData\" , \"genClassificationData\" , \"genOrdinalRegressionData\" , \"genLupiData\" , ] def quick_generate ( problem : object , ** kwargs ) -> [ np . ndarray , np . ndarray ]: \"\"\" Method to wrap individual data generation functions. Allows passing `problem` as a string such as \"classification\" or `ProblemName` object of the corresponding type. For possible kwargs see `genClassificationData' or `genLupiData`. Parameters ---------- problem : str or `ProblemName` Type of data to generate (e.g. \"classification\" or `ProblemName.CLASSIFICATION` kwargs : **dict arguments to pass to the generation functions Returns ------- Tuple[numpy.ndarray, numpy.ndarray] \"\"\" if problem is \"regression\" or problem is ProblemName . REGRESSION : gen = genRegressionData elif problem is \"classification\" or problem is ProblemName . CLASSIFICATION : gen = genClassificationData elif problem is \"ordreg\" or problem is ProblemName . ORDINALREGRESSION : gen = genOrdinalRegressionData elif problem is \"lupi_regression\" or problem is ProblemName . LUPI_REGRESSION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_REGRESSION elif problem is \"lupi_classification\" or problem is ProblemName . LUPI_CLASSIFICATION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_CLASSIFICATION elif problem is \"lupi_ordreg\" or problem is ProblemName . LUPI_ORDREGRESSION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_ORDREGRESSION else : raise ValueError ( \"Unknown problem type. Try 'regression', 'classification' or 'ordreg' and/or add 'lupi_' prefix\" ) return gen ( ** kwargs ) Sub-modules fri.toydata.gen_data fri.toydata.gen_lupi Functions genClassificationData def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None ) Generate synthetic classification data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation. Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes. Raises ValueError Description ValueError Wrong parameters for specified amonut of features/samples. Examples X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None X.shape (200, 2) y.shape (200,) View Source def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None , ) : \"\"\"Generate synthetic classification data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes. Raises ------ ValueError Description ValueError Wrong parameters for specified amonut of features/samples. Examples --------- >>> X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None >>> X.shape (200, 2) >>> y.shape (200,) \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) X = np . zeros (( n_samples , n_features )) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 X_informative , Y = generate_binary_classification_problem ( n_samples , n_strel + part_size , random_state ) X = _fillVariableSpace ( X_informative , random_state , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) # Add target noise if flip_y > 0 : n_flip = int ( flip_y * n_samples ) Y [ random_state.choice(n_samples, n_flip) ] *= - 1 # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise / X . std ()) return X , Y genLupiData def genLupiData ( problemName : fri . ProblemName , n_samples : int = 100 , random_state : object = None , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 ) Generate Lupi Data for Classification, Regression and Ordinal Regression Problems Parameters problemName : ProblemName Defines the type of y-values of the problem. Example ProblemName.CLASSIFICATION . n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated. Returns X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] View Source def genLupiData ( problemName : ProblemName , n_samples : int = 100 , random_state : object = None , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 , ) : \"\"\" Generate Lupi Data for Classification, Regression and Ordinal Regression Problems Parameters ---------- problemName : ProblemName Defines the type of y-values of the problem. Example `ProblemName.CLASSIFICATION`. n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated. Returns ------- X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] \"\"\" random_state = check_random_state ( random_state ) n_informative = n_strel + ( n_weakrel > 0 ) # Create truth ( prototype ) vector which contains true feature contributions # We enforce minimum of 0.1 to circumvent problems when testing for relevance w = random_state . uniform ( low = 0.5 , high = 1 , size = n_informative ) X_informative = random_state . normal ( size = ( n_samples , n_informative )) scores = np . dot ( X_informative , w ) n_features = n_strel + n_weakrel + n_repeated + n_irrel X_priv = _fillVariableSpace ( X_informative , random_state , n_features = n_features , n_redundant = n_weakrel , n_strel = n_strel , n_repeated = n_repeated , partition =[ n_weakrel ] , ) if ( problemName == \"classification\" or problemName == ProblemName . LUPI_CLASSIFICATION ) : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.65 / X_priv . std () ) X = X_priv + e y = np . zeros_like ( scores ) y [ scores > 0 ] = 1 y [ scores <= 0 ] = - 1 elif problemName == \"regression\" or problemName == ProblemName . LUPI_REGRESSION : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.5 / X_priv . std () ) X = X_priv + e y = scores elif ( problemName == \"ordinalRegression\" or problemName == ProblemName . LUPI_ORDREGRESSION ) : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.2 / X_priv . std () ) X = X_priv + e step = 1 / ( n_ordinal_bins ) quantiles = [ i * step for i in range(1, n_ordinal_bins) ] bs = np . quantile ( scores , quantiles ) bs = np . append ( bs , np . inf ) scores = scores [ :, np.newaxis ] y = np . sum ( scores - bs >= 0 , - 1 ) if n_irrel > 0 : X = np . hstack ( [ X, random_state.normal(size=(n_samples, n_irrel)) ] ) X_priv = np . hstack ( [ X_priv, random_state.normal(size=(n_samples, n_irrel)) ] ) if label_noise > 0 : sample = random_state . choice ( len ( y ), int ( len ( y ) * label_noise )) y [ sample ] = random_state . permutation ( y [ sample ] ) return ( X , X_priv , y . squeeze ()) genOrdinalRegressionData def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 ) Generate ordinal regression data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ValueError Wrong parameters for specified amonut of features/samples. View Source def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 , ) : \"\"\" Generate ordinal regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) if not n_target_bins > 1 : raise ValueError ( \"At least 2 target bins needed\" ) # Use normal regression data as starting point X_regression , Y_regression = genRegressionData ( n_samples = int ( n_samples ), n_features = int ( n_features ), n_redundant = int ( n_redundant ), n_strel = int ( n_strel ), n_repeated = int ( n_repeated ), noise = 0 , random_state = random_state , partition = partition , ) bin_size = int ( np . floor ( n_samples / n_target_bins )) rest = int ( n_samples - ( bin_size * n_target_bins )) # Sort the target values and rearange the data accordingly sort_indices = np . argsort ( Y_regression ) X = X_regression [ sort_indices ] Y = Y_regression [ sort_indices ] # Assign ordinal classes as target values for i in range ( n_target_bins ) : Y [ bin_size * i : bin_size * (i + 1) ] = i # Put non divisable rest into last bin if rest > 0 : Y [ -rest: ] = n_target_bins - 1 X , Y = shuffle ( X , Y , random_state = random_state ) # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise ) return X , Y genRegressionData def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None ) -> object Generate synthetic regression data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ValueError Wrong parameters for specified amonut of features/samples. View Source def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , ) -> object : \"\"\"Generate synthetic regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 n_informative = n_strel + part_size X = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [ :n_informative, : ] = 0.3 bias = 0 y = np . dot ( X , ground_truth ) + bias # Add noise if noise > 0.0 : y += random_state . normal ( scale = noise , size = y . shape ) X = _fillVariableSpace ( X , random_state , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) y = np . squeeze ( y ) return X , y","title":"Index"},{"location":"reference/fri/toydata/#module-fritoydata","text":"View Source import numpy as np from fri import ProblemName from .gen_data import genRegressionData , genClassificationData , genOrdinalRegressionData from .gen_lupi import genLupiData __all__ = [ \"genRegressionData\" , \"genClassificationData\" , \"genOrdinalRegressionData\" , \"genLupiData\" , ] def quick_generate ( problem : object , ** kwargs ) -> [ np . ndarray , np . ndarray ]: \"\"\" Method to wrap individual data generation functions. Allows passing `problem` as a string such as \"classification\" or `ProblemName` object of the corresponding type. For possible kwargs see `genClassificationData' or `genLupiData`. Parameters ---------- problem : str or `ProblemName` Type of data to generate (e.g. \"classification\" or `ProblemName.CLASSIFICATION` kwargs : **dict arguments to pass to the generation functions Returns ------- Tuple[numpy.ndarray, numpy.ndarray] \"\"\" if problem is \"regression\" or problem is ProblemName . REGRESSION : gen = genRegressionData elif problem is \"classification\" or problem is ProblemName . CLASSIFICATION : gen = genClassificationData elif problem is \"ordreg\" or problem is ProblemName . ORDINALREGRESSION : gen = genOrdinalRegressionData elif problem is \"lupi_regression\" or problem is ProblemName . LUPI_REGRESSION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_REGRESSION elif problem is \"lupi_classification\" or problem is ProblemName . LUPI_CLASSIFICATION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_CLASSIFICATION elif problem is \"lupi_ordreg\" or problem is ProblemName . LUPI_ORDREGRESSION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_ORDREGRESSION else : raise ValueError ( \"Unknown problem type. Try 'regression', 'classification' or 'ordreg' and/or add 'lupi_' prefix\" ) return gen ( ** kwargs )","title":"Module fri.toydata"},{"location":"reference/fri/toydata/#sub-modules","text":"fri.toydata.gen_data fri.toydata.gen_lupi","title":"Sub-modules"},{"location":"reference/fri/toydata/#functions","text":"","title":"Functions"},{"location":"reference/fri/toydata/#genclassificationdata","text":"def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None ) Generate synthetic classification data","title":"genClassificationData"},{"location":"reference/fri/toydata/#parameters","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation.","title":"Parameters"},{"location":"reference/fri/toydata/#returns","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes.","title":"Returns"},{"location":"reference/fri/toydata/#raises","text":"ValueError Description ValueError Wrong parameters for specified amonut of features/samples.","title":"Raises"},{"location":"reference/fri/toydata/#examples","text":"X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None X.shape (200, 2) y.shape (200,) View Source def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None , ) : \"\"\"Generate synthetic classification data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes. Raises ------ ValueError Description ValueError Wrong parameters for specified amonut of features/samples. Examples --------- >>> X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None >>> X.shape (200, 2) >>> y.shape (200,) \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) X = np . zeros (( n_samples , n_features )) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 X_informative , Y = generate_binary_classification_problem ( n_samples , n_strel + part_size , random_state ) X = _fillVariableSpace ( X_informative , random_state , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) # Add target noise if flip_y > 0 : n_flip = int ( flip_y * n_samples ) Y [ random_state.choice(n_samples, n_flip) ] *= - 1 # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise / X . std ()) return X , Y","title":"Examples"},{"location":"reference/fri/toydata/#genlupidata","text":"def genLupiData ( problemName : fri . ProblemName , n_samples : int = 100 , random_state : object = None , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 ) Generate Lupi Data for Classification, Regression and Ordinal Regression Problems","title":"genLupiData"},{"location":"reference/fri/toydata/#parameters_1","text":"problemName : ProblemName Defines the type of y-values of the problem. Example ProblemName.CLASSIFICATION . n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated.","title":"Parameters"},{"location":"reference/fri/toydata/#returns_1","text":"X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] View Source def genLupiData ( problemName : ProblemName , n_samples : int = 100 , random_state : object = None , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 , ) : \"\"\" Generate Lupi Data for Classification, Regression and Ordinal Regression Problems Parameters ---------- problemName : ProblemName Defines the type of y-values of the problem. Example `ProblemName.CLASSIFICATION`. n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated. Returns ------- X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] \"\"\" random_state = check_random_state ( random_state ) n_informative = n_strel + ( n_weakrel > 0 ) # Create truth ( prototype ) vector which contains true feature contributions # We enforce minimum of 0.1 to circumvent problems when testing for relevance w = random_state . uniform ( low = 0.5 , high = 1 , size = n_informative ) X_informative = random_state . normal ( size = ( n_samples , n_informative )) scores = np . dot ( X_informative , w ) n_features = n_strel + n_weakrel + n_repeated + n_irrel X_priv = _fillVariableSpace ( X_informative , random_state , n_features = n_features , n_redundant = n_weakrel , n_strel = n_strel , n_repeated = n_repeated , partition =[ n_weakrel ] , ) if ( problemName == \"classification\" or problemName == ProblemName . LUPI_CLASSIFICATION ) : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.65 / X_priv . std () ) X = X_priv + e y = np . zeros_like ( scores ) y [ scores > 0 ] = 1 y [ scores <= 0 ] = - 1 elif problemName == \"regression\" or problemName == ProblemName . LUPI_REGRESSION : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.5 / X_priv . std () ) X = X_priv + e y = scores elif ( problemName == \"ordinalRegression\" or problemName == ProblemName . LUPI_ORDREGRESSION ) : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.2 / X_priv . std () ) X = X_priv + e step = 1 / ( n_ordinal_bins ) quantiles = [ i * step for i in range(1, n_ordinal_bins) ] bs = np . quantile ( scores , quantiles ) bs = np . append ( bs , np . inf ) scores = scores [ :, np.newaxis ] y = np . sum ( scores - bs >= 0 , - 1 ) if n_irrel > 0 : X = np . hstack ( [ X, random_state.normal(size=(n_samples, n_irrel)) ] ) X_priv = np . hstack ( [ X_priv, random_state.normal(size=(n_samples, n_irrel)) ] ) if label_noise > 0 : sample = random_state . choice ( len ( y ), int ( len ( y ) * label_noise )) y [ sample ] = random_state . permutation ( y [ sample ] ) return ( X , X_priv , y . squeeze ())","title":"Returns"},{"location":"reference/fri/toydata/#genordinalregressiondata","text":"def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 ) Generate ordinal regression data","title":"genOrdinalRegressionData"},{"location":"reference/fri/toydata/#parameters_2","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes","title":"Parameters"},{"location":"reference/fri/toydata/#returns_2","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target).","title":"Returns"},{"location":"reference/fri/toydata/#raises_1","text":"ValueError Wrong parameters for specified amonut of features/samples. View Source def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 , ) : \"\"\" Generate ordinal regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) if not n_target_bins > 1 : raise ValueError ( \"At least 2 target bins needed\" ) # Use normal regression data as starting point X_regression , Y_regression = genRegressionData ( n_samples = int ( n_samples ), n_features = int ( n_features ), n_redundant = int ( n_redundant ), n_strel = int ( n_strel ), n_repeated = int ( n_repeated ), noise = 0 , random_state = random_state , partition = partition , ) bin_size = int ( np . floor ( n_samples / n_target_bins )) rest = int ( n_samples - ( bin_size * n_target_bins )) # Sort the target values and rearange the data accordingly sort_indices = np . argsort ( Y_regression ) X = X_regression [ sort_indices ] Y = Y_regression [ sort_indices ] # Assign ordinal classes as target values for i in range ( n_target_bins ) : Y [ bin_size * i : bin_size * (i + 1) ] = i # Put non divisable rest into last bin if rest > 0 : Y [ -rest: ] = n_target_bins - 1 X , Y = shuffle ( X , Y , random_state = random_state ) # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise ) return X , Y","title":"Raises"},{"location":"reference/fri/toydata/#genregressiondata","text":"def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None ) -> object Generate synthetic regression data","title":"genRegressionData"},{"location":"reference/fri/toydata/#parameters_3","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation.","title":"Parameters"},{"location":"reference/fri/toydata/#returns_3","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target).","title":"Returns"},{"location":"reference/fri/toydata/#raises_2","text":"ValueError Wrong parameters for specified amonut of features/samples. View Source def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , ) -> object : \"\"\"Generate synthetic regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 n_informative = n_strel + part_size X = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [ :n_informative, : ] = 0.3 bias = 0 y = np . dot ( X , ground_truth ) + bias # Add noise if noise > 0.0 : y += random_state . normal ( scale = noise , size = y . shape ) X = _fillVariableSpace ( X , random_state , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) y = np . squeeze ( y ) return X , y","title":"Raises"},{"location":"reference/fri/toydata/gen_data/","text":"Module fri.toydata.gen_data View Source import numpy as np from numpy.random import RandomState from sklearn.utils import check_random_state from sklearn.utils import shuffle def _combFeat ( n , size , strRelFeat , randomstate ): # Split each strongly relevant feature into linear combination of it weakFeats = np . tile ( strRelFeat , ( size , 1 )) . T weakFeats = randomstate . uniform ( low = 1 , high = 2 , size = size ) + weakFeats return weakFeats def _dummyFeat ( n , randomstate ): return randomstate . randn ( n ) def _repeatFeat ( feats , i , randomstate ): i_pick = randomstate . choice ( i ) return feats [:, i_pick ] def _checkParam ( n_samples : int = 100 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , n_features = 1 , flip_y : float = 0 , noise : float = 1 , partition = None , ** kwargs , ): if not 1 < n_samples : raise ValueError ( \"We need at least 2 samples.\" ) if not 0 < n_features : raise ValueError ( \"We need at least one feature.\" ) if not 0 <= flip_y < 1 : raise ValueError ( \"Flip percentage has to be between 0 and 1.\" ) if not n_redundant + n_repeated + n_strel <= n_features : raise ValueError ( \"Inconsistent number of features\" ) if n_strel + n_redundant < 1 : raise ValueError ( \"No informative features.\" ) if n_strel == 0 and n_redundant < 2 : raise ValueError ( \"We need more than 1 redundant feature.\" ) if partition is not None : if sum ( partition ) != n_redundant : raise ValueError ( \"Sum of partition values should yield number of redundant features.\" ) if 0 in partition or 1 in partition : raise ValueError ( \"Subset defined in Partition needs at least 2 features. 0 and 1 is not allowed.\" ) def _fillVariableSpace ( X_informative , random_state : RandomState , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , partition = None , ): n_samples = len ( X_informative ) if partition is not None : assert n_redundant == np . sum ( partition ) # Create dummy array X = np . zeros (( int ( n_samples ), int ( n_features ))) # Add strongly relevant X [:, : n_strel ] = X_informative [:, : n_strel ] # Save strongly relevant used in creation of weakly ones holdout = X_informative [:, n_strel :] i = n_strel pi = 0 for x in range ( holdout . shape [ 1 ]): size = partition [ pi ] X [:, i : i + size ] = _combFeat ( n_samples , size , holdout [:, x ], random_state ) i += size pi += 1 for x in range ( n_repeated ): X [:, i ] = _repeatFeat ( X [:, : i ], i , random_state ) i += 1 for x in range ( n_features - i ): X [:, i ] = _dummyFeat ( n_samples , random_state ) i += 1 return X def generate_binary_classification_problem ( n_samples : int , features : int , random_state : RandomState = None , data_range = 1 ): \"\"\" Generate data uniformly distributed in a square and perfectly separated by the hyperplane given by normal_vector and b. Keyword arguments: n_samples -- number of samples required (default 100) n_features -- number of features required normal_vector -- the normal vector of the separating hyperplane data_range -- data is distributed between -data_range and data_range (default 10) \"\"\" random_state = check_random_state ( random_state ) data = random_state . normal ( size = ( n_samples , features ), scale = data_range ) labels = np . sum ( data , 1 ) > 0 labels = labels . astype ( int ) labels [ labels == 0 ] = - 1 return data , labels def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None , ): \"\"\"Generate synthetic classification data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes. Raises ------ ValueError Description ValueError Wrong parameters for specified amonut of features/samples. Examples --------- >>> X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None >>> X.shape (200, 2) >>> y.shape (200,) \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) X = np . zeros (( n_samples , n_features )) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 X_informative , Y = generate_binary_classification_problem ( n_samples , n_strel + part_size , random_state ) X = _fillVariableSpace ( X_informative , random_state , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) # Add target noise if flip_y > 0 : n_flip = int ( flip_y * n_samples ) Y [ random_state . choice ( n_samples , n_flip )] *= - 1 # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise / X . std ()) return X , Y def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , ) -> object : \"\"\"Generate synthetic regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 n_informative = n_strel + part_size X = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [: n_informative , :] = 0.3 bias = 0 y = np . dot ( X , ground_truth ) + bias # Add noise if noise > 0.0 : y += random_state . normal ( scale = noise , size = y . shape ) X = _fillVariableSpace ( X , random_state , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) y = np . squeeze ( y ) return X , y def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 , ): \"\"\" Generate ordinal regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) if not n_target_bins > 1 : raise ValueError ( \"At least 2 target bins needed\" ) # Use normal regression data as starting point X_regression , Y_regression = genRegressionData ( n_samples = int ( n_samples ), n_features = int ( n_features ), n_redundant = int ( n_redundant ), n_strel = int ( n_strel ), n_repeated = int ( n_repeated ), noise = 0 , random_state = random_state , partition = partition , ) bin_size = int ( np . floor ( n_samples / n_target_bins )) rest = int ( n_samples - ( bin_size * n_target_bins )) # Sort the target values and rearange the data accordingly sort_indices = np . argsort ( Y_regression ) X = X_regression [ sort_indices ] Y = Y_regression [ sort_indices ] # Assign ordinal classes as target values for i in range ( n_target_bins ): Y [ bin_size * i : bin_size * ( i + 1 )] = i # Put non divisable rest into last bin if rest > 0 : Y [ - rest :] = n_target_bins - 1 X , Y = shuffle ( X , Y , random_state = random_state ) # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise ) return X , Y Functions genClassificationData def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None ) Generate synthetic classification data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation. Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes. Raises ValueError Description ValueError Wrong parameters for specified amonut of features/samples. Examples X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None X.shape (200, 2) y.shape (200,) View Source def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None , ) : \"\"\"Generate synthetic classification data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes. Raises ------ ValueError Description ValueError Wrong parameters for specified amonut of features/samples. Examples --------- >>> X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None >>> X.shape (200, 2) >>> y.shape (200,) \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) X = np . zeros (( n_samples , n_features )) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 X_informative , Y = generate_binary_classification_problem ( n_samples , n_strel + part_size , random_state ) X = _fillVariableSpace ( X_informative , random_state , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) # Add target noise if flip_y > 0 : n_flip = int ( flip_y * n_samples ) Y [ random_state.choice(n_samples, n_flip) ] *= - 1 # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise / X . std ()) return X , Y genOrdinalRegressionData def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 ) Generate ordinal regression data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ValueError Wrong parameters for specified amonut of features/samples. View Source def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 , ) : \"\"\" Generate ordinal regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) if not n_target_bins > 1 : raise ValueError ( \"At least 2 target bins needed\" ) # Use normal regression data as starting point X_regression , Y_regression = genRegressionData ( n_samples = int ( n_samples ), n_features = int ( n_features ), n_redundant = int ( n_redundant ), n_strel = int ( n_strel ), n_repeated = int ( n_repeated ), noise = 0 , random_state = random_state , partition = partition , ) bin_size = int ( np . floor ( n_samples / n_target_bins )) rest = int ( n_samples - ( bin_size * n_target_bins )) # Sort the target values and rearange the data accordingly sort_indices = np . argsort ( Y_regression ) X = X_regression [ sort_indices ] Y = Y_regression [ sort_indices ] # Assign ordinal classes as target values for i in range ( n_target_bins ) : Y [ bin_size * i : bin_size * (i + 1) ] = i # Put non divisable rest into last bin if rest > 0 : Y [ -rest: ] = n_target_bins - 1 X , Y = shuffle ( X , Y , random_state = random_state ) # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise ) return X , Y genRegressionData def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None ) -> object Generate synthetic regression data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ValueError Wrong parameters for specified amonut of features/samples. View Source def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , ) -> object : \"\"\"Generate synthetic regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 n_informative = n_strel + part_size X = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [ :n_informative, : ] = 0.3 bias = 0 y = np . dot ( X , ground_truth ) + bias # Add noise if noise > 0.0 : y += random_state . normal ( scale = noise , size = y . shape ) X = _fillVariableSpace ( X , random_state , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) y = np . squeeze ( y ) return X , y generate_binary_classification_problem def generate_binary_classification_problem ( n_samples : int , features : int , random_state : numpy . random . mtrand . RandomState = None , data_range = 1 ) Generate data uniformly distributed in a square and perfectly separated by the hyperplane given by normal_vector and b. Keyword arguments: n_samples -- number of samples required (default 100) n_features -- number of features required normal_vector -- the normal vector of the separating hyperplane data_range -- data is distributed between -data_range and data_range (default 10) View Source def generate_binary_classification_problem ( n_samples : int , features : int , random_state : RandomState = None , data_range = 1 ): \"\"\" Generate data uniformly distributed in a square and perfectly separated by the hyperplane given by normal_vector and b. Keyword arguments: n_samples -- number of samples required (default 100) n_features -- number of features required normal_vector -- the normal vector of the separating hyperplane data_range -- data is distributed between -data_range and data_range (default 10) \"\"\" random_state = check_random_state ( random_state ) data = random_state . normal ( size = ( n_samples , features ), scale = data_range ) labels = np . sum ( data , 1 ) > 0 labels = labels . astype ( int ) labels [ labels == 0 ] = - 1 return data , labels","title":"Gen Data"},{"location":"reference/fri/toydata/gen_data/#module-fritoydatagen_data","text":"View Source import numpy as np from numpy.random import RandomState from sklearn.utils import check_random_state from sklearn.utils import shuffle def _combFeat ( n , size , strRelFeat , randomstate ): # Split each strongly relevant feature into linear combination of it weakFeats = np . tile ( strRelFeat , ( size , 1 )) . T weakFeats = randomstate . uniform ( low = 1 , high = 2 , size = size ) + weakFeats return weakFeats def _dummyFeat ( n , randomstate ): return randomstate . randn ( n ) def _repeatFeat ( feats , i , randomstate ): i_pick = randomstate . choice ( i ) return feats [:, i_pick ] def _checkParam ( n_samples : int = 100 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , n_features = 1 , flip_y : float = 0 , noise : float = 1 , partition = None , ** kwargs , ): if not 1 < n_samples : raise ValueError ( \"We need at least 2 samples.\" ) if not 0 < n_features : raise ValueError ( \"We need at least one feature.\" ) if not 0 <= flip_y < 1 : raise ValueError ( \"Flip percentage has to be between 0 and 1.\" ) if not n_redundant + n_repeated + n_strel <= n_features : raise ValueError ( \"Inconsistent number of features\" ) if n_strel + n_redundant < 1 : raise ValueError ( \"No informative features.\" ) if n_strel == 0 and n_redundant < 2 : raise ValueError ( \"We need more than 1 redundant feature.\" ) if partition is not None : if sum ( partition ) != n_redundant : raise ValueError ( \"Sum of partition values should yield number of redundant features.\" ) if 0 in partition or 1 in partition : raise ValueError ( \"Subset defined in Partition needs at least 2 features. 0 and 1 is not allowed.\" ) def _fillVariableSpace ( X_informative , random_state : RandomState , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , partition = None , ): n_samples = len ( X_informative ) if partition is not None : assert n_redundant == np . sum ( partition ) # Create dummy array X = np . zeros (( int ( n_samples ), int ( n_features ))) # Add strongly relevant X [:, : n_strel ] = X_informative [:, : n_strel ] # Save strongly relevant used in creation of weakly ones holdout = X_informative [:, n_strel :] i = n_strel pi = 0 for x in range ( holdout . shape [ 1 ]): size = partition [ pi ] X [:, i : i + size ] = _combFeat ( n_samples , size , holdout [:, x ], random_state ) i += size pi += 1 for x in range ( n_repeated ): X [:, i ] = _repeatFeat ( X [:, : i ], i , random_state ) i += 1 for x in range ( n_features - i ): X [:, i ] = _dummyFeat ( n_samples , random_state ) i += 1 return X def generate_binary_classification_problem ( n_samples : int , features : int , random_state : RandomState = None , data_range = 1 ): \"\"\" Generate data uniformly distributed in a square and perfectly separated by the hyperplane given by normal_vector and b. Keyword arguments: n_samples -- number of samples required (default 100) n_features -- number of features required normal_vector -- the normal vector of the separating hyperplane data_range -- data is distributed between -data_range and data_range (default 10) \"\"\" random_state = check_random_state ( random_state ) data = random_state . normal ( size = ( n_samples , features ), scale = data_range ) labels = np . sum ( data , 1 ) > 0 labels = labels . astype ( int ) labels [ labels == 0 ] = - 1 return data , labels def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None , ): \"\"\"Generate synthetic classification data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes. Raises ------ ValueError Description ValueError Wrong parameters for specified amonut of features/samples. Examples --------- >>> X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None >>> X.shape (200, 2) >>> y.shape (200,) \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) X = np . zeros (( n_samples , n_features )) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 X_informative , Y = generate_binary_classification_problem ( n_samples , n_strel + part_size , random_state ) X = _fillVariableSpace ( X_informative , random_state , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) # Add target noise if flip_y > 0 : n_flip = int ( flip_y * n_samples ) Y [ random_state . choice ( n_samples , n_flip )] *= - 1 # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise / X . std ()) return X , Y def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , ) -> object : \"\"\"Generate synthetic regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 n_informative = n_strel + part_size X = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [: n_informative , :] = 0.3 bias = 0 y = np . dot ( X , ground_truth ) + bias # Add noise if noise > 0.0 : y += random_state . normal ( scale = noise , size = y . shape ) X = _fillVariableSpace ( X , random_state , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) y = np . squeeze ( y ) return X , y def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 , ): \"\"\" Generate ordinal regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) if not n_target_bins > 1 : raise ValueError ( \"At least 2 target bins needed\" ) # Use normal regression data as starting point X_regression , Y_regression = genRegressionData ( n_samples = int ( n_samples ), n_features = int ( n_features ), n_redundant = int ( n_redundant ), n_strel = int ( n_strel ), n_repeated = int ( n_repeated ), noise = 0 , random_state = random_state , partition = partition , ) bin_size = int ( np . floor ( n_samples / n_target_bins )) rest = int ( n_samples - ( bin_size * n_target_bins )) # Sort the target values and rearange the data accordingly sort_indices = np . argsort ( Y_regression ) X = X_regression [ sort_indices ] Y = Y_regression [ sort_indices ] # Assign ordinal classes as target values for i in range ( n_target_bins ): Y [ bin_size * i : bin_size * ( i + 1 )] = i # Put non divisable rest into last bin if rest > 0 : Y [ - rest :] = n_target_bins - 1 X , Y = shuffle ( X , Y , random_state = random_state ) # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise ) return X , Y","title":"Module fri.toydata.gen_data"},{"location":"reference/fri/toydata/gen_data/#functions","text":"","title":"Functions"},{"location":"reference/fri/toydata/gen_data/#genclassificationdata","text":"def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None ) Generate synthetic classification data","title":"genClassificationData"},{"location":"reference/fri/toydata/gen_data/#parameters","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation.","title":"Parameters"},{"location":"reference/fri/toydata/gen_data/#returns","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes.","title":"Returns"},{"location":"reference/fri/toydata/gen_data/#raises","text":"ValueError Description ValueError Wrong parameters for specified amonut of features/samples.","title":"Raises"},{"location":"reference/fri/toydata/gen_data/#examples","text":"X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None X.shape (200, 2) y.shape (200,) View Source def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None , ) : \"\"\"Generate synthetic classification data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes. Raises ------ ValueError Description ValueError Wrong parameters for specified amonut of features/samples. Examples --------- >>> X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None >>> X.shape (200, 2) >>> y.shape (200,) \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) X = np . zeros (( n_samples , n_features )) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 X_informative , Y = generate_binary_classification_problem ( n_samples , n_strel + part_size , random_state ) X = _fillVariableSpace ( X_informative , random_state , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) # Add target noise if flip_y > 0 : n_flip = int ( flip_y * n_samples ) Y [ random_state.choice(n_samples, n_flip) ] *= - 1 # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise / X . std ()) return X , Y","title":"Examples"},{"location":"reference/fri/toydata/gen_data/#genordinalregressiondata","text":"def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 ) Generate ordinal regression data","title":"genOrdinalRegressionData"},{"location":"reference/fri/toydata/gen_data/#parameters_1","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes","title":"Parameters"},{"location":"reference/fri/toydata/gen_data/#returns_1","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target).","title":"Returns"},{"location":"reference/fri/toydata/gen_data/#raises_1","text":"ValueError Wrong parameters for specified amonut of features/samples. View Source def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 , ) : \"\"\" Generate ordinal regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) if not n_target_bins > 1 : raise ValueError ( \"At least 2 target bins needed\" ) # Use normal regression data as starting point X_regression , Y_regression = genRegressionData ( n_samples = int ( n_samples ), n_features = int ( n_features ), n_redundant = int ( n_redundant ), n_strel = int ( n_strel ), n_repeated = int ( n_repeated ), noise = 0 , random_state = random_state , partition = partition , ) bin_size = int ( np . floor ( n_samples / n_target_bins )) rest = int ( n_samples - ( bin_size * n_target_bins )) # Sort the target values and rearange the data accordingly sort_indices = np . argsort ( Y_regression ) X = X_regression [ sort_indices ] Y = Y_regression [ sort_indices ] # Assign ordinal classes as target values for i in range ( n_target_bins ) : Y [ bin_size * i : bin_size * (i + 1) ] = i # Put non divisable rest into last bin if rest > 0 : Y [ -rest: ] = n_target_bins - 1 X , Y = shuffle ( X , Y , random_state = random_state ) # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise ) return X , Y","title":"Raises"},{"location":"reference/fri/toydata/gen_data/#genregressiondata","text":"def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None ) -> object Generate synthetic regression data","title":"genRegressionData"},{"location":"reference/fri/toydata/gen_data/#parameters_2","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation.","title":"Parameters"},{"location":"reference/fri/toydata/gen_data/#returns_2","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target).","title":"Returns"},{"location":"reference/fri/toydata/gen_data/#raises_2","text":"ValueError Wrong parameters for specified amonut of features/samples. View Source def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , ) -> object : \"\"\"Generate synthetic regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 n_informative = n_strel + part_size X = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [ :n_informative, : ] = 0.3 bias = 0 y = np . dot ( X , ground_truth ) + bias # Add noise if noise > 0.0 : y += random_state . normal ( scale = noise , size = y . shape ) X = _fillVariableSpace ( X , random_state , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) y = np . squeeze ( y ) return X , y","title":"Raises"},{"location":"reference/fri/toydata/gen_data/#generate_binary_classification_problem","text":"def generate_binary_classification_problem ( n_samples : int , features : int , random_state : numpy . random . mtrand . RandomState = None , data_range = 1 ) Generate data uniformly distributed in a square and perfectly separated by the hyperplane given by normal_vector and b. Keyword arguments: n_samples -- number of samples required (default 100) n_features -- number of features required normal_vector -- the normal vector of the separating hyperplane data_range -- data is distributed between -data_range and data_range (default 10) View Source def generate_binary_classification_problem ( n_samples : int , features : int , random_state : RandomState = None , data_range = 1 ): \"\"\" Generate data uniformly distributed in a square and perfectly separated by the hyperplane given by normal_vector and b. Keyword arguments: n_samples -- number of samples required (default 100) n_features -- number of features required normal_vector -- the normal vector of the separating hyperplane data_range -- data is distributed between -data_range and data_range (default 10) \"\"\" random_state = check_random_state ( random_state ) data = random_state . normal ( size = ( n_samples , features ), scale = data_range ) labels = np . sum ( data , 1 ) > 0 labels = labels . astype ( int ) labels [ labels == 0 ] = - 1 return data , labels","title":"generate_binary_classification_problem"},{"location":"reference/fri/toydata/gen_lupi/","text":"Module fri.toydata.gen_lupi View Source import numpy as np from sklearn.utils import check_random_state from fri import ProblemName from .gen_data import _fillVariableSpace def _checkLupiParam ( problemName , lupiType , n_strel , n_weakrel , n_priv_weakrel , partition , partition_priv ): \"\"\" Checks if the parameters supplied to the genLupiData() function are okay. Parameters ---------- problemName : Str Must be one of ['classification', 'regression', 'ordinalRegression'] lupiType : Str Must be one of ['cleanLabels', 'cleanFeatures'] n_strel : int Stands for the number of strongly relevant features to generate in genLupiData() Must be greater than 0 n_weakrel : int Must be equal to the length of the partition list n_priv_weakrel : int Must be equal to the length of the partition_priv list partition : list of int The length of the list must be equal to n_weakrel partition_priv : list of int The length of the list must be equal to n_priv_weakrel \"\"\" if type ( problemName ) is not ProblemName : raise ValueError ( \"Not of Type ProblemName\" ) if lupiType not in [ \"cleanLabels\" , \"cleanFeatures\" ]: raise ValueError ( \"The lupiType parameter must be a string out of ['cleanLabels', 'cleanFeatures'].\" ) if n_strel < 1 : raise ValueError ( \"At least one strongly relevant feature is necessary (Parmeter 'n_strel' must be greater than 0).\" ) if partition is not None : if sum ( partition ) != n_weakrel : raise ValueError ( \"The sum over the entries in the partition list must be equal to the parameter 'n_weakrel'.\" ) if 0 in partition or 1 in partition : raise ValueError ( \"The entries in the partition list must be greater or equal to 2.\" ) if partition_priv is not None : if sum ( partition_priv ) != n_priv_weakrel : raise ValueError ( \"The sum over the entries in the partition_priv list must be equal to the parameter 'n_priv_weakrel'.\" ) if 0 in partition_priv or 1 in partition_priv : raise ValueError ( \"The entries in the partition_priv list must be greater or equal to 2.\" ) if lupiType == \"cleanLabels\" and n_priv_weakrel > 0 : raise ValueError ( \"The 'cleanLabels' data has only one strongly relevant feature by nature, this can be repeated ('n_priv_repeated'),\" \"or useless information can be added ('n_priv_irrel') but it can not be weakend => n_priv_weakrel hast to be 0.\" ) # def _genWeakFeatures(n_weakrel, X, random_state, partition): # \"\"\" # Generate n_weakrel features out of the strRelFeature # # Parameters # ---------- # n_weakrel : int # Number of weakly relevant feature to be generated # X : array of shape [n_samples, n_features] # Contains the data out of which the weakly relevant features are created # random_state : Random State object # Used to generate the samples # partition : list of int # Used to define how many weak features are calculated from the same strong feature # The sum of the entries in the partition list must be equal to n_weakrel # # # Returns # ---------- # X_weakrel : array of shape [n_samples, n_weakrel] # Contains the data of the generated weak relevant features # \"\"\" # # X_weakrel = np.zeros([X.shape[0], n_weakrel]) # # if partition is None: # for i in range(n_weakrel): # X_weakrel[:, i] = X[ # :, random_state.choice(X.shape[1]) # ] + random_state.normal(loc=0, scale=1, size=1) # else: # idx = 0 # for j in range(len(partition)): # X_weakrel[:, idx : idx + partition[j]] = np.tile( # X[:, random_state.choice(X.shape[1])], (partition[j], 1) # ).T + random_state.normal(loc=0, scale=1, size=partition[j]) # idx += partition[j] # # return X_weakrel # # # def _genRepeatedFeatures(n_repeated, X, random_state): # \"\"\" # Generate repeated features by picking a random existing feature out of X # # Parameters # ---------- # n_repeated : int # Number of repeated features to create # X : array of shape [n_samples, n_features] # Contains the data of which the repeated features are picked # random_state : Random State object # Used to randomly pick a feature out of X # \"\"\" # # X_repeated = np.zeros([X.shape[0], n_repeated]) # for i in range(n_repeated): # X_repeated[:, i] = X[:, random_state.choice(X.shape[1])] # # return X_repeated def genLupiData ( problemName : ProblemName , n_samples : int = 100 , random_state : object = None , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 , ): \"\"\" Generate Lupi Data for Classification, Regression and Ordinal Regression Problems Parameters ---------- problemName : ProblemName Defines the type of y-values of the problem. Example `ProblemName.CLASSIFICATION`. n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated. Returns ------- X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] \"\"\" random_state = check_random_state ( random_state ) n_informative = n_strel + ( n_weakrel > 0 ) # Create truth (prototype) vector which contains true feature contributions # We enforce minimum of 0.1 to circumvent problems when testing for relevance w = random_state . uniform ( low = 0.5 , high = 1 , size = n_informative ) X_informative = random_state . normal ( size = ( n_samples , n_informative )) scores = np . dot ( X_informative , w ) n_features = n_strel + n_weakrel + n_repeated + n_irrel X_priv = _fillVariableSpace ( X_informative , random_state , n_features = n_features , n_redundant = n_weakrel , n_strel = n_strel , n_repeated = n_repeated , partition = [ n_weakrel ], ) if ( problemName == \"classification\" or problemName == ProblemName . LUPI_CLASSIFICATION ): e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ]), scale = 0.65 / X_priv . std () ) X = X_priv + e y = np . zeros_like ( scores ) y [ scores > 0 ] = 1 y [ scores <= 0 ] = - 1 elif problemName == \"regression\" or problemName == ProblemName . LUPI_REGRESSION : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ]), scale = 0.5 / X_priv . std () ) X = X_priv + e y = scores elif ( problemName == \"ordinalRegression\" or problemName == ProblemName . LUPI_ORDREGRESSION ): e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ]), scale = 0.2 / X_priv . std () ) X = X_priv + e step = 1 / ( n_ordinal_bins ) quantiles = [ i * step for i in range ( 1 , n_ordinal_bins )] bs = np . quantile ( scores , quantiles ) bs = np . append ( bs , np . inf ) scores = scores [:, np . newaxis ] y = np . sum ( scores - bs >= 0 , - 1 ) if n_irrel > 0 : X = np . hstack ([ X , random_state . normal ( size = ( n_samples , n_irrel ))]) X_priv = np . hstack ([ X_priv , random_state . normal ( size = ( n_samples , n_irrel ))]) if label_noise > 0 : sample = random_state . choice ( len ( y ), int ( len ( y ) * label_noise )) y [ sample ] = random_state . permutation ( y [ sample ]) return ( X , X_priv , y . squeeze ()) Functions genLupiData def genLupiData ( problemName : fri . ProblemName , n_samples : int = 100 , random_state : object = None , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 ) Generate Lupi Data for Classification, Regression and Ordinal Regression Problems Parameters problemName : ProblemName Defines the type of y-values of the problem. Example ProblemName.CLASSIFICATION . n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated. Returns X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] View Source def genLupiData ( problemName : ProblemName , n_samples : int = 100 , random_state : object = None , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 , ) : \"\"\" Generate Lupi Data for Classification, Regression and Ordinal Regression Problems Parameters ---------- problemName : ProblemName Defines the type of y-values of the problem. Example `ProblemName.CLASSIFICATION`. n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated. Returns ------- X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] \"\"\" random_state = check_random_state ( random_state ) n_informative = n_strel + ( n_weakrel > 0 ) # Create truth ( prototype ) vector which contains true feature contributions # We enforce minimum of 0.1 to circumvent problems when testing for relevance w = random_state . uniform ( low = 0.5 , high = 1 , size = n_informative ) X_informative = random_state . normal ( size = ( n_samples , n_informative )) scores = np . dot ( X_informative , w ) n_features = n_strel + n_weakrel + n_repeated + n_irrel X_priv = _fillVariableSpace ( X_informative , random_state , n_features = n_features , n_redundant = n_weakrel , n_strel = n_strel , n_repeated = n_repeated , partition =[ n_weakrel ] , ) if ( problemName == \"classification\" or problemName == ProblemName . LUPI_CLASSIFICATION ) : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.65 / X_priv . std () ) X = X_priv + e y = np . zeros_like ( scores ) y [ scores > 0 ] = 1 y [ scores <= 0 ] = - 1 elif problemName == \"regression\" or problemName == ProblemName . LUPI_REGRESSION : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.5 / X_priv . std () ) X = X_priv + e y = scores elif ( problemName == \"ordinalRegression\" or problemName == ProblemName . LUPI_ORDREGRESSION ) : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.2 / X_priv . std () ) X = X_priv + e step = 1 / ( n_ordinal_bins ) quantiles = [ i * step for i in range(1, n_ordinal_bins) ] bs = np . quantile ( scores , quantiles ) bs = np . append ( bs , np . inf ) scores = scores [ :, np.newaxis ] y = np . sum ( scores - bs >= 0 , - 1 ) if n_irrel > 0 : X = np . hstack ( [ X, random_state.normal(size=(n_samples, n_irrel)) ] ) X_priv = np . hstack ( [ X_priv, random_state.normal(size=(n_samples, n_irrel)) ] ) if label_noise > 0 : sample = random_state . choice ( len ( y ), int ( len ( y ) * label_noise )) y [ sample ] = random_state . permutation ( y [ sample ] ) return ( X , X_priv , y . squeeze ())","title":"Gen Lupi"},{"location":"reference/fri/toydata/gen_lupi/#module-fritoydatagen_lupi","text":"View Source import numpy as np from sklearn.utils import check_random_state from fri import ProblemName from .gen_data import _fillVariableSpace def _checkLupiParam ( problemName , lupiType , n_strel , n_weakrel , n_priv_weakrel , partition , partition_priv ): \"\"\" Checks if the parameters supplied to the genLupiData() function are okay. Parameters ---------- problemName : Str Must be one of ['classification', 'regression', 'ordinalRegression'] lupiType : Str Must be one of ['cleanLabels', 'cleanFeatures'] n_strel : int Stands for the number of strongly relevant features to generate in genLupiData() Must be greater than 0 n_weakrel : int Must be equal to the length of the partition list n_priv_weakrel : int Must be equal to the length of the partition_priv list partition : list of int The length of the list must be equal to n_weakrel partition_priv : list of int The length of the list must be equal to n_priv_weakrel \"\"\" if type ( problemName ) is not ProblemName : raise ValueError ( \"Not of Type ProblemName\" ) if lupiType not in [ \"cleanLabels\" , \"cleanFeatures\" ]: raise ValueError ( \"The lupiType parameter must be a string out of ['cleanLabels', 'cleanFeatures'].\" ) if n_strel < 1 : raise ValueError ( \"At least one strongly relevant feature is necessary (Parmeter 'n_strel' must be greater than 0).\" ) if partition is not None : if sum ( partition ) != n_weakrel : raise ValueError ( \"The sum over the entries in the partition list must be equal to the parameter 'n_weakrel'.\" ) if 0 in partition or 1 in partition : raise ValueError ( \"The entries in the partition list must be greater or equal to 2.\" ) if partition_priv is not None : if sum ( partition_priv ) != n_priv_weakrel : raise ValueError ( \"The sum over the entries in the partition_priv list must be equal to the parameter 'n_priv_weakrel'.\" ) if 0 in partition_priv or 1 in partition_priv : raise ValueError ( \"The entries in the partition_priv list must be greater or equal to 2.\" ) if lupiType == \"cleanLabels\" and n_priv_weakrel > 0 : raise ValueError ( \"The 'cleanLabels' data has only one strongly relevant feature by nature, this can be repeated ('n_priv_repeated'),\" \"or useless information can be added ('n_priv_irrel') but it can not be weakend => n_priv_weakrel hast to be 0.\" ) # def _genWeakFeatures(n_weakrel, X, random_state, partition): # \"\"\" # Generate n_weakrel features out of the strRelFeature # # Parameters # ---------- # n_weakrel : int # Number of weakly relevant feature to be generated # X : array of shape [n_samples, n_features] # Contains the data out of which the weakly relevant features are created # random_state : Random State object # Used to generate the samples # partition : list of int # Used to define how many weak features are calculated from the same strong feature # The sum of the entries in the partition list must be equal to n_weakrel # # # Returns # ---------- # X_weakrel : array of shape [n_samples, n_weakrel] # Contains the data of the generated weak relevant features # \"\"\" # # X_weakrel = np.zeros([X.shape[0], n_weakrel]) # # if partition is None: # for i in range(n_weakrel): # X_weakrel[:, i] = X[ # :, random_state.choice(X.shape[1]) # ] + random_state.normal(loc=0, scale=1, size=1) # else: # idx = 0 # for j in range(len(partition)): # X_weakrel[:, idx : idx + partition[j]] = np.tile( # X[:, random_state.choice(X.shape[1])], (partition[j], 1) # ).T + random_state.normal(loc=0, scale=1, size=partition[j]) # idx += partition[j] # # return X_weakrel # # # def _genRepeatedFeatures(n_repeated, X, random_state): # \"\"\" # Generate repeated features by picking a random existing feature out of X # # Parameters # ---------- # n_repeated : int # Number of repeated features to create # X : array of shape [n_samples, n_features] # Contains the data of which the repeated features are picked # random_state : Random State object # Used to randomly pick a feature out of X # \"\"\" # # X_repeated = np.zeros([X.shape[0], n_repeated]) # for i in range(n_repeated): # X_repeated[:, i] = X[:, random_state.choice(X.shape[1])] # # return X_repeated def genLupiData ( problemName : ProblemName , n_samples : int = 100 , random_state : object = None , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 , ): \"\"\" Generate Lupi Data for Classification, Regression and Ordinal Regression Problems Parameters ---------- problemName : ProblemName Defines the type of y-values of the problem. Example `ProblemName.CLASSIFICATION`. n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated. Returns ------- X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] \"\"\" random_state = check_random_state ( random_state ) n_informative = n_strel + ( n_weakrel > 0 ) # Create truth (prototype) vector which contains true feature contributions # We enforce minimum of 0.1 to circumvent problems when testing for relevance w = random_state . uniform ( low = 0.5 , high = 1 , size = n_informative ) X_informative = random_state . normal ( size = ( n_samples , n_informative )) scores = np . dot ( X_informative , w ) n_features = n_strel + n_weakrel + n_repeated + n_irrel X_priv = _fillVariableSpace ( X_informative , random_state , n_features = n_features , n_redundant = n_weakrel , n_strel = n_strel , n_repeated = n_repeated , partition = [ n_weakrel ], ) if ( problemName == \"classification\" or problemName == ProblemName . LUPI_CLASSIFICATION ): e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ]), scale = 0.65 / X_priv . std () ) X = X_priv + e y = np . zeros_like ( scores ) y [ scores > 0 ] = 1 y [ scores <= 0 ] = - 1 elif problemName == \"regression\" or problemName == ProblemName . LUPI_REGRESSION : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ]), scale = 0.5 / X_priv . std () ) X = X_priv + e y = scores elif ( problemName == \"ordinalRegression\" or problemName == ProblemName . LUPI_ORDREGRESSION ): e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ]), scale = 0.2 / X_priv . std () ) X = X_priv + e step = 1 / ( n_ordinal_bins ) quantiles = [ i * step for i in range ( 1 , n_ordinal_bins )] bs = np . quantile ( scores , quantiles ) bs = np . append ( bs , np . inf ) scores = scores [:, np . newaxis ] y = np . sum ( scores - bs >= 0 , - 1 ) if n_irrel > 0 : X = np . hstack ([ X , random_state . normal ( size = ( n_samples , n_irrel ))]) X_priv = np . hstack ([ X_priv , random_state . normal ( size = ( n_samples , n_irrel ))]) if label_noise > 0 : sample = random_state . choice ( len ( y ), int ( len ( y ) * label_noise )) y [ sample ] = random_state . permutation ( y [ sample ]) return ( X , X_priv , y . squeeze ())","title":"Module fri.toydata.gen_lupi"},{"location":"reference/fri/toydata/gen_lupi/#functions","text":"","title":"Functions"},{"location":"reference/fri/toydata/gen_lupi/#genlupidata","text":"def genLupiData ( problemName : fri . ProblemName , n_samples : int = 100 , random_state : object = None , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 ) Generate Lupi Data for Classification, Regression and Ordinal Regression Problems","title":"genLupiData"},{"location":"reference/fri/toydata/gen_lupi/#parameters","text":"problemName : ProblemName Defines the type of y-values of the problem. Example ProblemName.CLASSIFICATION . n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated.","title":"Parameters"},{"location":"reference/fri/toydata/gen_lupi/#returns","text":"X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] View Source def genLupiData ( problemName : ProblemName , n_samples : int = 100 , random_state : object = None , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 , ) : \"\"\" Generate Lupi Data for Classification, Regression and Ordinal Regression Problems Parameters ---------- problemName : ProblemName Defines the type of y-values of the problem. Example `ProblemName.CLASSIFICATION`. n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated. Returns ------- X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] \"\"\" random_state = check_random_state ( random_state ) n_informative = n_strel + ( n_weakrel > 0 ) # Create truth ( prototype ) vector which contains true feature contributions # We enforce minimum of 0.1 to circumvent problems when testing for relevance w = random_state . uniform ( low = 0.5 , high = 1 , size = n_informative ) X_informative = random_state . normal ( size = ( n_samples , n_informative )) scores = np . dot ( X_informative , w ) n_features = n_strel + n_weakrel + n_repeated + n_irrel X_priv = _fillVariableSpace ( X_informative , random_state , n_features = n_features , n_redundant = n_weakrel , n_strel = n_strel , n_repeated = n_repeated , partition =[ n_weakrel ] , ) if ( problemName == \"classification\" or problemName == ProblemName . LUPI_CLASSIFICATION ) : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.65 / X_priv . std () ) X = X_priv + e y = np . zeros_like ( scores ) y [ scores > 0 ] = 1 y [ scores <= 0 ] = - 1 elif problemName == \"regression\" or problemName == ProblemName . LUPI_REGRESSION : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.5 / X_priv . std () ) X = X_priv + e y = scores elif ( problemName == \"ordinalRegression\" or problemName == ProblemName . LUPI_ORDREGRESSION ) : e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ), scale = 0.2 / X_priv . std () ) X = X_priv + e step = 1 / ( n_ordinal_bins ) quantiles = [ i * step for i in range(1, n_ordinal_bins) ] bs = np . quantile ( scores , quantiles ) bs = np . append ( bs , np . inf ) scores = scores [ :, np.newaxis ] y = np . sum ( scores - bs >= 0 , - 1 ) if n_irrel > 0 : X = np . hstack ( [ X, random_state.normal(size=(n_samples, n_irrel)) ] ) X_priv = np . hstack ( [ X_priv, random_state.normal(size=(n_samples, n_irrel)) ] ) if label_noise > 0 : sample = random_state . choice ( len ( y ), int ( len ( y ) * label_noise )) y [ sample ] = random_state . permutation ( y [ sample ] ) return ( X , X_priv , y . squeeze ())","title":"Returns"}]}