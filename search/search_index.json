{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Feature Relevance Intervals - FRI This repository contains the Python implementation of the Feature Relevance Intervals method (FRI) also called Feature Relevance Bounds. Documentation Check out our online documentation at fri.lpfann.me . There you can find a quick start guide and more background information. You can also run the guide directly without setup online here . Installation FRI requires Python 3.6+ . For a stable version from PyPI use $ pip install fri To install the development version clone the repository and execute inside the project folder: $ pip install -e . Usage Please refer to the documentation for advice. Testing To test if the library was installed correctly you can use the pytest command to run all included tests. $ pip install -e . [ tests ] then simply run $ pytest References [1] G\u00f6pfert C, Pfannschmidt L, Hammer B. Feature Relevance Bounds for Linear Classification. In: Proceedings of the ESANN. 25th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/publication/2908201 [2] G\u00f6pfert C, Pfannschmidt L, G\u00f6pfert JP, Hammer B. Interpretation of Linear Classifiers by Means of Feature Relevance Bounds. Neurocomputing. https://pub.uni-bielefeld.de/publication/2915273 [3] Lukas Pfannschmidt, Jonathan Jakob, Michael Biehl, Peter Tino, Barbara Hammer: Feature Relevance Bounds for Ordinal Regression . Proceedings of the ESANN. 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; Accepted. https://pub.uni-bielefeld.de/record/2933893 [4] Pfannschmidt L, G\u00f6pfert C, Neumann U, Heider D, Hammer B: FRI - Feature Relevance Intervals for Interpretable and Interactive Data Exploration. Presented at the 16th IEEE International Conference on Computational Intelligence in Bioinformatics and Computational Biology, Certosa di Pontignano, Siena - Tuscany, Italy. https://ieeexplore.ieee.org/document/8791489","title":"Home"},{"location":"#feature-relevance-intervals-fri","text":"This repository contains the Python implementation of the Feature Relevance Intervals method (FRI) also called Feature Relevance Bounds.","title":"Feature Relevance Intervals - FRI"},{"location":"#documentation","text":"Check out our online documentation at fri.lpfann.me . There you can find a quick start guide and more background information. You can also run the guide directly without setup online here .","title":"Documentation"},{"location":"#installation","text":"FRI requires Python 3.6+ . For a stable version from PyPI use $ pip install fri To install the development version clone the repository and execute inside the project folder: $ pip install -e .","title":"Installation"},{"location":"#usage","text":"Please refer to the documentation for advice.","title":"Usage"},{"location":"#testing","text":"To test if the library was installed correctly you can use the pytest command to run all included tests. $ pip install -e . [ tests ] then simply run $ pytest","title":"Testing"},{"location":"#references","text":"[1] G\u00f6pfert C, Pfannschmidt L, Hammer B. Feature Relevance Bounds for Linear Classification. In: Proceedings of the ESANN. 25th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/publication/2908201 [2] G\u00f6pfert C, Pfannschmidt L, G\u00f6pfert JP, Hammer B. Interpretation of Linear Classifiers by Means of Feature Relevance Bounds. Neurocomputing. https://pub.uni-bielefeld.de/publication/2915273 [3] Lukas Pfannschmidt, Jonathan Jakob, Michael Biehl, Peter Tino, Barbara Hammer: Feature Relevance Bounds for Ordinal Regression . Proceedings of the ESANN. 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; Accepted. https://pub.uni-bielefeld.de/record/2933893 [4] Pfannschmidt L, G\u00f6pfert C, Neumann U, Heider D, Hammer B: FRI - Feature Relevance Intervals for Interpretable and Interactive Data Exploration. Presented at the 16th IEEE International Conference on Computational Intelligence in Bioinformatics and Computational Biology, Certosa di Pontignano, Siena - Tuscany, Italy. https://ieeexplore.ieee.org/document/8791489","title":"References"},{"location":"docs/","text":"Overview [FRI]{.title-ref} is a Python 3 package for analytical feature selection purposes. It allows superior feature selection in the sense that all important features are conserved. At the moment we support multiple linear models for solving Classification, Regression and Ordinal Regression Problems. We also support LUPI paradigm where at learning time, privileged information is available. Check out the Quick Start Guide </notebooks/Guide.ipynb> {.interpreted-text role=\"ref\"} for a practical introduction. For theoretical explanations look at the background or read the references for the long version. The source code can be found at GitHub . If you have any problems or suggestions open an issue! Contents ::: {.toctree maxdepth=\"2\"} notebooks/Guide.ipynb background license api/user test GitHub \\< https://github.com/lpfann/fri > ::: Installation FRI is available on PyPI and can be installed via pip : pip install fri All dependencies should be installed automatically if not already present. Indices and tables genindex modindex search","title":"Index"},{"location":"docs/#overview","text":"[FRI]{.title-ref} is a Python 3 package for analytical feature selection purposes. It allows superior feature selection in the sense that all important features are conserved. At the moment we support multiple linear models for solving Classification, Regression and Ordinal Regression Problems. We also support LUPI paradigm where at learning time, privileged information is available. Check out the Quick Start Guide </notebooks/Guide.ipynb> {.interpreted-text role=\"ref\"} for a practical introduction. For theoretical explanations look at the background or read the references for the long version. The source code can be found at GitHub . If you have any problems or suggestions open an issue!","title":"Overview"},{"location":"docs/#contents","text":"::: {.toctree maxdepth=\"2\"} notebooks/Guide.ipynb background license api/user test GitHub \\< https://github.com/lpfann/fri > :::","title":"Contents"},{"location":"docs/#installation","text":"FRI is available on PyPI and can be installed via pip : pip install fri All dependencies should be installed automatically if not already present.","title":"Installation"},{"location":"docs/#indices-and-tables","text":"genindex modindex search","title":"Indices and tables"},{"location":"docs/Guide/","text":"Quick start guide Installation Stable Fri can be installed via the Python Package Index (PyPI). If you have pip installed just execute the command pip install fri to get the newest stable version. The dependencies should be installed and checked automatically. If you have problems installing please open issue at our tracker . Development To install a bleeding edge dev version of FRI you can clone the GitHub repository using git clone git @github . com : lpfann / fri . git and then check out the dev branch: git checkout dev . To check if everything works as intented you can use pytest to run the unit tests. Just run the command pytest in the main project folder # For the purpose of viewing this notebook online we install the library directly with pip ! pip install fri Requirement already satisfied : fri in / home / lpfannschmidt / Dropbox / Promotion / prj / om_estimator ( 6 . 0 . 0 + 160 . g95126ec ) Requirement already satisfied : numpy in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from fri ) ( 1 . 16 . 4 ) Requirement already satisfied : scipy >= 1 . 0 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from fri ) ( 1 . 3 . 0 ) Requirement already satisfied : scikit - learn >= 0 . 21 . 1 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from fri ) ( 0 . 21 . 3 ) Requirement already satisfied : joblib >= 0 . 13 . 2 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from fri ) ( 0 . 13 . 2 ) Requirement already satisfied : cvxpy >= 1 . 0 . 21 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from fri ) ( 1 . 0 . 24 ) Requirement already satisfied : ecos >= 2 . 0 . 5 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from fri ) ( 2 . 0 . 7 . post1 ) Requirement already satisfied : matplotlib in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from fri ) ( 3 . 1 . 1 ) Requirement already satisfied : multiprocess in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from cvxpy >= 1 . 0 . 21 -> fri ) ( 0 . 70 . 8 ) Requirement already satisfied : six in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from cvxpy >= 1 . 0 . 21 -> fri ) ( 1 . 12 . 0 ) Requirement already satisfied : scs >= 1 . 1 . 3 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from cvxpy >= 1 . 0 . 21 -> fri ) ( 2 . 1 . 1 . post2 ) Requirement already satisfied : osqp >= 0 . 4 . 1 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from cvxpy >= 1 . 0 . 21 -> fri ) ( 0 . 5 . 0 ) Requirement already satisfied : python - dateutil >= 2 . 1 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from matplotlib -> fri ) ( 2 . 8 . 0 ) Requirement already satisfied : pyparsing != 2 . 0 . 4 , != 2 . 1 . 2 , != 2 . 1 . 6 , >= 2 . 0 . 1 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from matplotlib -> fri ) ( 2 . 4 . 2 ) Requirement already satisfied : cycler >= 0 . 10 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from matplotlib -> fri ) ( 0 . 10 . 0 ) Requirement already satisfied : kiwisolver >= 1 . 0 . 1 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from matplotlib -> fri ) ( 1 . 1 . 0 ) Requirement already satisfied : dill >= 0 . 3 . 0 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from multiprocess -> cvxpy >= 1 . 0 . 21 -> fri ) ( 0 . 3 . 0 ) Requirement already satisfied : future in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from osqp >= 0 . 4 . 1 -> cvxpy >= 1 . 0 . 21 -> fri ) ( 0 . 17 . 1 ) Requirement already satisfied : setuptools in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from kiwisolver >= 1 . 0 . 1 -> matplotlib -> fri ) ( 41 . 0 . 1 ) Using FRI Now we showcase the workflow of using FRI on a simple classification problem. Data To have something to work with, we need some data first. fri includes a generation method for binary classification and regression data. In our case we need some classification data. from fri import genClassificationData We want to create a small set with a few features. Because we want to showcase the all-relevant feature selection, we generate multiple strongly and weakly relevant features. n = 100 features = 6 strongly_relevant = 2 weakly_relevant = 2 X , y = genClassificationData ( n_samples = n , n_features = features , n_strel = strongly_relevant , n_redundant = weakly_relevant , random_state = 123 ) The method also prints out the parameters again. X . shape ( 100 , 6 ) We created a binary classification set with 6 features of which 2 are strongly relevant and 2 weakly relevant. Preprocess Because our method expects mean centered data we need to standardize it first. This centers the values around 0 and deviation to the standard deviation from sklearn.preprocessing import StandardScaler X_scaled = StandardScaler () . fit_transform ( X ) Model Now we need to creata a Model. We use the FRI module. import fri fri provides a convenience class fri.FRI to create a model. fri.FRI needs the type of problem as a first argument of type ProblemName . Depending on the Problem you want to analyze pick from one of the available models in ProblemName . list ( fri . ProblemName ) [ < ProblemName . CLASSIFICATION : < class 'fri.model.classification.Classification' >> , < ProblemName . REGRESSION : < class 'fri.model.regression.Regression' >> , < ProblemName . ORDINALREGRESSION : < class 'fri.model.ordinal_regression.OrdinalRegression' >> , < ProblemName . LUPI_CLASSIFICATION : < class 'fri.model.lupi_classification.LUPI_Classification' >> , < ProblemName . LUPI_REGRESSION : < class 'fri.model.lupi_regression.LUPI_Regression' >> , < ProblemName . LUPI_ORDREGRESSION : < class 'fri.model.lupi_ordinal_regression_exp.LUPI_OrdinalRegression' >> ] Because we have Classification data we use the ProblemName.CLASSIFICATION to instantiate our model. fri_model = fri . FRI ( fri . ProblemName . CLASSIFICATION , slack_loss = 0.001 , slack_regularization = 0.001 ) fri_model FRI ( n_jobs = 1 , n_param_search = 50 , n_probe_features = 80 , normalize = True , problemName = None , random_state =< mtrand . RandomState object at 0 x7f2f94384288 > , slack_loss = None , slack_regularization = None , verbose = 0 ) We used no parameters for creation so the defaults are active. Fitting to data Now we can just fit the model to the data using scikit-learn like commands. fri_model . fit ( X_scaled , y ) FRI ( n_jobs = 1 , n_param_search = 50 , n_probe_features = 80 , normalize = True , problemName = None , random_state =< mtrand . RandomState object at 0 x7f2f94384288 > , slack_loss = None , slack_regularization = None , verbose = 0 ) The resulting feature relevance bounds are saved in the interval_ variable. fri_model . interval_ array ([[ 0 . 34388504 , 0 . 35158714 ], [ 0 . 3087857 , 0 . 31784745 ], [ 0 . 23366248 , 0 . 34163576 ], [ 0 . , 0 . 10814646 ], [ 0 . , 0 . 00287764 ], [ 0 . , 0 . 0025781 ]]) If you want to print out the relevance class use the print_interval_with_class() function. print ( fri_model . print_interval_with_class ()) ############## Relevance bounds ############## feature : [ LB -- UB], relevance class 0 : [ 0 . 3 -- 0.4], Strong relevant 1 : [ 0 . 3 -- 0.3], Strong relevant 2 : [ 0 . 2 -- 0.3], Strong relevant 3 : [ 0 . 0 -- 0.1], Irrelevant 4 : [ 0 . 0 -- 0.0], Irrelevant 5 : [ 0 . 0 -- 0.0], Irrelevant The bounds are grouped in 2d sublists for each feature. To acess the relevance bounds for feature 2 we would use fri_model . interval_ [ 2 ] array ([ 0 . 23366248 , 0 . 34163576 ]) The relevance classes are saved in the corresponding variable relevance_classes_ : fri_model . relevance_classes_ array ([ 2 , 2 , 2 , 0 , 0 , 0 ]) 2 denotes strongly relevant features, 1 weakly relevant and 0 irrelevant. Plot results The bounds in numerical form are useful for postprocesing. If we want a human to look at it, we recommend the plot function plot_relevance_bars . We can also color the bars according to relevance_classes_ # Import plot function from fri.plot import plot_relevance_bars import matplotlib.pyplot as plt % matplotlib inline # Create new figure, where we can put an axis on fig , ax = plt . subplots ( 1 , 1 , figsize = ( 6 , 3 )) # plot the bars on the axis, colored according to fri out = plot_relevance_bars ( ax , fri_model . interval_ , classes = fri_model . relevance_classes_ ) Print internal Parameters If we want to take at internal parameters, we can use the verbose flag in the model creation. fri_model = fri . FRI ( fri . ProblemName . CLASSIFICATION , verbose = True ) fri_model . fit ( X_scaled , y ) Fitting 3 folds for each of 50 candidates , totalling 150 fits [ Parallel ( n_jobs = 1 ) ]: Using backend SequentialBackend with 1 concurrent workers . [ Parallel ( n_jobs = 1 ) ]: Done 150 out of 150 | elapsed : 0 . 9 s finished [ Parallel ( n_jobs = 1 ) ]: Using backend SequentialBackend with 1 concurrent workers . [ Parallel ( n_jobs = 1 ) ]: Done 18 out of 18 | elapsed : 0 . 2 s finished [ Parallel ( n_jobs = 1 ) ]: Using backend SequentialBackend with 1 concurrent workers . ******************** Best found baseline model ******************** Classification_SVM ( C = 2 . 252620785394559 ) score : 0 . 98 ' loss: 3.912340912240545 ' ' w_l1: 10.701925716627462 ' ' w: shape (6,) ' ' b: shape () ' ' slack: shape (100,) ' ****************************** [ Parallel ( n_jobs = 1 ) ]: Done 160 out of 160 | elapsed : 1 . 5 s finished [ Parallel ( n_jobs = 1 ) ]: Using backend SequentialBackend with 1 concurrent workers . FS threshold : - 0 . 020384605044528673 - 0 . 024502494977920716 , Mean : 0 . 002058944966696022 , Std : 0 . 005444840132729435 , n_probes 43 FS threshold : - 0 . 03633035235181863 - 0 . 14343569308400425 , Mean : 0 . 05355267036609281 , Std : 0 . 021690778769127776 , n_probes 41 [ Parallel ( n_jobs = 1 ) ]: Done 80 out of 80 | elapsed : 0 . 8 s finished FRI ( n_jobs = 1 , n_param_search = 50 , n_probe_features = 80 , normalize = True , problemName = None , random_state =< mtrand . RandomState object at 0 x7f2f94384288 > , slack_loss = None , slack_regularization = None , verbose = True ) This prints out the parameters of the baseline model One can also see the best selected hyperparameter according to gridsearch and the training score of the model in score . Setting constraints manually Our model also allows to compute relevance bounds when the user sets a given range for the features. We use a dictionary to encode our constraints. preset = {} Example As an example, let us constrain the third from our example to the minimum relevance bound. preset [ 2 ] = fri_model . interval_ [ 2 , 0 ] We use the function constrained_intervals . Note: we need to fit the model before we can use this function. We already did that, so we are fine. const_ints = fri_model . constrained_intervals ( preset = preset ) --------------------------------------------------------------------------- IndexError Traceback ( most recent call last ) < ipython - input - 25 - b9f69bfa650a > in < module > ----> 1 const_ints = fri_model . constrained_intervals ( preset = preset ) ~/ Dropbox / Promotion / prj / om_estimator / fri / main . py in constrained_intervals ( self , preset ) 215 216 return self . _relevance_bounds_computer . compute_multi_preset_relevance_bounds ( preset = preset , --> 217 lupi_features = self . lupi_features_ ) 218 219 def print_interval_with_class ( self ) : ~/ Dropbox / Promotion / prj / om_estimator / fri / compute . py in compute_multi_preset_relevance_bounds ( self , preset , lupi_features ) 256 257 # Add sign to presets --> 258 preset = self . _add_sign_to_preset ( preset ) 259 260 # Calculate all bounds with feature i set to min_i ~/ Dropbox / Promotion / prj / om_estimator / fri / compute . py in _add_sign_to_preset ( self , unsigned_presets ) 287 unsigned_preset_i = np . sign ( w [ i ] ) * preset 288 # accumulate maximal feature contribution --> 289 sum += unsigned_preset_i [ 1 ] 290 signed_presets [ i ] = unsigned_preset_i 291 IndexError : invalid index to scalar variable . constrained_interval --------------------------------------------------------------------------- NameError Traceback ( most recent call last ) < ipython - input - 26 - 4 ecb5fc7bf2e > in < module > ----> 1 constrained_interval NameError : name 'constrained_interval' is not defined Feature 3 is set to its minimum (at 0). How does it look visually? fig , ax = plt . subplots ( 1 , 1 , figsize = ( 6 , 3 )) out = plot_relevance_bars ( ax , constrained_interval ) --------------------------------------------------------------------------- NameError Traceback ( most recent call last ) < ipython - input - 53 - 3 bd61fa3c5a6 > in < module > 1 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 6 , 3 )) ----> 2 out = plot_relevance_bars(ax, constrained_interval) NameError : name 'constrained_interval' is not defined Feature 3 is reduced to its minimum (no contribution). In turn, its correlated partner feature 4 had to take its maximum contribution.","title":"Guide"},{"location":"docs/Guide/#quick-start-guide","text":"","title":"Quick start guide"},{"location":"docs/Guide/#installation","text":"","title":"Installation"},{"location":"docs/Guide/#stable","text":"Fri can be installed via the Python Package Index (PyPI). If you have pip installed just execute the command pip install fri to get the newest stable version. The dependencies should be installed and checked automatically. If you have problems installing please open issue at our tracker .","title":"Stable"},{"location":"docs/Guide/#development","text":"To install a bleeding edge dev version of FRI you can clone the GitHub repository using git clone git @github . com : lpfann / fri . git and then check out the dev branch: git checkout dev . To check if everything works as intented you can use pytest to run the unit tests. Just run the command pytest in the main project folder # For the purpose of viewing this notebook online we install the library directly with pip ! pip install fri Requirement already satisfied : fri in / home / lpfannschmidt / Dropbox / Promotion / prj / om_estimator ( 6 . 0 . 0 + 160 . g95126ec ) Requirement already satisfied : numpy in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from fri ) ( 1 . 16 . 4 ) Requirement already satisfied : scipy >= 1 . 0 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from fri ) ( 1 . 3 . 0 ) Requirement already satisfied : scikit - learn >= 0 . 21 . 1 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from fri ) ( 0 . 21 . 3 ) Requirement already satisfied : joblib >= 0 . 13 . 2 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from fri ) ( 0 . 13 . 2 ) Requirement already satisfied : cvxpy >= 1 . 0 . 21 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from fri ) ( 1 . 0 . 24 ) Requirement already satisfied : ecos >= 2 . 0 . 5 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from fri ) ( 2 . 0 . 7 . post1 ) Requirement already satisfied : matplotlib in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from fri ) ( 3 . 1 . 1 ) Requirement already satisfied : multiprocess in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from cvxpy >= 1 . 0 . 21 -> fri ) ( 0 . 70 . 8 ) Requirement already satisfied : six in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from cvxpy >= 1 . 0 . 21 -> fri ) ( 1 . 12 . 0 ) Requirement already satisfied : scs >= 1 . 1 . 3 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from cvxpy >= 1 . 0 . 21 -> fri ) ( 2 . 1 . 1 . post2 ) Requirement already satisfied : osqp >= 0 . 4 . 1 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from cvxpy >= 1 . 0 . 21 -> fri ) ( 0 . 5 . 0 ) Requirement already satisfied : python - dateutil >= 2 . 1 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from matplotlib -> fri ) ( 2 . 8 . 0 ) Requirement already satisfied : pyparsing != 2 . 0 . 4 , != 2 . 1 . 2 , != 2 . 1 . 6 , >= 2 . 0 . 1 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from matplotlib -> fri ) ( 2 . 4 . 2 ) Requirement already satisfied : cycler >= 0 . 10 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from matplotlib -> fri ) ( 0 . 10 . 0 ) Requirement already satisfied : kiwisolver >= 1 . 0 . 1 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from matplotlib -> fri ) ( 1 . 1 . 0 ) Requirement already satisfied : dill >= 0 . 3 . 0 in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from multiprocess -> cvxpy >= 1 . 0 . 21 -> fri ) ( 0 . 3 . 0 ) Requirement already satisfied : future in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from osqp >= 0 . 4 . 1 -> cvxpy >= 1 . 0 . 21 -> fri ) ( 0 . 17 . 1 ) Requirement already satisfied : setuptools in / home / lpfannschmidt / anaconda3 / lib / python3 . 7 / site - packages ( from kiwisolver >= 1 . 0 . 1 -> matplotlib -> fri ) ( 41 . 0 . 1 )","title":"Development"},{"location":"docs/Guide/#using-fri","text":"Now we showcase the workflow of using FRI on a simple classification problem.","title":"Using FRI"},{"location":"docs/Guide/#data","text":"To have something to work with, we need some data first. fri includes a generation method for binary classification and regression data. In our case we need some classification data. from fri import genClassificationData We want to create a small set with a few features. Because we want to showcase the all-relevant feature selection, we generate multiple strongly and weakly relevant features. n = 100 features = 6 strongly_relevant = 2 weakly_relevant = 2 X , y = genClassificationData ( n_samples = n , n_features = features , n_strel = strongly_relevant , n_redundant = weakly_relevant , random_state = 123 ) The method also prints out the parameters again. X . shape ( 100 , 6 ) We created a binary classification set with 6 features of which 2 are strongly relevant and 2 weakly relevant.","title":"Data"},{"location":"docs/Guide/#preprocess","text":"Because our method expects mean centered data we need to standardize it first. This centers the values around 0 and deviation to the standard deviation from sklearn.preprocessing import StandardScaler X_scaled = StandardScaler () . fit_transform ( X )","title":"Preprocess"},{"location":"docs/Guide/#model","text":"Now we need to creata a Model. We use the FRI module. import fri fri provides a convenience class fri.FRI to create a model. fri.FRI needs the type of problem as a first argument of type ProblemName . Depending on the Problem you want to analyze pick from one of the available models in ProblemName . list ( fri . ProblemName ) [ < ProblemName . CLASSIFICATION : < class 'fri.model.classification.Classification' >> , < ProblemName . REGRESSION : < class 'fri.model.regression.Regression' >> , < ProblemName . ORDINALREGRESSION : < class 'fri.model.ordinal_regression.OrdinalRegression' >> , < ProblemName . LUPI_CLASSIFICATION : < class 'fri.model.lupi_classification.LUPI_Classification' >> , < ProblemName . LUPI_REGRESSION : < class 'fri.model.lupi_regression.LUPI_Regression' >> , < ProblemName . LUPI_ORDREGRESSION : < class 'fri.model.lupi_ordinal_regression_exp.LUPI_OrdinalRegression' >> ] Because we have Classification data we use the ProblemName.CLASSIFICATION to instantiate our model. fri_model = fri . FRI ( fri . ProblemName . CLASSIFICATION , slack_loss = 0.001 , slack_regularization = 0.001 ) fri_model FRI ( n_jobs = 1 , n_param_search = 50 , n_probe_features = 80 , normalize = True , problemName = None , random_state =< mtrand . RandomState object at 0 x7f2f94384288 > , slack_loss = None , slack_regularization = None , verbose = 0 ) We used no parameters for creation so the defaults are active.","title":"Model"},{"location":"docs/Guide/#fitting-to-data","text":"Now we can just fit the model to the data using scikit-learn like commands. fri_model . fit ( X_scaled , y ) FRI ( n_jobs = 1 , n_param_search = 50 , n_probe_features = 80 , normalize = True , problemName = None , random_state =< mtrand . RandomState object at 0 x7f2f94384288 > , slack_loss = None , slack_regularization = None , verbose = 0 ) The resulting feature relevance bounds are saved in the interval_ variable. fri_model . interval_ array ([[ 0 . 34388504 , 0 . 35158714 ], [ 0 . 3087857 , 0 . 31784745 ], [ 0 . 23366248 , 0 . 34163576 ], [ 0 . , 0 . 10814646 ], [ 0 . , 0 . 00287764 ], [ 0 . , 0 . 0025781 ]]) If you want to print out the relevance class use the print_interval_with_class() function. print ( fri_model . print_interval_with_class ()) ############## Relevance bounds ############## feature : [ LB -- UB], relevance class 0 : [ 0 . 3 -- 0.4], Strong relevant 1 : [ 0 . 3 -- 0.3], Strong relevant 2 : [ 0 . 2 -- 0.3], Strong relevant 3 : [ 0 . 0 -- 0.1], Irrelevant 4 : [ 0 . 0 -- 0.0], Irrelevant 5 : [ 0 . 0 -- 0.0], Irrelevant The bounds are grouped in 2d sublists for each feature. To acess the relevance bounds for feature 2 we would use fri_model . interval_ [ 2 ] array ([ 0 . 23366248 , 0 . 34163576 ]) The relevance classes are saved in the corresponding variable relevance_classes_ : fri_model . relevance_classes_ array ([ 2 , 2 , 2 , 0 , 0 , 0 ]) 2 denotes strongly relevant features, 1 weakly relevant and 0 irrelevant.","title":"Fitting to data"},{"location":"docs/Guide/#plot-results","text":"The bounds in numerical form are useful for postprocesing. If we want a human to look at it, we recommend the plot function plot_relevance_bars . We can also color the bars according to relevance_classes_ # Import plot function from fri.plot import plot_relevance_bars import matplotlib.pyplot as plt % matplotlib inline # Create new figure, where we can put an axis on fig , ax = plt . subplots ( 1 , 1 , figsize = ( 6 , 3 )) # plot the bars on the axis, colored according to fri out = plot_relevance_bars ( ax , fri_model . interval_ , classes = fri_model . relevance_classes_ )","title":"Plot results"},{"location":"docs/Guide/#print-internal-parameters","text":"If we want to take at internal parameters, we can use the verbose flag in the model creation. fri_model = fri . FRI ( fri . ProblemName . CLASSIFICATION , verbose = True ) fri_model . fit ( X_scaled , y ) Fitting 3 folds for each of 50 candidates , totalling 150 fits [ Parallel ( n_jobs = 1 ) ]: Using backend SequentialBackend with 1 concurrent workers . [ Parallel ( n_jobs = 1 ) ]: Done 150 out of 150 | elapsed : 0 . 9 s finished [ Parallel ( n_jobs = 1 ) ]: Using backend SequentialBackend with 1 concurrent workers . [ Parallel ( n_jobs = 1 ) ]: Done 18 out of 18 | elapsed : 0 . 2 s finished [ Parallel ( n_jobs = 1 ) ]: Using backend SequentialBackend with 1 concurrent workers . ******************** Best found baseline model ******************** Classification_SVM ( C = 2 . 252620785394559 ) score : 0 . 98 ' loss: 3.912340912240545 ' ' w_l1: 10.701925716627462 ' ' w: shape (6,) ' ' b: shape () ' ' slack: shape (100,) ' ****************************** [ Parallel ( n_jobs = 1 ) ]: Done 160 out of 160 | elapsed : 1 . 5 s finished [ Parallel ( n_jobs = 1 ) ]: Using backend SequentialBackend with 1 concurrent workers . FS threshold : - 0 . 020384605044528673 - 0 . 024502494977920716 , Mean : 0 . 002058944966696022 , Std : 0 . 005444840132729435 , n_probes 43 FS threshold : - 0 . 03633035235181863 - 0 . 14343569308400425 , Mean : 0 . 05355267036609281 , Std : 0 . 021690778769127776 , n_probes 41 [ Parallel ( n_jobs = 1 ) ]: Done 80 out of 80 | elapsed : 0 . 8 s finished FRI ( n_jobs = 1 , n_param_search = 50 , n_probe_features = 80 , normalize = True , problemName = None , random_state =< mtrand . RandomState object at 0 x7f2f94384288 > , slack_loss = None , slack_regularization = None , verbose = True ) This prints out the parameters of the baseline model One can also see the best selected hyperparameter according to gridsearch and the training score of the model in score .","title":"Print internal Parameters"},{"location":"docs/Guide/#setting-constraints-manually","text":"Our model also allows to compute relevance bounds when the user sets a given range for the features. We use a dictionary to encode our constraints. preset = {}","title":"Setting constraints manually"},{"location":"docs/Guide/#example","text":"As an example, let us constrain the third from our example to the minimum relevance bound. preset [ 2 ] = fri_model . interval_ [ 2 , 0 ] We use the function constrained_intervals . Note: we need to fit the model before we can use this function. We already did that, so we are fine. const_ints = fri_model . constrained_intervals ( preset = preset ) --------------------------------------------------------------------------- IndexError Traceback ( most recent call last ) < ipython - input - 25 - b9f69bfa650a > in < module > ----> 1 const_ints = fri_model . constrained_intervals ( preset = preset ) ~/ Dropbox / Promotion / prj / om_estimator / fri / main . py in constrained_intervals ( self , preset ) 215 216 return self . _relevance_bounds_computer . compute_multi_preset_relevance_bounds ( preset = preset , --> 217 lupi_features = self . lupi_features_ ) 218 219 def print_interval_with_class ( self ) : ~/ Dropbox / Promotion / prj / om_estimator / fri / compute . py in compute_multi_preset_relevance_bounds ( self , preset , lupi_features ) 256 257 # Add sign to presets --> 258 preset = self . _add_sign_to_preset ( preset ) 259 260 # Calculate all bounds with feature i set to min_i ~/ Dropbox / Promotion / prj / om_estimator / fri / compute . py in _add_sign_to_preset ( self , unsigned_presets ) 287 unsigned_preset_i = np . sign ( w [ i ] ) * preset 288 # accumulate maximal feature contribution --> 289 sum += unsigned_preset_i [ 1 ] 290 signed_presets [ i ] = unsigned_preset_i 291 IndexError : invalid index to scalar variable . constrained_interval --------------------------------------------------------------------------- NameError Traceback ( most recent call last ) < ipython - input - 26 - 4 ecb5fc7bf2e > in < module > ----> 1 constrained_interval NameError : name 'constrained_interval' is not defined Feature 3 is set to its minimum (at 0). How does it look visually? fig , ax = plt . subplots ( 1 , 1 , figsize = ( 6 , 3 )) out = plot_relevance_bars ( ax , constrained_interval ) --------------------------------------------------------------------------- NameError Traceback ( most recent call last ) < ipython - input - 53 - 3 bd61fa3c5a6 > in < module > 1 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 6 , 3 )) ----> 2 out = plot_relevance_bars(ax, constrained_interval) NameError : name 'constrained_interval' is not defined Feature 3 is reduced to its minimum (no contribution). In turn, its correlated partner feature 4 had to take its maximum contribution.","title":"Example"},{"location":"docs/background/","text":"Background ::: {.note} ::: {.admonition-title} Note ::: We presented [FRI]{.title-ref} at the CIBCB conference. Check out the slides <https://lpfann.me/talk/cibc19/talk.pdf> {.interpreted-text role=\"download\"} for a short primer into how it works. ::: Feature selection is the task of finding relevant features used in a machine learning model. Often used for this task are models which produce a sparse subset of all input features by permitting the use of additional features (e.g. Lasso with L1 regularization). But these models are often tuned to filter out redundancies in the input set and produce only an unstable solution especially in the presence of higher dimensional data. FRI calculates relevance bound values for all input features. These bounds give rise to intervals which we named \\'feature relevance intervals\\' (FRI). A given interval symbolizes the allowed contribution each feature has, when it is allowed to be maximized and minimized independently from the others. This allows us to approximate the global solution instead of relying on the local solutions of the alternatives. With these we can classify features into three classes: Strongly relevant : features which are crucial for model performance Weakly relevant : features which are important but can be substituted by another weakly relevant feature Irrelevant : features which have no association with the target variable References G\u00f6pfert C, Pfannschmidt L, Hammer B: Feature Relevance Bounds for Linear Classification. In: Proceedings of the ESANN. 25th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/publication/2908201 G\u00f6pfert C, Pfannschmidt L, G\u00f6pfert JP, Hammer B: Interpretation of Linear Classifiers by Means of Feature Relevance Bounds. Neurocomputing. https://pub.uni-bielefeld.de/publication/2915273 Pfannschmidt L, Jakob J, Biehl M, Tino P, Hammer B: Feature Relevance Bounds for Ordinal Regression . Proceedings of the ESANN. 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/record/2933893 Pfannschmidt L, G\u00f6pfert C, Neumann U, Heider D, Hammer B: FRI - Feature Relevance Intervals for Interpretable and Interactive Data Exploration. Presented at the 16th IEEE International Conference on Computational Intelligence in Bioinformatics and Computational Biology, Certosa di Pontignano, Siena - Tuscany, Italy. https://ieeexplore.ieee.org/document/8791489","title":"Background"},{"location":"docs/background/#background","text":"::: {.note} ::: {.admonition-title} Note ::: We presented [FRI]{.title-ref} at the CIBCB conference. Check out the slides <https://lpfann.me/talk/cibc19/talk.pdf> {.interpreted-text role=\"download\"} for a short primer into how it works. ::: Feature selection is the task of finding relevant features used in a machine learning model. Often used for this task are models which produce a sparse subset of all input features by permitting the use of additional features (e.g. Lasso with L1 regularization). But these models are often tuned to filter out redundancies in the input set and produce only an unstable solution especially in the presence of higher dimensional data. FRI calculates relevance bound values for all input features. These bounds give rise to intervals which we named \\'feature relevance intervals\\' (FRI). A given interval symbolizes the allowed contribution each feature has, when it is allowed to be maximized and minimized independently from the others. This allows us to approximate the global solution instead of relying on the local solutions of the alternatives. With these we can classify features into three classes: Strongly relevant : features which are crucial for model performance Weakly relevant : features which are important but can be substituted by another weakly relevant feature Irrelevant : features which have no association with the target variable","title":"Background"},{"location":"docs/background/#references","text":"G\u00f6pfert C, Pfannschmidt L, Hammer B: Feature Relevance Bounds for Linear Classification. In: Proceedings of the ESANN. 25th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/publication/2908201 G\u00f6pfert C, Pfannschmidt L, G\u00f6pfert JP, Hammer B: Interpretation of Linear Classifiers by Means of Feature Relevance Bounds. Neurocomputing. https://pub.uni-bielefeld.de/publication/2915273 Pfannschmidt L, Jakob J, Biehl M, Tino P, Hammer B: Feature Relevance Bounds for Ordinal Regression . Proceedings of the ESANN. 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/record/2933893 Pfannschmidt L, G\u00f6pfert C, Neumann U, Heider D, Hammer B: FRI - Feature Relevance Intervals for Interpretable and Interactive Data Exploration. Presented at the 16th IEEE International Conference on Computational Intelligence in Bioinformatics and Computational Biology, Certosa di Pontignano, Siena - Tuscany, Italy. https://ieeexplore.ieee.org/document/8791489","title":"References"},{"location":"docs/references/","text":"References G\u00f6pfert C, Pfannschmidt L, Hammer B: Feature Relevance Bounds for Linear Classification. In: Proceedings of the ESANN. 25th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/publication/2908201 G\u00f6pfert C, Pfannschmidt L, G\u00f6pfert JP, Hammer B: Interpretation of Linear Classifiers by Means of Feature Relevance Bounds. Neurocomputing. https://pub.uni-bielefeld.de/publication/2915273 Pfannschmidt L, Jakob J, Biehl M, Tino P, Hammer B: Feature Relevance Bounds for Ordinal Regression . Proceedings of the ESANN. 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/record/2933893 Pfannschmidt L, G\u00f6pfert C, Neumann U, Heider D, Hammer B: FRI - Feature Relevance Intervals for Interpretable and Interactive Data Exploration. Presented at the 16th IEEE International Conference on Computational Intelligence in Bioinformatics and Computational Biology, Certosa di Pontignano, Siena - Tuscany, Italy. https://ieeexplore.ieee.org/document/8791489","title":"References"},{"location":"docs/references/#references","text":"G\u00f6pfert C, Pfannschmidt L, Hammer B: Feature Relevance Bounds for Linear Classification. In: Proceedings of the ESANN. 25th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/publication/2908201 G\u00f6pfert C, Pfannschmidt L, G\u00f6pfert JP, Hammer B: Interpretation of Linear Classifiers by Means of Feature Relevance Bounds. Neurocomputing. https://pub.uni-bielefeld.de/publication/2915273 Pfannschmidt L, Jakob J, Biehl M, Tino P, Hammer B: Feature Relevance Bounds for Ordinal Regression . Proceedings of the ESANN. 27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning; https://pub.uni-bielefeld.de/record/2933893 Pfannschmidt L, G\u00f6pfert C, Neumann U, Heider D, Hammer B: FRI - Feature Relevance Intervals for Interpretable and Interactive Data Exploration. Presented at the 16th IEEE International Conference on Computational Intelligence in Bioinformatics and Computational Biology, Certosa di Pontignano, Siena - Tuscany, Italy. https://ieeexplore.ieee.org/document/8791489","title":"References"},{"location":"reference/fri/","text":"Module fri View Source import logging # noinspection PyUnresolvedReferences from fri._version import __version__ logging . basicConfig ( level = logging . INFO ) from enum import Enum import fri.model class ProblemName ( Enum ): \"\"\" Enum which contains usable models for which feature relevance intervals can be computed in :func:`~FRI`. \"\"\" CLASSIFICATION = fri . model . Classification REGRESSION = fri . model . Regression ORDINALREGRESSION = fri . model . OrdinalRegression LUPI_CLASSIFICATION = fri . model . LUPI_Classification LUPI_REGRESSION = fri . model . LUPI_Regression LUPI_ORDREGRESSION = fri . model . LUPI_OrdinalRegression NORMAL_MODELS = [ ProblemName . CLASSIFICATION , ProblemName . REGRESSION , ProblemName . ORDINALREGRESSION , ] LUPI_MODELS = [ ProblemName . LUPI_CLASSIFICATION , ProblemName . LUPI_REGRESSION , ProblemName . LUPI_ORDREGRESSION , ] from fri.toydata import ( genRegressionData , genClassificationData , genOrdinalRegressionData , quick_generate , genLupiData , ) from fri.main import FRIBase from fri.plot import plot_intervals class FRI ( FRIBase ): def __init__ ( self , problemName : object , random_state : object = None , n_jobs : object = 1 , verbose : object = 0 , n_param_search : object = 10 , n_probe_features : object = 20 , slack_regularization : object = 0.001 , slack_loss : object = 0.001 , normalize : object = True , ** kwargs , ): \"\"\" Main class to use `FRI` in programattic fashion following the scikit-learn paradigm. Parameters ---------- problemName: `ProblemName` or str Type of Problem as enum value or explicit string (e.g. \"classification\") random_state: object or int Random state object or int n_jobs: int or None Number of threads or -1 for automatic. verbose: int Verbosity if > 0 n_param_search: int Number of parameter samples in random search for hyperparameters. n_probe_features: int Number of probes to generate to improve feature selection. slack_regularization: float Allow deviation from optimal L1 norm. slack_loss: float Allow deviation of loss. normalize: boolean Normalize relevace bounds to range of [0,1] depending on L1 norm. \"\"\" if isinstance ( problemName , ProblemName ): problemtype = problemName . value else : if problemName == \"classification\" or problemName == \"class\" : problemtype = ProblemName . CLASSIFICATION elif problemName == \"regression\" or problemName == \"reg\" : problemtype = ProblemName . REGRESSION elif problemName == \"ordinalregression\" or problemName == \"ordreg\" : problemtype = ProblemName . ORDINALREGRESSION elif problemName == \"lupi_classification\" or problemName == \"lupi_class\" : problemtype = ProblemName . LUPI_CLASSIFICATION if problemtype is None : names = [ enum . name . lower () for enum in ProblemName ] print ( f \"Parameter 'problemName' was not recognized or unset. Try one of {names}.\" ) else : super () . __init__ ( problemtype , random_state = random_state , n_jobs = n_jobs , verbose = verbose , n_param_search = n_param_search , n_probe_features = n_probe_features , w_l1_slack = slack_regularization , loss_slack = slack_loss , normalize = normalize , ** kwargs , ) __all__ = [ \"genRegressionData\" , \"genClassificationData\" , \"genOrdinalRegressionData\" , \"quick_generate\" , \"plot_intervals\" , \"ProblemName\" , \"FRI\" , \"LUPI_MODELS\" , \"NORMAL_MODELS\" , \"genLupiData\" , ] Sub-modules fri.compute fri.main fri.model fri.parameter_searcher fri.plot fri.toydata fri.utils Functions genClassificationData def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None ) Generate synthetic classification data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation. Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes. Raises ValueError Description ValueError Wrong parameters for specified amonut of features/samples. Examples X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None X.shape (200, 2) y.shape (200,) View Source def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 1 , flip_y : float = 0 , random_state : object = None , partition = None , ) : \"\"\" Generate synthetic classification data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float Added gaussian noise to data . Parameter scales Std of normal distribution . flip_y : float , optional Ratio of samples randomly switched to wrong class . random_state : object , optional Randomstate object used for generation . Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output classes . Raises ------ ValueError Description ValueError Wrong parameters for specified amonut of features / samples . Examples --------- >>> X , y = genClassificationData ( n_samples = 200 ) Generating dataset with d = 2 , n = 200 , strongly = 1 , weakly = 0 , partition of weakly = None >>> X . shape ( 200 , 2 ) >>> y . shape ( 200 , ) \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) X = np . zeros (( n_samples , n_features )) # Find partitions which def \u00ed ne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 X_informative , Y = generate_binary_classification_problem ( n_samples , n_strel + part_size , random_state ) X = _fillVariableSpace ( X_informative , random_state , n_samples = n_samples , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) # Add target noise if flip_y > 0 : n_flip = int ( flip_y * n_samples ) Y [ random_state . choice ( n_samples , n_flip ) ] *= - 1 # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ) , scale = noise ) return X , Y genLupiData def ( problemName : fri . ProblemName , n_samples : int = 100 , random_state : object = None , noise : float = 0.1 , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel_groups : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 ) Generate Lupi Data for Classification, Regression and Ordinal Regression Problems Parameters problemName : ProblemName Defines the type of y-values of the problem. Example ProblemName.CLASSIFICATION . n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. noise : float, optional Noise of the created samples around ground truth. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel_groups : int, optional Number of 2 feature groups which are part of redundant subsets (weakly relevant) n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated. Returns X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] View Source def genLupiData ( problemName : ProblemName , n_samples : int = 100 , random_state : object = None , noise : float = 0 . 1 , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel_groups : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0 . 0 , ) : \"\"\" Generate Lupi Data for Classification , Regression and Ordinal Regression Problems Parameters ---------- problemName : ProblemName Defines the type of y - values of the problem . Example ` ProblemName . CLASSIFICATION `. n_samples : int , optional Number of samples random_state : object , optional Randomstate object used for generation . noise : float , optional Noise of the created samples around ground truth . n_ordinal_bins : int , optional Number of bins in which the regressional target variable is split to form the ordinal classes , Only has an effect if problemType == ' ordinalRegression ' n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_weakrel_groups : int , optional Number of 2 feature groups which are part of redundant subsets ( weakly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . n_irrel : int , optional Number of features which are irrelevant to the underlying model label_noise : float , optional Percentage of labels which get permutated . Returns ------- X : array of shape [ n_samples , ( n_strel + n_weakrel + n_repeated + n_irrel ) ] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [ n_samples ] The generated target values In case of problemType == ' classification ' : values are in [ 0 , 1 ] In case of problemType == ' regression ' : values are continious In case of problemType == ' ordinalRegression ' : values are in [ 0 , n_ordinal_bins ] \"\"\" random_state = check_random_state ( random_state ) n_informative = n_strel + n_weakrel_groups w = random_state . normal ( size = n_informative ) X_informative = random_state . normal ( size = ( n_samples , n_informative )) X_priv_strel = X_informative [:, : n_strel ] X_priv_weakrel = np . zeros ( [ n_samples , n_weakrel_groups * 2 ] ) idx = 0 for i in range ( n_weakrel_groups ) : X_priv_weakrel [:, idx : idx + 2 ] = np . tile ( X_informative [:, n_strel + i ], ( 2 , 1 ) ) . T + random_state . normal ( loc = 0 , scale = np . std ( X_informative [:, n_strel + i ] ) , size = 2 ) idx += 2 X_priv_repeated = _genRepeatedFeatures ( n_repeated , np . hstack ( [ X_priv_strel , X_priv_weakrel ] ) , random_state ) X_priv = np . hstack ( [ X_priv_strel , X_priv_weakrel , X_priv_repeated ] ) e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ) , scale = noise * np . std ( X_priv ) ) X = X_priv + e scores = np . dot ( X_informative , w ) if ( problemName == \" classification \" or problemName == ProblemName . LUPI_CLASSIFICATION ) : y = scores > 0 elif problemName == \" regression \" or problemName == ProblemName . LUPI_REGRESSION : y = scores elif ( problemName == \" ordinalRegression \" or problemName == ProblemName . LUPI_ORDREGRESSION ) : step = 1 / ( n_ordinal_bins ) quantiles = [ i * step for i in range ( 1 , n_ordinal_bins ) ] bs = np . quantile ( scores , quantiles ) bs = np . append ( bs , np . inf ) scores = scores [:, np . newaxis ] y = np . sum ( scores - bs >= 0 , - 1 ) if n_irrel > 0 : X = np . hstack ( [ X , random_state . normal ( size = ( n_samples , n_irrel )) ] ) X_priv = np . hstack ( [ X_priv , random_state . normal ( size = ( n_samples , n_irrel )) ] ) if label_noise > 0 : sample = random_state . choice ( len ( y ) , int ( len ( y ) * label_noise )) y [ sample ] = random_state . permutation ( y [ sample ] ) return ( X , X_priv , y . squeeze ()) genOrdinalRegressionData def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 ) Generate ordinal regression data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ValueError Wrong parameters for specified amonut of features/samples. View Source def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 0 , random_state : object = None , partition = None , n_target_bins : int = 3 , ) : \"\"\" Generate ordinal regression data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float , optional Noise of the created samples around ground truth . random_state : object , optional Randomstate object used for generation . n_target_bins : int , optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output values ( target ) . Raises ------ ValueError Wrong parameters for specified amonut of features / samples . \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) if not n_target_bins > 1 : raise ValueError ( \" At least 2 target bins needed \" ) # Use normal regression data as starting point X_regression , Y_regression = genRegressionData ( n_samples = int ( n_samples ) , n_features = int ( n_features ) , n_redundant = int ( n_redundant ) , n_strel = int ( n_strel ) , n_repeated = int ( n_repeated ) , noise = 0 , random_state = random_state , partition = partition , ) bin_size = int ( np . floor ( n_samples / n_target_bins )) rest = int ( n_samples - ( bin_size * n_target_bins )) # Sort the target values and rearange the data accordingly sort_indices = np . argsort ( Y_regression ) X = X_regression [ sort_indices ] Y = Y_regression [ sort_indices ] # Assign ordinal classes as target values for i in range ( n_target_bins ) : Y [ bin_size * i : bin_size * ( i + 1 ) ] = i # Put non divisable rest into last bin if rest > 0 : Y [ - rest :] = n_target_bins - 1 X , Y = shuffle ( X , Y , random_state = random_state ) # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ) , scale = noise ) return X , Y genRegressionData def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None ) -> object Generate synthetic regression data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ValueError Wrong parameters for specified amonut of features/samples. View Source def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 0 , random_state : object = None , partition = None , ) -> object : \"\"\" Generate synthetic regression data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float , optional Noise of the created samples around ground truth . random_state : object , optional Randomstate object used for generation . Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output values ( target ) . Raises ------ ValueError Wrong parameters for specified amonut of features / samples . \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) # Find partitions which def \u00ed ne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 n_informative = n_strel + part_size X = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [: n_informative , :] = 0 . 3 bias = 0 y = np . dot ( X , ground_truth ) + bias # Add noise if noise > 0 . 0 : y += random_state . normal ( scale = noise , size = y . shape ) X = _fillVariableSpace ( X , random_state , n_samples = n_samples , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , noise = noise , partition = partition , ) y = np . squeeze ( y ) return X , y plot_intervals def ( model , ticklabels = None ) Plot the relevance intervals. Parameters model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) View Source def plot_intervals ( model , ticklabels = None ) : \"\"\" Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before . ticklabels : list of str , optional Strs for ticklabels on x - axis ( features ) \"\"\" if model . interval_ is not None : plotIntervals ( model . interval_ , ticklabels = ticklabels , classes = model . relevance_classes_ ) else : print ( \" Intervals not computed. Try running fit() function first. \" ) quick_generate def ( problem : object , ** kwargs ) -> [ < class ' numpy . ndarray '>, <class ' numpy . ndarray '>] Method to wrap individual data generation functions. Allows passing problem as a string such as \"classification\" or ProblemName object of the corresponding type. For possible kwargs see genClassificationData' or genLupiData`. Parameters problem : str or ProblemName Type of data to generate (e.g. \"classification\" or ProblemName.CLASSIFICATION kwargs : **dict arguments to pass to the generation functions Returns Tuple[numpy.ndarray, numpy.ndarray] View Source def quick_generate ( problem : object , ** kwargs ) -> [ np . ndarray , np . ndarray ]: \"\"\" Method to wrap individual data generation functions . Allows passing ` problem ` as a string such as \" classification \" or ` ProblemName ` object of the corresponding type . For possible kwargs see ` genClassificationData ' or `genLupiData`. Parameters ---------- problem : str or ` ProblemName ` Type of data to generate ( e . g . \" classification \" or ` ProblemName . CLASSIFICATION ` kwargs : ** dict arguments to pass to the generation functions Returns ------- Tuple [ numpy . ndarray , numpy . ndarray ] \"\"\" if problem is \" regression \" or problem is ProblemName . REGRESSION : gen = genRegressionData elif problem is \" classification \" or problem is ProblemName . CLASSIFICATION : gen = genClassificationData elif problem is \" ordreg \" or problem is ProblemName . ORDINALREGRESSION : gen = genOrdinalRegressionData elif problem is \" lupi_regression \" or problem is ProblemName . LUPI_REGRESSION : gen = genLupiData kwargs [ \" problemName \" ] = ProblemName . LUPI_REGRESSION elif problem is \" lupi_classification \" or problem is ProblemName . LUPI_CLASSIFICATION : gen = genLupiData kwargs [ \" problemName \" ] = ProblemName . LUPI_CLASSIFICATION elif problem is \" lupi_ordreg \" or problem is ProblemName . LUPI_ORDREGRESSION : gen = genLupiData kwargs [ \" problemName \" ] = ProblemName . LUPI_ORDREGRESSION else : raise ValueError ( \" Unknown problem type. Try 'regression', 'classification' or 'ordreg' and/or add 'lupi_' prefix \" ) return gen ( ** kwargs ) Classes FRI class ( problemName : object , random_state : object = None , n_jobs : object = 1 , verbose : object = 0 , n_param_search : object = 10 , n_probe_features : object = 20 , slack_regularization : object = 0.001 , slack_loss : object = 0.001 , normalize : object = True , ** kwargs ) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Main class to use FRI in programattic fashion following the scikit-learn paradigm. Parameters problemName: ProblemName or str Type of Problem as enum value or explicit string (e.g. \"classification\") random_state: object or int Random state object or int n_jobs: int or None Number of threads or -1 for automatic. verbose: int Verbosity if > 0 n_param_search: int Number of parameter samples in random search for hyperparameters. n_probe_features: int Number of probes to generate to improve feature selection. slack_regularization: float Allow deviation from optimal L1 norm. slack_loss: float Allow deviation of loss. normalize: boolean Normalize relevace bounds to range of [0,1] depending on L1 norm. Ancestors (in MRO) fri.main.FRIBase sklearn.base.BaseEstimator sklearn.feature_selection.base.SelectorMixin sklearn.base.TransformerMixin View Source class FRI ( FRIBase ) : def __init__ ( self , problemName : object , random_state : object = None , n_jobs : object = 1 , verbose : object = 0 , n_param_search : object = 10 , n_probe_features : object = 20 , slack_regularization : object = 0 . 001 , slack_loss : object = 0 . 001 , normalize : object = True , ** kwargs , ) : \"\"\" Main class to use ` FRI ` in programattic fashion following the scikit - learn paradigm . Parameters ---------- problemName : ` ProblemName ` or str Type of Problem as enum value or explicit string ( e . g . \" classification \" ) random_state : object or int Random state object or int n_jobs : int or None Number of threads or - 1 for automatic . verbose : int Verbosity if > 0 n_param_search : int Number of parameter samples in random search for hyperparameters . n_probe_features : int Number of probes to generate to improve feature selection . slack_regularization : float Allow deviation from optimal L1 norm . slack_loss : float Allow deviation of loss . normalize : boolean Normalize relevace bounds to range of [ 0 , 1 ] depending on L1 norm . \"\"\" if isinstance ( problemName , ProblemName ) : problemtype = problemName . value else : if problemName == \" classification \" or problemName == \" class \" : problemtype = ProblemName . CLASSIFICATION elif problemName == \" regression \" or problemName == \" reg \" : problemtype = ProblemName . REGRESSION elif problemName == \" ordinalregression \" or problemName == \" ordreg \" : problemtype = ProblemName . ORDINALREGRESSION elif problemName == \" lupi_classification \" or problemName == \" lupi_class \" : problemtype = ProblemName . LUPI_CLASSIFICATION if problemtype is None : names = [ enum . name . lower () for enum in ProblemName ] print ( f \" Parameter 'problemName' was not recognized or unset. Try one of {names}. \" ) else : super () . __init__ ( problemtype , random_state = random_state , n_jobs = n_jobs , verbose = verbose , n_param_search = n_param_search , n_probe_features = n_probe_features , w_l1_slack = slack_regularization , loss_slack = slack_loss , normalize = normalize , ** kwargs , ) ProblemName class ( * args , ** kwargs ) Enum which contains usable models for which feature relevance intervals can be computed in :func: ~FRI . Ancestors (in MRO) enum.Enum Class variables ``` python3 CLASSIFICATION ``` : ``` python3 LUPI_CLASSIFICATION ``` : ``` python3 LUPI_ORDREGRESSION ``` : ``` python3 LUPI_REGRESSION ``` : ``` python3 ORDINALREGRESSION ``` : ``` python3 REGRESSION ``` : View Source class ProblemName ( Enum ) : \"\"\" Enum which contains usable models for which feature relevance intervals can be computed in : func :` ~ FRI `. \"\"\" CLASSIFICATION = fri . model . Classification REGRESSION = fri . model . Regression ORDINALREGRESSION = fri . model . OrdinalRegression LUPI_CLASSIFICATION = fri . model . LUPI_Classification LUPI_REGRESSION = fri . model . LUPI_Regression LUPI_ORDREGRESSION = fri . model . LUPI_OrdinalRegression","title":"Index"},{"location":"reference/fri/#module-fri","text":"View Source import logging # noinspection PyUnresolvedReferences from fri._version import __version__ logging . basicConfig ( level = logging . INFO ) from enum import Enum import fri.model class ProblemName ( Enum ): \"\"\" Enum which contains usable models for which feature relevance intervals can be computed in :func:`~FRI`. \"\"\" CLASSIFICATION = fri . model . Classification REGRESSION = fri . model . Regression ORDINALREGRESSION = fri . model . OrdinalRegression LUPI_CLASSIFICATION = fri . model . LUPI_Classification LUPI_REGRESSION = fri . model . LUPI_Regression LUPI_ORDREGRESSION = fri . model . LUPI_OrdinalRegression NORMAL_MODELS = [ ProblemName . CLASSIFICATION , ProblemName . REGRESSION , ProblemName . ORDINALREGRESSION , ] LUPI_MODELS = [ ProblemName . LUPI_CLASSIFICATION , ProblemName . LUPI_REGRESSION , ProblemName . LUPI_ORDREGRESSION , ] from fri.toydata import ( genRegressionData , genClassificationData , genOrdinalRegressionData , quick_generate , genLupiData , ) from fri.main import FRIBase from fri.plot import plot_intervals class FRI ( FRIBase ): def __init__ ( self , problemName : object , random_state : object = None , n_jobs : object = 1 , verbose : object = 0 , n_param_search : object = 10 , n_probe_features : object = 20 , slack_regularization : object = 0.001 , slack_loss : object = 0.001 , normalize : object = True , ** kwargs , ): \"\"\" Main class to use `FRI` in programattic fashion following the scikit-learn paradigm. Parameters ---------- problemName: `ProblemName` or str Type of Problem as enum value or explicit string (e.g. \"classification\") random_state: object or int Random state object or int n_jobs: int or None Number of threads or -1 for automatic. verbose: int Verbosity if > 0 n_param_search: int Number of parameter samples in random search for hyperparameters. n_probe_features: int Number of probes to generate to improve feature selection. slack_regularization: float Allow deviation from optimal L1 norm. slack_loss: float Allow deviation of loss. normalize: boolean Normalize relevace bounds to range of [0,1] depending on L1 norm. \"\"\" if isinstance ( problemName , ProblemName ): problemtype = problemName . value else : if problemName == \"classification\" or problemName == \"class\" : problemtype = ProblemName . CLASSIFICATION elif problemName == \"regression\" or problemName == \"reg\" : problemtype = ProblemName . REGRESSION elif problemName == \"ordinalregression\" or problemName == \"ordreg\" : problemtype = ProblemName . ORDINALREGRESSION elif problemName == \"lupi_classification\" or problemName == \"lupi_class\" : problemtype = ProblemName . LUPI_CLASSIFICATION if problemtype is None : names = [ enum . name . lower () for enum in ProblemName ] print ( f \"Parameter 'problemName' was not recognized or unset. Try one of {names}.\" ) else : super () . __init__ ( problemtype , random_state = random_state , n_jobs = n_jobs , verbose = verbose , n_param_search = n_param_search , n_probe_features = n_probe_features , w_l1_slack = slack_regularization , loss_slack = slack_loss , normalize = normalize , ** kwargs , ) __all__ = [ \"genRegressionData\" , \"genClassificationData\" , \"genOrdinalRegressionData\" , \"quick_generate\" , \"plot_intervals\" , \"ProblemName\" , \"FRI\" , \"LUPI_MODELS\" , \"NORMAL_MODELS\" , \"genLupiData\" , ]","title":"Module fri"},{"location":"reference/fri/#sub-modules","text":"fri.compute fri.main fri.model fri.parameter_searcher fri.plot fri.toydata fri.utils","title":"Sub-modules"},{"location":"reference/fri/#functions","text":"","title":"Functions"},{"location":"reference/fri/#genclassificationdata","text":"def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None ) Generate synthetic classification data","title":"genClassificationData"},{"location":"reference/fri/#parameters","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation.","title":"Parameters"},{"location":"reference/fri/#returns","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes.","title":"Returns"},{"location":"reference/fri/#raises","text":"ValueError Description ValueError Wrong parameters for specified amonut of features/samples.","title":"Raises"},{"location":"reference/fri/#examples","text":"X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None X.shape (200, 2) y.shape (200,) View Source def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 1 , flip_y : float = 0 , random_state : object = None , partition = None , ) : \"\"\" Generate synthetic classification data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float Added gaussian noise to data . Parameter scales Std of normal distribution . flip_y : float , optional Ratio of samples randomly switched to wrong class . random_state : object , optional Randomstate object used for generation . Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output classes . Raises ------ ValueError Description ValueError Wrong parameters for specified amonut of features / samples . Examples --------- >>> X , y = genClassificationData ( n_samples = 200 ) Generating dataset with d = 2 , n = 200 , strongly = 1 , weakly = 0 , partition of weakly = None >>> X . shape ( 200 , 2 ) >>> y . shape ( 200 , ) \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) X = np . zeros (( n_samples , n_features )) # Find partitions which def \u00ed ne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 X_informative , Y = generate_binary_classification_problem ( n_samples , n_strel + part_size , random_state ) X = _fillVariableSpace ( X_informative , random_state , n_samples = n_samples , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) # Add target noise if flip_y > 0 : n_flip = int ( flip_y * n_samples ) Y [ random_state . choice ( n_samples , n_flip ) ] *= - 1 # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ) , scale = noise ) return X , Y","title":"Examples"},{"location":"reference/fri/#genlupidata","text":"def ( problemName : fri . ProblemName , n_samples : int = 100 , random_state : object = None , noise : float = 0.1 , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel_groups : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 ) Generate Lupi Data for Classification, Regression and Ordinal Regression Problems","title":"genLupiData"},{"location":"reference/fri/#parameters_1","text":"problemName : ProblemName Defines the type of y-values of the problem. Example ProblemName.CLASSIFICATION . n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. noise : float, optional Noise of the created samples around ground truth. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel_groups : int, optional Number of 2 feature groups which are part of redundant subsets (weakly relevant) n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated.","title":"Parameters"},{"location":"reference/fri/#returns_1","text":"X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] View Source def genLupiData ( problemName : ProblemName , n_samples : int = 100 , random_state : object = None , noise : float = 0 . 1 , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel_groups : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0 . 0 , ) : \"\"\" Generate Lupi Data for Classification , Regression and Ordinal Regression Problems Parameters ---------- problemName : ProblemName Defines the type of y - values of the problem . Example ` ProblemName . CLASSIFICATION `. n_samples : int , optional Number of samples random_state : object , optional Randomstate object used for generation . noise : float , optional Noise of the created samples around ground truth . n_ordinal_bins : int , optional Number of bins in which the regressional target variable is split to form the ordinal classes , Only has an effect if problemType == ' ordinalRegression ' n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_weakrel_groups : int , optional Number of 2 feature groups which are part of redundant subsets ( weakly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . n_irrel : int , optional Number of features which are irrelevant to the underlying model label_noise : float , optional Percentage of labels which get permutated . Returns ------- X : array of shape [ n_samples , ( n_strel + n_weakrel + n_repeated + n_irrel ) ] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [ n_samples ] The generated target values In case of problemType == ' classification ' : values are in [ 0 , 1 ] In case of problemType == ' regression ' : values are continious In case of problemType == ' ordinalRegression ' : values are in [ 0 , n_ordinal_bins ] \"\"\" random_state = check_random_state ( random_state ) n_informative = n_strel + n_weakrel_groups w = random_state . normal ( size = n_informative ) X_informative = random_state . normal ( size = ( n_samples , n_informative )) X_priv_strel = X_informative [:, : n_strel ] X_priv_weakrel = np . zeros ( [ n_samples , n_weakrel_groups * 2 ] ) idx = 0 for i in range ( n_weakrel_groups ) : X_priv_weakrel [:, idx : idx + 2 ] = np . tile ( X_informative [:, n_strel + i ], ( 2 , 1 ) ) . T + random_state . normal ( loc = 0 , scale = np . std ( X_informative [:, n_strel + i ] ) , size = 2 ) idx += 2 X_priv_repeated = _genRepeatedFeatures ( n_repeated , np . hstack ( [ X_priv_strel , X_priv_weakrel ] ) , random_state ) X_priv = np . hstack ( [ X_priv_strel , X_priv_weakrel , X_priv_repeated ] ) e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ) , scale = noise * np . std ( X_priv ) ) X = X_priv + e scores = np . dot ( X_informative , w ) if ( problemName == \" classification \" or problemName == ProblemName . LUPI_CLASSIFICATION ) : y = scores > 0 elif problemName == \" regression \" or problemName == ProblemName . LUPI_REGRESSION : y = scores elif ( problemName == \" ordinalRegression \" or problemName == ProblemName . LUPI_ORDREGRESSION ) : step = 1 / ( n_ordinal_bins ) quantiles = [ i * step for i in range ( 1 , n_ordinal_bins ) ] bs = np . quantile ( scores , quantiles ) bs = np . append ( bs , np . inf ) scores = scores [:, np . newaxis ] y = np . sum ( scores - bs >= 0 , - 1 ) if n_irrel > 0 : X = np . hstack ( [ X , random_state . normal ( size = ( n_samples , n_irrel )) ] ) X_priv = np . hstack ( [ X_priv , random_state . normal ( size = ( n_samples , n_irrel )) ] ) if label_noise > 0 : sample = random_state . choice ( len ( y ) , int ( len ( y ) * label_noise )) y [ sample ] = random_state . permutation ( y [ sample ] ) return ( X , X_priv , y . squeeze ())","title":"Returns"},{"location":"reference/fri/#genordinalregressiondata","text":"def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 ) Generate ordinal regression data","title":"genOrdinalRegressionData"},{"location":"reference/fri/#parameters_2","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes","title":"Parameters"},{"location":"reference/fri/#returns_2","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target).","title":"Returns"},{"location":"reference/fri/#raises_1","text":"ValueError Wrong parameters for specified amonut of features/samples. View Source def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 0 , random_state : object = None , partition = None , n_target_bins : int = 3 , ) : \"\"\" Generate ordinal regression data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float , optional Noise of the created samples around ground truth . random_state : object , optional Randomstate object used for generation . n_target_bins : int , optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output values ( target ) . Raises ------ ValueError Wrong parameters for specified amonut of features / samples . \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) if not n_target_bins > 1 : raise ValueError ( \" At least 2 target bins needed \" ) # Use normal regression data as starting point X_regression , Y_regression = genRegressionData ( n_samples = int ( n_samples ) , n_features = int ( n_features ) , n_redundant = int ( n_redundant ) , n_strel = int ( n_strel ) , n_repeated = int ( n_repeated ) , noise = 0 , random_state = random_state , partition = partition , ) bin_size = int ( np . floor ( n_samples / n_target_bins )) rest = int ( n_samples - ( bin_size * n_target_bins )) # Sort the target values and rearange the data accordingly sort_indices = np . argsort ( Y_regression ) X = X_regression [ sort_indices ] Y = Y_regression [ sort_indices ] # Assign ordinal classes as target values for i in range ( n_target_bins ) : Y [ bin_size * i : bin_size * ( i + 1 ) ] = i # Put non divisable rest into last bin if rest > 0 : Y [ - rest :] = n_target_bins - 1 X , Y = shuffle ( X , Y , random_state = random_state ) # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ) , scale = noise ) return X , Y","title":"Raises"},{"location":"reference/fri/#genregressiondata","text":"def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None ) -> object Generate synthetic regression data","title":"genRegressionData"},{"location":"reference/fri/#parameters_3","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation.","title":"Parameters"},{"location":"reference/fri/#returns_3","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target).","title":"Returns"},{"location":"reference/fri/#raises_2","text":"ValueError Wrong parameters for specified amonut of features/samples. View Source def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 0 , random_state : object = None , partition = None , ) -> object : \"\"\" Generate synthetic regression data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float , optional Noise of the created samples around ground truth . random_state : object , optional Randomstate object used for generation . Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output values ( target ) . Raises ------ ValueError Wrong parameters for specified amonut of features / samples . \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) # Find partitions which def \u00ed ne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 n_informative = n_strel + part_size X = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [: n_informative , :] = 0 . 3 bias = 0 y = np . dot ( X , ground_truth ) + bias # Add noise if noise > 0 . 0 : y += random_state . normal ( scale = noise , size = y . shape ) X = _fillVariableSpace ( X , random_state , n_samples = n_samples , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , noise = noise , partition = partition , ) y = np . squeeze ( y ) return X , y","title":"Raises"},{"location":"reference/fri/#plot_intervals","text":"def ( model , ticklabels = None ) Plot the relevance intervals.","title":"plot_intervals"},{"location":"reference/fri/#parameters_4","text":"model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) View Source def plot_intervals ( model , ticklabels = None ) : \"\"\" Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before . ticklabels : list of str , optional Strs for ticklabels on x - axis ( features ) \"\"\" if model . interval_ is not None : plotIntervals ( model . interval_ , ticklabels = ticklabels , classes = model . relevance_classes_ ) else : print ( \" Intervals not computed. Try running fit() function first. \" )","title":"Parameters"},{"location":"reference/fri/#quick_generate","text":"def ( problem : object , ** kwargs ) -> [ < class ' numpy . ndarray '>, <class ' numpy . ndarray '>] Method to wrap individual data generation functions. Allows passing problem as a string such as \"classification\" or ProblemName object of the corresponding type. For possible kwargs see genClassificationData' or genLupiData`.","title":"quick_generate"},{"location":"reference/fri/#parameters_5","text":"problem : str or ProblemName Type of data to generate (e.g. \"classification\" or ProblemName.CLASSIFICATION kwargs : **dict arguments to pass to the generation functions","title":"Parameters"},{"location":"reference/fri/#returns_4","text":"Tuple[numpy.ndarray, numpy.ndarray] View Source def quick_generate ( problem : object , ** kwargs ) -> [ np . ndarray , np . ndarray ]: \"\"\" Method to wrap individual data generation functions . Allows passing ` problem ` as a string such as \" classification \" or ` ProblemName ` object of the corresponding type . For possible kwargs see ` genClassificationData ' or `genLupiData`. Parameters ---------- problem : str or ` ProblemName ` Type of data to generate ( e . g . \" classification \" or ` ProblemName . CLASSIFICATION ` kwargs : ** dict arguments to pass to the generation functions Returns ------- Tuple [ numpy . ndarray , numpy . ndarray ] \"\"\" if problem is \" regression \" or problem is ProblemName . REGRESSION : gen = genRegressionData elif problem is \" classification \" or problem is ProblemName . CLASSIFICATION : gen = genClassificationData elif problem is \" ordreg \" or problem is ProblemName . ORDINALREGRESSION : gen = genOrdinalRegressionData elif problem is \" lupi_regression \" or problem is ProblemName . LUPI_REGRESSION : gen = genLupiData kwargs [ \" problemName \" ] = ProblemName . LUPI_REGRESSION elif problem is \" lupi_classification \" or problem is ProblemName . LUPI_CLASSIFICATION : gen = genLupiData kwargs [ \" problemName \" ] = ProblemName . LUPI_CLASSIFICATION elif problem is \" lupi_ordreg \" or problem is ProblemName . LUPI_ORDREGRESSION : gen = genLupiData kwargs [ \" problemName \" ] = ProblemName . LUPI_ORDREGRESSION else : raise ValueError ( \" Unknown problem type. Try 'regression', 'classification' or 'ordreg' and/or add 'lupi_' prefix \" ) return gen ( ** kwargs )","title":"Returns"},{"location":"reference/fri/#classes","text":"","title":"Classes"},{"location":"reference/fri/#fri","text":"class ( problemName : object , random_state : object = None , n_jobs : object = 1 , verbose : object = 0 , n_param_search : object = 10 , n_probe_features : object = 20 , slack_regularization : object = 0.001 , slack_loss : object = 0.001 , normalize : object = True , ** kwargs ) Base class for all estimators in scikit-learn","title":"FRI"},{"location":"reference/fri/#notes","text":"All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Main class to use FRI in programattic fashion following the scikit-learn paradigm.","title":"Notes"},{"location":"reference/fri/#parameters_6","text":"problemName: ProblemName or str Type of Problem as enum value or explicit string (e.g. \"classification\") random_state: object or int Random state object or int n_jobs: int or None Number of threads or -1 for automatic. verbose: int Verbosity if > 0 n_param_search: int Number of parameter samples in random search for hyperparameters. n_probe_features: int Number of probes to generate to improve feature selection. slack_regularization: float Allow deviation from optimal L1 norm. slack_loss: float Allow deviation of loss. normalize: boolean Normalize relevace bounds to range of [0,1] depending on L1 norm.","title":"Parameters"},{"location":"reference/fri/#ancestors-in-mro","text":"fri.main.FRIBase sklearn.base.BaseEstimator sklearn.feature_selection.base.SelectorMixin sklearn.base.TransformerMixin View Source class FRI ( FRIBase ) : def __init__ ( self , problemName : object , random_state : object = None , n_jobs : object = 1 , verbose : object = 0 , n_param_search : object = 10 , n_probe_features : object = 20 , slack_regularization : object = 0 . 001 , slack_loss : object = 0 . 001 , normalize : object = True , ** kwargs , ) : \"\"\" Main class to use ` FRI ` in programattic fashion following the scikit - learn paradigm . Parameters ---------- problemName : ` ProblemName ` or str Type of Problem as enum value or explicit string ( e . g . \" classification \" ) random_state : object or int Random state object or int n_jobs : int or None Number of threads or - 1 for automatic . verbose : int Verbosity if > 0 n_param_search : int Number of parameter samples in random search for hyperparameters . n_probe_features : int Number of probes to generate to improve feature selection . slack_regularization : float Allow deviation from optimal L1 norm . slack_loss : float Allow deviation of loss . normalize : boolean Normalize relevace bounds to range of [ 0 , 1 ] depending on L1 norm . \"\"\" if isinstance ( problemName , ProblemName ) : problemtype = problemName . value else : if problemName == \" classification \" or problemName == \" class \" : problemtype = ProblemName . CLASSIFICATION elif problemName == \" regression \" or problemName == \" reg \" : problemtype = ProblemName . REGRESSION elif problemName == \" ordinalregression \" or problemName == \" ordreg \" : problemtype = ProblemName . ORDINALREGRESSION elif problemName == \" lupi_classification \" or problemName == \" lupi_class \" : problemtype = ProblemName . LUPI_CLASSIFICATION if problemtype is None : names = [ enum . name . lower () for enum in ProblemName ] print ( f \" Parameter 'problemName' was not recognized or unset. Try one of {names}. \" ) else : super () . __init__ ( problemtype , random_state = random_state , n_jobs = n_jobs , verbose = verbose , n_param_search = n_param_search , n_probe_features = n_probe_features , w_l1_slack = slack_regularization , loss_slack = slack_loss , normalize = normalize , ** kwargs , )","title":"Ancestors (in MRO)"},{"location":"reference/fri/#problemname","text":"class ( * args , ** kwargs ) Enum which contains usable models for which feature relevance intervals can be computed in :func: ~FRI .","title":"ProblemName"},{"location":"reference/fri/#ancestors-in-mro_1","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/fri/#class-variables","text":"``` python3 CLASSIFICATION ``` : ``` python3 LUPI_CLASSIFICATION ``` : ``` python3 LUPI_ORDREGRESSION ``` : ``` python3 LUPI_REGRESSION ``` : ``` python3 ORDINALREGRESSION ``` : ``` python3 REGRESSION ``` : View Source class ProblemName ( Enum ) : \"\"\" Enum which contains usable models for which feature relevance intervals can be computed in : func :` ~ FRI `. \"\"\" CLASSIFICATION = fri . model . Classification REGRESSION = fri . model . Regression ORDINALREGRESSION = fri . model . OrdinalRegression LUPI_CLASSIFICATION = fri . model . LUPI_Classification LUPI_REGRESSION = fri . model . LUPI_Regression LUPI_ORDREGRESSION = fri . model . LUPI_OrdinalRegression","title":"Class variables"},{"location":"reference/fri/compute/","text":"Module fri.compute View Source import logging from collections import defaultdict import joblib import numpy as np from scipy import stats from fri.model.base_cvxproblem import Relevance_CVXProblem from fri.model.base_initmodel import InitModel from fri.model.base_type import ProblemType from fri.utils import permutate_feature_in_data MIN_N_PROBE_FEATURES = 20 # Lower bound of probe features def _start_solver_worker ( bound : Relevance_CVXProblem ): \"\"\" Worker thread method for parallel computation \"\"\" return bound . solve () class RelevanceBoundsIntervals ( object ): def __init__ ( self , data , problem_type : ProblemType , best_init_model : InitModel , random_state , n_resampling , n_jobs , verbose , normalize = True , ): self . data = data self . problem_type = problem_type self . verbose = verbose self . n_jobs = n_jobs self . n_resampling = n_resampling self . random_state = random_state self . best_init_model = best_init_model self . best_hyperparameters = best_init_model . hyperparam self . normalize = normalize # Relax constraints to improve stability relaxed_constraints = problem_type . get_relaxed_constraints ( best_init_model . constraints ) self . init_constraints = relaxed_constraints def get_normalized_lupi_intervals ( self , lupi_features , presetModel = None ): # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO: handle other data formats all_d = X . shape [ 1 ] normal_d = all_d - lupi_features # Compute relevance bounds and probes for normal features and LUPI with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : d_n = _get_necessary_dimensions ( normal_d , presetModel ) rb = self . compute_relevance_bounds ( d_n , parallel = parallel ) probe_upper = self . compute_probe_values ( d_n , True , parallel = parallel ) probe_lower = self . compute_probe_values ( d_n , False , parallel = parallel ) d_l = _get_necessary_dimensions ( all_d , presetModel , start = normal_d ) rb_l = self . compute_relevance_bounds ( d_l , parallel = parallel ) probe_priv_upper = self . compute_probe_values ( d_l , True , parallel = parallel ) probe_priv_lower = self . compute_probe_values ( d_l , False , parallel = parallel ) probes = [ probe_lower , probe_upper , probe_priv_lower , probe_priv_upper ] # # Postprocess # # Get Scaling Parameters l1 = self . init_constraints [ \"w_l1\" ] l1_priv = self . init_constraints [ \"w_priv_l1\" ] # Normalize Normal and Lupi features rb_norm = self . _postprocessing ( l1 , rb ) rb_l_norm = self . _postprocessing ( l1_priv , rb_l ) interval_ = np . concatenate ([ rb_norm , rb_l_norm ]) # Normalize Probes probe_lower = self . _postprocessing ( l1 , probe_lower ) probe_upper = self . _postprocessing ( l1 , probe_upper ) probe_priv_lower = self . _postprocessing ( l1_priv , probe_priv_lower ) probe_priv_upper = self . _postprocessing ( l1_priv , probe_priv_upper ) # # # Classify features fc = feature_classification ( probe_lower , probe_upper , rb_norm , verbose = self . verbose ) fc_l = feature_classification ( probe_priv_lower , probe_priv_upper , rb_l_norm , verbose = self . verbose ) fc_both = np . concatenate ([ fc , fc_l ]) return interval_ , fc_both def get_normalized_intervals ( self , presetModel = None ): # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO: handle other data formats d = X . shape [ 1 ] # Depending on the preset model, we dont need to compute all bounds # e.g. in the case of fixed features we skip those dims = _get_necessary_dimensions ( d , presetModel ) with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : relevance_bounds = self . compute_relevance_bounds ( dims , parallel = parallel , presetModel = presetModel ) probe_values_upper = self . compute_probe_values ( dims , isUpper = True , parallel = parallel , presetModel = presetModel ) probe_values_lower = self . compute_probe_values ( dims , isUpper = False , parallel = parallel , presetModel = presetModel ) # Postprocess bounds norm_bounds = self . _postprocessing ( self . best_init_model . L1_factor , relevance_bounds ) norm_probe_values_upper = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_upper ) norm_probe_values_lower = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_lower ) feature_classes = feature_classification ( norm_probe_values_lower , norm_probe_values_upper , norm_bounds , verbose = self . verbose , ) return norm_bounds , feature_classes def compute_relevance_bounds ( self , dims , parallel = None , presetModel = None , solverargs = None ): init_model_state = self . best_init_model . model_state work_queue = self . _generate_relevance_bounds_tasks ( dims , self . data , presetModel , init_model_state ) # Solve relevance bounds in parallel (when available) if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) bound_results = parallel ( map ( joblib . delayed ( _start_solver_worker ), work_queue )) # Retrieve results and aggregate values in dict solved_bounds = defaultdict ( list ) for finished_bound in bound_results : # Only add bounds with feasible solutions if finished_bound . is_solved : solved_bounds [ finished_bound . current_feature ] . append ( finished_bound ) # Initalize array for pair of bounds(= intervals) length = len ( dims ) intervals = np . zeros (( length , 2 )) for abs_index , rel_index in zip ( dims , range ( length )): # Return interval for feature i (can be a fixed value when set beforehand) interval_i = self . _create_interval ( abs_index , solved_bounds , presetModel ) intervals [ rel_index ] = interval_i return intervals # TODO: add model model_state (omega, bias) to return value def compute_probe_values ( self , dims , isUpper = True , parallel = None , presetModel = None ): # Get model parameters init_model_state = self . best_init_model . model_state # Prepare parallel framework if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) # Generate probe_queue = self . _generate_probe_value_tasks ( self . data , dims , isUpper , self . n_resampling , self . random_state , presetModel , init_model_state , ) # Compute solution probe_results = parallel ( map ( joblib . delayed ( _start_solver_worker ), probe_queue )) # probe_values.extend([probe.objective.value for probe in probe_results if probe.is_solved]) candidates = defaultdict ( list ) for candidate in probe_results : # Only add bounds with feasible solutions if candidate . is_solved : candidates [ candidate . probeID ] . append ( candidate ) probe_values = [] for probes_for_ID in candidates . values (): if isUpper : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( probes_for_ID ) ) else : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( probes_for_ID ) ) return np . array ( probe_values ) def _generate_relevance_bounds_tasks ( self , dims , data , preset_model = None , best_model_state = None ): # Do not compute bounds for fixed features if preset_model is not None : dims = [ di for di in dims if di not in preset_model ] # Instantiate objects for computation later for di in dims : # Add Lower Bound problem(s) to work list yield from self . problem_type . get_cvxproblem_template . generate_lower_bound_problem ( self . best_hyperparameters , self . init_constraints , best_model_state , data , di , preset_model , ) # Add problem(s) for Upper bound yield from self . problem_type . get_cvxproblem_template . generate_upper_bound_problem ( self . best_hyperparameters , self . init_constraints , best_model_state , data , di , preset_model , ) def _generate_probe_value_tasks ( self , data , dims , isUpper , n_resampling , random_state , preset_model = None , best_model_state = None , ): if isUpper : factory = ( self . problem_type . get_cvxproblem_template . generate_upper_bound_problem ) else : factory = ( self . problem_type . get_cvxproblem_template . generate_lower_bound_problem ) # Random sample n_resampling shadow features by permuting real features and computing upper bound random_choice = random_state . choice ( a = dims , size = n_resampling ) # Instantiate objects for i , di in enumerate ( random_choice ): data_perm = permutate_feature_in_data ( data , di , random_state ) # We only use upper bounds as probe features yield from factory ( self . best_hyperparameters , self . init_constraints , best_model_state , data_perm , di , preset_model , probeID = i , ) def _create_interval ( self , feature : int , solved_bounds : dict , presetModel : dict = None ): # Return preset values for fixed features if presetModel is not None : if feature in presetModel : return presetModel [ feature ] all_bounds = solved_bounds [ feature ] min_problems_candidates = [ p for p in all_bounds if p . isLowerBound ] max_problems_candidates = [ p for p in all_bounds if not p . isLowerBound ] if len ( all_bounds ) < 2 : logging . error ( f \"(Some) relevance bounds for feature {feature} were not solved.\" ) raise Exception ( \"Infeasible bound(s).\" ) lower_bound = self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( min_problems_candidates ) upper_bound = self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( max_problems_candidates ) return lower_bound , upper_bound def compute_single_preset_relevance_bounds ( self , i : int , signed_preset_i : [ float , float ] ): \"\"\" Method to run method once for one restricted feature Parameters ---------- i: restricted feature signed_preset_i: restricted range of feature i (set before optimization = preset) \"\"\" preset = { i : signed_preset_i } rangevector = self . compute_multi_preset_relevance_bounds ( preset ) return rangevector def compute_multi_preset_relevance_bounds ( self , preset , lupi_features = 0 ): \"\"\" Method to run method with preset values Parameters ---------- lupi_features \"\"\" X , y = self . data # The user is working with normalized values while we compute them unscaled if self . normalize : for k , v in preset . items (): preset [ k ] = np . asarray ( v ) * self . best_init_model . L1_factor # Add sign to presets preset = self . _add_sign_to_preset ( preset ) # Calculate all bounds with feature i set to min_i if lupi_features > 0 : rangevector , f_classes = self . get_normalized_lupi_intervals ( lupi_features , presetModel = preset ) else : rangevector , f_classes = self . get_normalized_intervals ( presetModel = preset ) return rangevector def _add_sign_to_preset ( self , unsigned_presets ): \"\"\" We need signed presets for our convex problem definition later. We reuse the coefficients of the optimal model for this Parameters ---------- unsigned_presets : dict Returns ------- dict \"\"\" signed_presets = {} # Obtain optimal model parameters w = self . best_init_model . model_state [ \"w\" ] sum = 0 for i , preset in unsigned_presets . items (): unsigned_preset_i = np . sign ( w [ i ]) * preset # accumulate maximal feature contribution sum += unsigned_preset_i [ 1 ] signed_presets [ i ] = unsigned_preset_i # Check if unsigned_presets makes sense l1 = self . init_constraints [ \"w_l1\" ] if sum > l1 : print ( \"maximum L1 norm of presets: \" , sum ) print ( \"L1 allowed:\" , l1 ) print ( \"Presets are not feasible. Try lowering values.\" ) return return signed_presets def _postprocessing ( self , L1 , rangevector , round_to_zero = True ): if self . normalize : assert L1 > 0 rangevector = rangevector . copy () / L1 if round_to_zero : rangevector [ rangevector <= 1e-11 ] = 0 return rangevector def _get_necessary_dimensions ( d : int , presetModel : dict = None , start = 0 ): dims = np . arange ( start , d ) # if presetModel is not None: # # Exclude fixed (preset) dimensions from being redundantly computed # dims = [di for di in dims if di not in presetModel.keys()] # TODO: check the removal of this block return dims def feature_classification ( probes_low , probes_up , relevance_bounds , fpr = 1e-4 , verbose = 0 ): logging . debug ( \"**** Feature Selection ****\" ) logging . debug ( \"Generating Lower Probe Statistic\" ) lower_stat = create_probe_statistic ( probes_low , fpr , verbose = verbose ) logging . debug ( \"Generating Upper Probe Statistic\" ) upper_stat = create_probe_statistic ( probes_up , fpr , verbose = verbose ) weakly = relevance_bounds [:, 1 ] > upper_stat [ 1 ] strongly = relevance_bounds [:, 0 ] > lower_stat [ 1 ] both = np . logical_and ( weakly , strongly ) prediction = np . zeros ( relevance_bounds . shape [ 0 ], dtype = np . int ) prediction [ weakly ] = 1 prediction [ both ] = 2 return prediction def create_probe_statistic ( probe_values , fpr , verbose = 0 ): # Create prediction interval statistics based on randomly permutated probe features (based on real features) n = len ( probe_values ) if n == 0 : if verbose > 0 : logging . info ( \"All probes were infeasible. All features considered relevant.\" ) # # If all probes were infeasible we expect an empty list # # If they are infeasible it also means that only strongly relevant features were in the data # # As such we just set the prediction without considering the statistics mean = 0 else : probe_values = np . asarray ( probe_values ) mean = probe_values . mean () if mean == 0 : lower_threshold , upper_threshold = mean , mean s = 0 else : s = probe_values . std () lower_threshold = mean + stats . t ( df = n - 1 ) . ppf ( fpr ) * s * np . sqrt ( 1 + ( 1 / n )) upper_threshold = mean - stats . t ( df = n - 1 ) . ppf ( fpr ) * s * np . sqrt ( 1 + ( 1 / n )) if verbose > 0 : print ( f \"FS threshold: {lower_threshold}-{upper_threshold}, Mean:{mean}, Std:{s}, n_probes {n}\" ) return lower_threshold , upper_threshold Functions create_probe_statistic def ( probe_values , fpr , verbose = 0 ) View Source def create_probe_statistic ( probe_values , fpr , verbose = 0 ) : # Create prediction interval statistics based on randomly permutated probe features ( based on real features ) n = len ( probe_values ) if n == 0 : if verbose > 0 : logging . info ( \" All probes were infeasible. All features considered relevant. \" ) # # If all probes were infeasible we expect an empty list # # If they are infeasible it also means that only strongly relevant features were in the data # # As such we just set the prediction without considering the statistics mean = 0 else : probe_values = np . asarray ( probe_values ) mean = probe_values . mean () if mean == 0 : lower_threshold , upper_threshold = mean , mean s = 0 else : s = probe_values . std () lower_threshold = mean + stats . t ( df = n - 1 ) . ppf ( fpr ) * s * np . sqrt ( 1 + ( 1 / n )) upper_threshold = mean - stats . t ( df = n - 1 ) . ppf ( fpr ) * s * np . sqrt ( 1 + ( 1 / n )) if verbose > 0 : print ( f \" FS threshold: {lower_threshold}-{upper_threshold}, Mean:{mean}, Std:{s}, n_probes {n} \" ) return lower_threshold , upper_threshold feature_classification def ( probes_low , probes_up , relevance_bounds , fpr = 0.0001 , verbose = 0 ) View Source def feature_classification ( probes_low , probes_up , relevance_bounds , fpr = 1 e - 4 , verbose = 0 ) : logging . debug ( \" **** Feature Selection **** \" ) logging . debug ( \" Generating Lower Probe Statistic \" ) lower_stat = create_probe_statistic ( probes_low , fpr , verbose = verbose ) logging . debug ( \" Generating Upper Probe Statistic \" ) upper_stat = create_probe_statistic ( probes_up , fpr , verbose = verbose ) weakly = relevance_bounds [:, 1 ] > upper_stat [ 1 ] strongly = relevance_bounds [:, 0 ] > lower_stat [ 1 ] both = np . logical_and ( weakly , strongly ) prediction = np . zeros ( relevance_bounds . shape [ 0 ], dtype = np . int ) prediction [ weakly ] = 1 prediction [ both ] = 2 return prediction Classes RelevanceBoundsIntervals class ( data , problem_type : fri . model . base_type . ProblemType , best_init_model : fri . model . base_initmodel . InitModel , random_state , n_resampling , n_jobs , verbose , normalize = True ) Methods ##### compute_multi_preset_relevance_bounds ``` python3 def ( self , preset , lupi_features = 0 ) ``` Method to run method with preset values Parameters ---------- lupi_features ??? example \" View Source \" def compute_multi_preset_relevance_bounds ( self , preset , lupi_features = 0 ) : \"\"\" Method to run method with preset values Parameters ---------- lupi_features \"\"\" X , y = self . data # The user is working with normalized values while we compute them unscaled if self . normalize : for k , v in preset . items () : preset [ k ] = np . asarray ( v ) * self . best_init_model . L1_factor # Add sign to presets preset = self . _add_sign_to_preset ( preset ) # Calculate all bounds with feature i set to min_i if lupi_features > 0 : rangevector , f_classes = self . get_normalized_lupi_intervals ( lupi_features , presetModel = preset ) else : rangevector , f_classes = self . get_normalized_intervals ( presetModel = preset ) return rangevector ##### compute_probe_values ``` python3 def ( self , dims , isUpper = True , parallel = None , presetModel = None ) ``` ??? example \" View Source \" def compute_probe_values ( self , dims , isUpper = True , parallel = None , presetModel = None ) : # Get model parameters init_model_state = self . best_init_model . model_state # Prepare parallel framework if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) # Generate probe_queue = self . _generate_probe_value_tasks ( self . data , dims , isUpper , self . n_resampling , self . random_state , presetModel , init_model_state , ) # Compute solution probe_results = parallel ( map ( joblib . delayed ( _start_solver_worker ) , probe_queue )) # probe_values . extend ( [ probe . objective . value for probe in probe_results if probe . is_solved ] ) candidates = defaultdict ( list ) for candidate in probe_results : # Only add bounds with feasible solutions if candidate . is_solved : candidates [ candidate . probeID ]. append ( candidate ) probe_values = [] for probes_for_ID in candidates . values () : if isUpper : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( probes_for_ID ) ) else : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( probes_for_ID ) ) return np . array ( probe_values ) ##### compute_relevance_bounds ``` python3 def ( self , dims , parallel = None , presetModel = None , solverargs = None ) ``` ??? example \" View Source \" def compute_relevance_bounds ( self , dims , parallel = None , presetModel = None , solverargs = None ) : init_model_state = self . best_init_model . model_state work_queue = self . _generate_relevance_bounds_tasks ( dims , self . data , presetModel , init_model_state ) # Solve relevance bounds in parallel ( when available ) if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) bound_results = parallel ( map ( joblib . delayed ( _start_solver_worker ) , work_queue )) # Retrieve results and aggregate values in dict solved_bounds = defaultdict ( list ) for finished_bound in bound_results : # Only add bounds with feasible solutions if finished_bound . is_solved : solved_bounds [ finished_bound . current_feature ]. append ( finished_bound ) # Initalize array for pair of bounds ( = intervals ) length = len ( dims ) intervals = np . zeros (( length , 2 )) for abs_index , rel_index in zip ( dims , range ( length )) : # Return interval for feature i ( can be a fixed value when set beforehand ) interval_i = self . _create_interval ( abs_index , solved_bounds , presetModel ) intervals [ rel_index ] = interval_i return intervals # TODO : add model model_state ( omega , bias ) to return value ##### compute_single_preset_relevance_bounds ``` python3 def ( self , i : int , signed_preset_i : [ < class ' float ' > , < class ' float ' > ] ) ``` Method to run method once for one restricted feature Parameters ---------- i : restricted feature signed_preset_i : restricted range of feature i ( set before optimization = preset ) ??? example \" View Source \" def compute_single_preset_relevance_bounds ( self , i : int , signed_preset_i : [ float , float ] ) : \"\"\" Method to run method once for one restricted feature Parameters ---------- i : restricted feature signed_preset_i : restricted range of feature i ( set before optimization = preset ) \"\"\" preset = { i : signed_preset_i } rangevector = self . compute_multi_preset_relevance_bounds ( preset ) return rangevector ##### get_normalized_intervals ``` python3 def ( self , presetModel = None ) ``` ??? example \" View Source \" def get_normalized_intervals ( self , presetModel = None ) : # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO : handle other data formats d = X . shape [ 1 ] # Depending on the preset model , we dont need to compute all bounds # e . g . in the case of fixed features we skip those dims = _get_necessary_dimensions ( d , presetModel ) with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : relevance_bounds = self . compute_relevance_bounds ( dims , parallel = parallel , presetModel = presetModel ) probe_values_upper = self . compute_probe_values ( dims , isUpper = True , parallel = parallel , presetModel = presetModel ) probe_values_lower = self . compute_probe_values ( dims , isUpper = False , parallel = parallel , presetModel = presetModel ) # Postprocess bounds norm_bounds = self . _postprocessing ( self . best_init_model . L1_factor , relevance_bounds ) norm_probe_values_upper = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_upper ) norm_probe_values_lower = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_lower ) feature_classes = feature_classification ( norm_probe_values_lower , norm_probe_values_upper , norm_bounds , verbose = self . verbose , ) return norm_bounds , feature_classes ##### get_normalized_lupi_intervals ``` python3 def ( self , lupi_features , presetModel = None ) ``` ??? example \" View Source \" def get_normalized_lupi_intervals ( self , lupi_features , presetModel = None ) : # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO : handle other data formats all_d = X . shape [ 1 ] normal_d = all_d - lupi_features # Compute relevance bounds and probes for normal features and LUPI with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : d_n = _get_necessary_dimensions ( normal_d , presetModel ) rb = self . compute_relevance_bounds ( d_n , parallel = parallel ) probe_upper = self . compute_probe_values ( d_n , True , parallel = parallel ) probe_lower = self . compute_probe_values ( d_n , False , parallel = parallel ) d_l = _get_necessary_dimensions ( all_d , presetModel , start = normal_d ) rb_l = self . compute_relevance_bounds ( d_l , parallel = parallel ) probe_priv_upper = self . compute_probe_values ( d_l , True , parallel = parallel ) probe_priv_lower = self . compute_probe_values ( d_l , False , parallel = parallel ) probes = [ probe_lower , probe_upper , probe_priv_lower , probe_priv_upper ] # # Postprocess # # Get Scaling Parameters l1 = self . init_constraints [ \" w_l1 \" ] l1_priv = self . init_constraints [ \" w_priv_l1 \" ] # Normalize Normal and Lupi features rb_norm = self . _postprocessing ( l1 , rb ) rb_l_norm = self . _postprocessing ( l1_priv , rb_l ) interval_ = np . concatenate ( [ rb_norm , rb_l_norm ] ) # Normalize Probes probe_lower = self . _postprocessing ( l1 , probe_lower ) probe_upper = self . _postprocessing ( l1 , probe_upper ) probe_priv_lower = self . _postprocessing ( l1_priv , probe_priv_lower ) probe_priv_upper = self . _postprocessing ( l1_priv , probe_priv_upper ) # # # Classify features fc = feature_classification ( probe_lower , probe_upper , rb_norm , verbose = self . verbose ) fc_l = feature_classification ( probe_priv_lower , probe_priv_upper , rb_l_norm , verbose = self . verbose ) fc_both = np . concatenate ( [ fc , fc_l ] ) return interval_ , fc_both View Source class RelevanceBoundsIntervals ( object ) : def __init__ ( self , data , problem_type : ProblemType , best_init_model : InitModel , random_state , n_resampling , n_jobs , verbose , normalize = True , ) : self . data = data self . problem_type = problem_type self . verbose = verbose self . n_jobs = n_jobs self . n_resampling = n_resampling self . random_state = random_state self . best_init_model = best_init_model self . best_hyperparameters = best_init_model . hyperparam self . normalize = normalize # Relax constraints to improve stability relaxed_constraints = problem_type . get_relaxed_constraints ( best_init_model . constraints ) self . init_constraints = relaxed_constraints def get_normalized_lupi_intervals ( self , lupi_features , presetModel = None ) : # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO : handle other data formats all_d = X . shape [ 1 ] normal_d = all_d - lupi_features # Compute relevance bounds and probes for normal features and LUPI with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : d_n = _get_necessary_dimensions ( normal_d , presetModel ) rb = self . compute_relevance_bounds ( d_n , parallel = parallel ) probe_upper = self . compute_probe_values ( d_n , True , parallel = parallel ) probe_lower = self . compute_probe_values ( d_n , False , parallel = parallel ) d_l = _get_necessary_dimensions ( all_d , presetModel , start = normal_d ) rb_l = self . compute_relevance_bounds ( d_l , parallel = parallel ) probe_priv_upper = self . compute_probe_values ( d_l , True , parallel = parallel ) probe_priv_lower = self . compute_probe_values ( d_l , False , parallel = parallel ) probes = [ probe_lower , probe_upper , probe_priv_lower , probe_priv_upper ] # # Postprocess # # Get Scaling Parameters l1 = self . init_constraints [ \" w_l1 \" ] l1_priv = self . init_constraints [ \" w_priv_l1 \" ] # Normalize Normal and Lupi features rb_norm = self . _postprocessing ( l1 , rb ) rb_l_norm = self . _postprocessing ( l1_priv , rb_l ) interval_ = np . concatenate ( [ rb_norm , rb_l_norm ] ) # Normalize Probes probe_lower = self . _postprocessing ( l1 , probe_lower ) probe_upper = self . _postprocessing ( l1 , probe_upper ) probe_priv_lower = self . _postprocessing ( l1_priv , probe_priv_lower ) probe_priv_upper = self . _postprocessing ( l1_priv , probe_priv_upper ) # # # Classify features fc = feature_classification ( probe_lower , probe_upper , rb_norm , verbose = self . verbose ) fc_l = feature_classification ( probe_priv_lower , probe_priv_upper , rb_l_norm , verbose = self . verbose ) fc_both = np . concatenate ( [ fc , fc_l ] ) return interval_ , fc_both def get_normalized_intervals ( self , presetModel = None ) : # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO : handle other data formats d = X . shape [ 1 ] # Depending on the preset model , we dont need to compute all bounds # e . g . in the case of fixed features we skip those dims = _get_necessary_dimensions ( d , presetModel ) with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : relevance_bounds = self . compute_relevance_bounds ( dims , parallel = parallel , presetModel = presetModel ) probe_values_upper = self . compute_probe_values ( dims , isUpper = True , parallel = parallel , presetModel = presetModel ) probe_values_lower = self . compute_probe_values ( dims , isUpper = False , parallel = parallel , presetModel = presetModel ) # Postprocess bounds norm_bounds = self . _postprocessing ( self . best_init_model . L1_factor , relevance_bounds ) norm_probe_values_upper = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_upper ) norm_probe_values_lower = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_lower ) feature_classes = feature_classification ( norm_probe_values_lower , norm_probe_values_upper , norm_bounds , verbose = self . verbose , ) return norm_bounds , feature_classes def compute_relevance_bounds ( self , dims , parallel = None , presetModel = None , solverargs = None ) : init_model_state = self . best_init_model . model_state work_queue = self . _generate_relevance_bounds_tasks ( dims , self . data , presetModel , init_model_state ) # Solve relevance bounds in parallel ( when available ) if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) bound_results = parallel ( map ( joblib . delayed ( _start_solver_worker ) , work_queue )) # Retrieve results and aggregate values in dict solved_bounds = defaultdict ( list ) for finished_bound in bound_results : # Only add bounds with feasible solutions if finished_bound . is_solved : solved_bounds [ finished_bound . current_feature ]. append ( finished_bound ) # Initalize array for pair of bounds ( = intervals ) length = len ( dims ) intervals = np . zeros (( length , 2 )) for abs_index , rel_index in zip ( dims , range ( length )) : # Return interval for feature i ( can be a fixed value when set beforehand ) interval_i = self . _create_interval ( abs_index , solved_bounds , presetModel ) intervals [ rel_index ] = interval_i return intervals # TODO : add model model_state ( omega , bias ) to return value def compute_probe_values ( self , dims , isUpper = True , parallel = None , presetModel = None ) : # Get model parameters init_model_state = self . best_init_model . model_state # Prepare parallel framework if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) # Generate probe_queue = self . _generate_probe_value_tasks ( self . data , dims , isUpper , self . n_resampling , self . random_state , presetModel , init_model_state , ) # Compute solution probe_results = parallel ( map ( joblib . delayed ( _start_solver_worker ) , probe_queue )) # probe_values . extend ( [ probe . objective . value for probe in probe_results if probe . is_solved ] ) candidates = defaultdict ( list ) for candidate in probe_results : # Only add bounds with feasible solutions if candidate . is_solved : candidates [ candidate . probeID ]. append ( candidate ) probe_values = [] for probes_for_ID in candidates . values () : if isUpper : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( probes_for_ID ) ) else : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( probes_for_ID ) ) return np . array ( probe_values ) def _generate_relevance_bounds_tasks ( self , dims , data , preset_model = None , best_model_state = None ) : # Do not compute bounds for fixed features if preset_model is not None : dims = [ di for di in dims if di not in preset_model ] # Instantiate objects for computation later for di in dims : # Add Lower Bound problem ( s ) to work list yield from self . problem_type . get_cvxproblem_template . generate_lower_bound_problem ( self . best_hyperparameters , self . init_constraints , best_model_state , data , di , preset_model , ) # Add problem ( s ) for Upper bound yield from self . problem_type . get_cvxproblem_template . generate_upper_bound_problem ( self . best_hyperparameters , self . init_constraints , best_model_state , data , di , preset_model , ) def _generate_probe_value_tasks ( self , data , dims , isUpper , n_resampling , random_state , preset_model = None , best_model_state = None , ) : if isUpper : factory = ( self . problem_type . get_cvxproblem_template . generate_upper_bound_problem ) else : factory = ( self . problem_type . get_cvxproblem_template . generate_lower_bound_problem ) # Random sample n_resampling shadow features by permuting real features and computing upper bound random_choice = random_state . choice ( a = dims , size = n_resampling ) # Instantiate objects for i , di in enumerate ( random_choice ) : data_perm = permutate_feature_in_data ( data , di , random_state ) # We only use upper bounds as probe features yield from factory ( self . best_hyperparameters , self . init_constraints , best_model_state , data_perm , di , preset_model , probeID = i , ) def _create_interval ( self , feature : int , solved_bounds : dict , presetModel : dict = None ) : # Return preset values for fixed features if presetModel is not None : if feature in presetModel : return presetModel [ feature ] all_bounds = solved_bounds [ feature ] min_problems_candidates = [ p for p in all_bounds if p . isLowerBound ] max_problems_candidates = [ p for p in all_bounds if not p . isLowerBound ] if len ( all_bounds ) < 2 : logging . error ( f \" (Some) relevance bounds for feature {feature} were not solved. \" ) raise Exception ( \" Infeasible bound(s). \" ) lower_bound = self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( min_problems_candidates ) upper_bound = self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( max_problems_candidates ) return lower_bound , upper_bound def compute_single_preset_relevance_bounds ( self , i : int , signed_preset_i : [ float , float ] ) : \"\"\" Method to run method once for one restricted feature Parameters ---------- i : restricted feature signed_preset_i : restricted range of feature i ( set before optimization = preset ) \"\"\" preset = { i : signed_preset_i } rangevector = self . compute_multi_preset_relevance_bounds ( preset ) return rangevector def compute_multi_preset_relevance_bounds ( self , preset , lupi_features = 0 ) : \"\"\" Method to run method with preset values Parameters ---------- lupi_features \"\"\" X , y = self . data # The user is working with normalized values while we compute them unscaled if self . normalize : for k , v in preset . items () : preset [ k ] = np . asarray ( v ) * self . best_init_model . L1_factor # Add sign to presets preset = self . _add_sign_to_preset ( preset ) # Calculate all bounds with feature i set to min_i if lupi_features > 0 : rangevector , f_classes = self . get_normalized_lupi_intervals ( lupi_features , presetModel = preset ) else : rangevector , f_classes = self . get_normalized_intervals ( presetModel = preset ) return rangevector def _add_sign_to_preset ( self , unsigned_presets ) : \"\"\" We need signed presets for our convex problem definition later . We reuse the coefficients of the optimal model for this Parameters ---------- unsigned_presets : dict Returns ------- dict \"\"\" signed_presets = {} # Obtain optimal model parameters w = self . best_init_model . model_state [ \" w \" ] sum = 0 for i , preset in unsigned_presets . items () : unsigned_preset_i = np . sign ( w [ i ] ) * preset # accumulate maximal feature contribution sum += unsigned_preset_i [ 1 ] signed_presets [ i ] = unsigned_preset_i # Check if unsigned_presets makes sense l1 = self . init_constraints [ \" w_l1 \" ] if sum > l1 : print ( \" maximum L1 norm of presets: \" , sum ) print ( \" L1 allowed: \" , l1 ) print ( \" Presets are not feasible. Try lowering values. \" ) return return signed_presets def _postprocessing ( self , L1 , rangevector , round_to_zero = True ) : if self . normalize : assert L1 > 0 rangevector = rangevector . copy () / L1 if round_to_zero : rangevector [ rangevector <= 1 e - 11 ] = 0 return rangevector","title":"Compute"},{"location":"reference/fri/compute/#module-fricompute","text":"View Source import logging from collections import defaultdict import joblib import numpy as np from scipy import stats from fri.model.base_cvxproblem import Relevance_CVXProblem from fri.model.base_initmodel import InitModel from fri.model.base_type import ProblemType from fri.utils import permutate_feature_in_data MIN_N_PROBE_FEATURES = 20 # Lower bound of probe features def _start_solver_worker ( bound : Relevance_CVXProblem ): \"\"\" Worker thread method for parallel computation \"\"\" return bound . solve () class RelevanceBoundsIntervals ( object ): def __init__ ( self , data , problem_type : ProblemType , best_init_model : InitModel , random_state , n_resampling , n_jobs , verbose , normalize = True , ): self . data = data self . problem_type = problem_type self . verbose = verbose self . n_jobs = n_jobs self . n_resampling = n_resampling self . random_state = random_state self . best_init_model = best_init_model self . best_hyperparameters = best_init_model . hyperparam self . normalize = normalize # Relax constraints to improve stability relaxed_constraints = problem_type . get_relaxed_constraints ( best_init_model . constraints ) self . init_constraints = relaxed_constraints def get_normalized_lupi_intervals ( self , lupi_features , presetModel = None ): # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO: handle other data formats all_d = X . shape [ 1 ] normal_d = all_d - lupi_features # Compute relevance bounds and probes for normal features and LUPI with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : d_n = _get_necessary_dimensions ( normal_d , presetModel ) rb = self . compute_relevance_bounds ( d_n , parallel = parallel ) probe_upper = self . compute_probe_values ( d_n , True , parallel = parallel ) probe_lower = self . compute_probe_values ( d_n , False , parallel = parallel ) d_l = _get_necessary_dimensions ( all_d , presetModel , start = normal_d ) rb_l = self . compute_relevance_bounds ( d_l , parallel = parallel ) probe_priv_upper = self . compute_probe_values ( d_l , True , parallel = parallel ) probe_priv_lower = self . compute_probe_values ( d_l , False , parallel = parallel ) probes = [ probe_lower , probe_upper , probe_priv_lower , probe_priv_upper ] # # Postprocess # # Get Scaling Parameters l1 = self . init_constraints [ \"w_l1\" ] l1_priv = self . init_constraints [ \"w_priv_l1\" ] # Normalize Normal and Lupi features rb_norm = self . _postprocessing ( l1 , rb ) rb_l_norm = self . _postprocessing ( l1_priv , rb_l ) interval_ = np . concatenate ([ rb_norm , rb_l_norm ]) # Normalize Probes probe_lower = self . _postprocessing ( l1 , probe_lower ) probe_upper = self . _postprocessing ( l1 , probe_upper ) probe_priv_lower = self . _postprocessing ( l1_priv , probe_priv_lower ) probe_priv_upper = self . _postprocessing ( l1_priv , probe_priv_upper ) # # # Classify features fc = feature_classification ( probe_lower , probe_upper , rb_norm , verbose = self . verbose ) fc_l = feature_classification ( probe_priv_lower , probe_priv_upper , rb_l_norm , verbose = self . verbose ) fc_both = np . concatenate ([ fc , fc_l ]) return interval_ , fc_both def get_normalized_intervals ( self , presetModel = None ): # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO: handle other data formats d = X . shape [ 1 ] # Depending on the preset model, we dont need to compute all bounds # e.g. in the case of fixed features we skip those dims = _get_necessary_dimensions ( d , presetModel ) with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : relevance_bounds = self . compute_relevance_bounds ( dims , parallel = parallel , presetModel = presetModel ) probe_values_upper = self . compute_probe_values ( dims , isUpper = True , parallel = parallel , presetModel = presetModel ) probe_values_lower = self . compute_probe_values ( dims , isUpper = False , parallel = parallel , presetModel = presetModel ) # Postprocess bounds norm_bounds = self . _postprocessing ( self . best_init_model . L1_factor , relevance_bounds ) norm_probe_values_upper = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_upper ) norm_probe_values_lower = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_lower ) feature_classes = feature_classification ( norm_probe_values_lower , norm_probe_values_upper , norm_bounds , verbose = self . verbose , ) return norm_bounds , feature_classes def compute_relevance_bounds ( self , dims , parallel = None , presetModel = None , solverargs = None ): init_model_state = self . best_init_model . model_state work_queue = self . _generate_relevance_bounds_tasks ( dims , self . data , presetModel , init_model_state ) # Solve relevance bounds in parallel (when available) if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) bound_results = parallel ( map ( joblib . delayed ( _start_solver_worker ), work_queue )) # Retrieve results and aggregate values in dict solved_bounds = defaultdict ( list ) for finished_bound in bound_results : # Only add bounds with feasible solutions if finished_bound . is_solved : solved_bounds [ finished_bound . current_feature ] . append ( finished_bound ) # Initalize array for pair of bounds(= intervals) length = len ( dims ) intervals = np . zeros (( length , 2 )) for abs_index , rel_index in zip ( dims , range ( length )): # Return interval for feature i (can be a fixed value when set beforehand) interval_i = self . _create_interval ( abs_index , solved_bounds , presetModel ) intervals [ rel_index ] = interval_i return intervals # TODO: add model model_state (omega, bias) to return value def compute_probe_values ( self , dims , isUpper = True , parallel = None , presetModel = None ): # Get model parameters init_model_state = self . best_init_model . model_state # Prepare parallel framework if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) # Generate probe_queue = self . _generate_probe_value_tasks ( self . data , dims , isUpper , self . n_resampling , self . random_state , presetModel , init_model_state , ) # Compute solution probe_results = parallel ( map ( joblib . delayed ( _start_solver_worker ), probe_queue )) # probe_values.extend([probe.objective.value for probe in probe_results if probe.is_solved]) candidates = defaultdict ( list ) for candidate in probe_results : # Only add bounds with feasible solutions if candidate . is_solved : candidates [ candidate . probeID ] . append ( candidate ) probe_values = [] for probes_for_ID in candidates . values (): if isUpper : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( probes_for_ID ) ) else : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( probes_for_ID ) ) return np . array ( probe_values ) def _generate_relevance_bounds_tasks ( self , dims , data , preset_model = None , best_model_state = None ): # Do not compute bounds for fixed features if preset_model is not None : dims = [ di for di in dims if di not in preset_model ] # Instantiate objects for computation later for di in dims : # Add Lower Bound problem(s) to work list yield from self . problem_type . get_cvxproblem_template . generate_lower_bound_problem ( self . best_hyperparameters , self . init_constraints , best_model_state , data , di , preset_model , ) # Add problem(s) for Upper bound yield from self . problem_type . get_cvxproblem_template . generate_upper_bound_problem ( self . best_hyperparameters , self . init_constraints , best_model_state , data , di , preset_model , ) def _generate_probe_value_tasks ( self , data , dims , isUpper , n_resampling , random_state , preset_model = None , best_model_state = None , ): if isUpper : factory = ( self . problem_type . get_cvxproblem_template . generate_upper_bound_problem ) else : factory = ( self . problem_type . get_cvxproblem_template . generate_lower_bound_problem ) # Random sample n_resampling shadow features by permuting real features and computing upper bound random_choice = random_state . choice ( a = dims , size = n_resampling ) # Instantiate objects for i , di in enumerate ( random_choice ): data_perm = permutate_feature_in_data ( data , di , random_state ) # We only use upper bounds as probe features yield from factory ( self . best_hyperparameters , self . init_constraints , best_model_state , data_perm , di , preset_model , probeID = i , ) def _create_interval ( self , feature : int , solved_bounds : dict , presetModel : dict = None ): # Return preset values for fixed features if presetModel is not None : if feature in presetModel : return presetModel [ feature ] all_bounds = solved_bounds [ feature ] min_problems_candidates = [ p for p in all_bounds if p . isLowerBound ] max_problems_candidates = [ p for p in all_bounds if not p . isLowerBound ] if len ( all_bounds ) < 2 : logging . error ( f \"(Some) relevance bounds for feature {feature} were not solved.\" ) raise Exception ( \"Infeasible bound(s).\" ) lower_bound = self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( min_problems_candidates ) upper_bound = self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( max_problems_candidates ) return lower_bound , upper_bound def compute_single_preset_relevance_bounds ( self , i : int , signed_preset_i : [ float , float ] ): \"\"\" Method to run method once for one restricted feature Parameters ---------- i: restricted feature signed_preset_i: restricted range of feature i (set before optimization = preset) \"\"\" preset = { i : signed_preset_i } rangevector = self . compute_multi_preset_relevance_bounds ( preset ) return rangevector def compute_multi_preset_relevance_bounds ( self , preset , lupi_features = 0 ): \"\"\" Method to run method with preset values Parameters ---------- lupi_features \"\"\" X , y = self . data # The user is working with normalized values while we compute them unscaled if self . normalize : for k , v in preset . items (): preset [ k ] = np . asarray ( v ) * self . best_init_model . L1_factor # Add sign to presets preset = self . _add_sign_to_preset ( preset ) # Calculate all bounds with feature i set to min_i if lupi_features > 0 : rangevector , f_classes = self . get_normalized_lupi_intervals ( lupi_features , presetModel = preset ) else : rangevector , f_classes = self . get_normalized_intervals ( presetModel = preset ) return rangevector def _add_sign_to_preset ( self , unsigned_presets ): \"\"\" We need signed presets for our convex problem definition later. We reuse the coefficients of the optimal model for this Parameters ---------- unsigned_presets : dict Returns ------- dict \"\"\" signed_presets = {} # Obtain optimal model parameters w = self . best_init_model . model_state [ \"w\" ] sum = 0 for i , preset in unsigned_presets . items (): unsigned_preset_i = np . sign ( w [ i ]) * preset # accumulate maximal feature contribution sum += unsigned_preset_i [ 1 ] signed_presets [ i ] = unsigned_preset_i # Check if unsigned_presets makes sense l1 = self . init_constraints [ \"w_l1\" ] if sum > l1 : print ( \"maximum L1 norm of presets: \" , sum ) print ( \"L1 allowed:\" , l1 ) print ( \"Presets are not feasible. Try lowering values.\" ) return return signed_presets def _postprocessing ( self , L1 , rangevector , round_to_zero = True ): if self . normalize : assert L1 > 0 rangevector = rangevector . copy () / L1 if round_to_zero : rangevector [ rangevector <= 1e-11 ] = 0 return rangevector def _get_necessary_dimensions ( d : int , presetModel : dict = None , start = 0 ): dims = np . arange ( start , d ) # if presetModel is not None: # # Exclude fixed (preset) dimensions from being redundantly computed # dims = [di for di in dims if di not in presetModel.keys()] # TODO: check the removal of this block return dims def feature_classification ( probes_low , probes_up , relevance_bounds , fpr = 1e-4 , verbose = 0 ): logging . debug ( \"**** Feature Selection ****\" ) logging . debug ( \"Generating Lower Probe Statistic\" ) lower_stat = create_probe_statistic ( probes_low , fpr , verbose = verbose ) logging . debug ( \"Generating Upper Probe Statistic\" ) upper_stat = create_probe_statistic ( probes_up , fpr , verbose = verbose ) weakly = relevance_bounds [:, 1 ] > upper_stat [ 1 ] strongly = relevance_bounds [:, 0 ] > lower_stat [ 1 ] both = np . logical_and ( weakly , strongly ) prediction = np . zeros ( relevance_bounds . shape [ 0 ], dtype = np . int ) prediction [ weakly ] = 1 prediction [ both ] = 2 return prediction def create_probe_statistic ( probe_values , fpr , verbose = 0 ): # Create prediction interval statistics based on randomly permutated probe features (based on real features) n = len ( probe_values ) if n == 0 : if verbose > 0 : logging . info ( \"All probes were infeasible. All features considered relevant.\" ) # # If all probes were infeasible we expect an empty list # # If they are infeasible it also means that only strongly relevant features were in the data # # As such we just set the prediction without considering the statistics mean = 0 else : probe_values = np . asarray ( probe_values ) mean = probe_values . mean () if mean == 0 : lower_threshold , upper_threshold = mean , mean s = 0 else : s = probe_values . std () lower_threshold = mean + stats . t ( df = n - 1 ) . ppf ( fpr ) * s * np . sqrt ( 1 + ( 1 / n )) upper_threshold = mean - stats . t ( df = n - 1 ) . ppf ( fpr ) * s * np . sqrt ( 1 + ( 1 / n )) if verbose > 0 : print ( f \"FS threshold: {lower_threshold}-{upper_threshold}, Mean:{mean}, Std:{s}, n_probes {n}\" ) return lower_threshold , upper_threshold","title":"Module fri.compute"},{"location":"reference/fri/compute/#functions","text":"","title":"Functions"},{"location":"reference/fri/compute/#create_probe_statistic","text":"def ( probe_values , fpr , verbose = 0 ) View Source def create_probe_statistic ( probe_values , fpr , verbose = 0 ) : # Create prediction interval statistics based on randomly permutated probe features ( based on real features ) n = len ( probe_values ) if n == 0 : if verbose > 0 : logging . info ( \" All probes were infeasible. All features considered relevant. \" ) # # If all probes were infeasible we expect an empty list # # If they are infeasible it also means that only strongly relevant features were in the data # # As such we just set the prediction without considering the statistics mean = 0 else : probe_values = np . asarray ( probe_values ) mean = probe_values . mean () if mean == 0 : lower_threshold , upper_threshold = mean , mean s = 0 else : s = probe_values . std () lower_threshold = mean + stats . t ( df = n - 1 ) . ppf ( fpr ) * s * np . sqrt ( 1 + ( 1 / n )) upper_threshold = mean - stats . t ( df = n - 1 ) . ppf ( fpr ) * s * np . sqrt ( 1 + ( 1 / n )) if verbose > 0 : print ( f \" FS threshold: {lower_threshold}-{upper_threshold}, Mean:{mean}, Std:{s}, n_probes {n} \" ) return lower_threshold , upper_threshold","title":"create_probe_statistic"},{"location":"reference/fri/compute/#feature_classification","text":"def ( probes_low , probes_up , relevance_bounds , fpr = 0.0001 , verbose = 0 ) View Source def feature_classification ( probes_low , probes_up , relevance_bounds , fpr = 1 e - 4 , verbose = 0 ) : logging . debug ( \" **** Feature Selection **** \" ) logging . debug ( \" Generating Lower Probe Statistic \" ) lower_stat = create_probe_statistic ( probes_low , fpr , verbose = verbose ) logging . debug ( \" Generating Upper Probe Statistic \" ) upper_stat = create_probe_statistic ( probes_up , fpr , verbose = verbose ) weakly = relevance_bounds [:, 1 ] > upper_stat [ 1 ] strongly = relevance_bounds [:, 0 ] > lower_stat [ 1 ] both = np . logical_and ( weakly , strongly ) prediction = np . zeros ( relevance_bounds . shape [ 0 ], dtype = np . int ) prediction [ weakly ] = 1 prediction [ both ] = 2 return prediction","title":"feature_classification"},{"location":"reference/fri/compute/#classes","text":"","title":"Classes"},{"location":"reference/fri/compute/#relevanceboundsintervals","text":"class ( data , problem_type : fri . model . base_type . ProblemType , best_init_model : fri . model . base_initmodel . InitModel , random_state , n_resampling , n_jobs , verbose , normalize = True )","title":"RelevanceBoundsIntervals"},{"location":"reference/fri/compute/#methods","text":"##### compute_multi_preset_relevance_bounds ``` python3 def ( self , preset , lupi_features = 0 ) ``` Method to run method with preset values Parameters ---------- lupi_features ??? example \" View Source \" def compute_multi_preset_relevance_bounds ( self , preset , lupi_features = 0 ) : \"\"\" Method to run method with preset values Parameters ---------- lupi_features \"\"\" X , y = self . data # The user is working with normalized values while we compute them unscaled if self . normalize : for k , v in preset . items () : preset [ k ] = np . asarray ( v ) * self . best_init_model . L1_factor # Add sign to presets preset = self . _add_sign_to_preset ( preset ) # Calculate all bounds with feature i set to min_i if lupi_features > 0 : rangevector , f_classes = self . get_normalized_lupi_intervals ( lupi_features , presetModel = preset ) else : rangevector , f_classes = self . get_normalized_intervals ( presetModel = preset ) return rangevector ##### compute_probe_values ``` python3 def ( self , dims , isUpper = True , parallel = None , presetModel = None ) ``` ??? example \" View Source \" def compute_probe_values ( self , dims , isUpper = True , parallel = None , presetModel = None ) : # Get model parameters init_model_state = self . best_init_model . model_state # Prepare parallel framework if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) # Generate probe_queue = self . _generate_probe_value_tasks ( self . data , dims , isUpper , self . n_resampling , self . random_state , presetModel , init_model_state , ) # Compute solution probe_results = parallel ( map ( joblib . delayed ( _start_solver_worker ) , probe_queue )) # probe_values . extend ( [ probe . objective . value for probe in probe_results if probe . is_solved ] ) candidates = defaultdict ( list ) for candidate in probe_results : # Only add bounds with feasible solutions if candidate . is_solved : candidates [ candidate . probeID ]. append ( candidate ) probe_values = [] for probes_for_ID in candidates . values () : if isUpper : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( probes_for_ID ) ) else : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( probes_for_ID ) ) return np . array ( probe_values ) ##### compute_relevance_bounds ``` python3 def ( self , dims , parallel = None , presetModel = None , solverargs = None ) ``` ??? example \" View Source \" def compute_relevance_bounds ( self , dims , parallel = None , presetModel = None , solverargs = None ) : init_model_state = self . best_init_model . model_state work_queue = self . _generate_relevance_bounds_tasks ( dims , self . data , presetModel , init_model_state ) # Solve relevance bounds in parallel ( when available ) if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) bound_results = parallel ( map ( joblib . delayed ( _start_solver_worker ) , work_queue )) # Retrieve results and aggregate values in dict solved_bounds = defaultdict ( list ) for finished_bound in bound_results : # Only add bounds with feasible solutions if finished_bound . is_solved : solved_bounds [ finished_bound . current_feature ]. append ( finished_bound ) # Initalize array for pair of bounds ( = intervals ) length = len ( dims ) intervals = np . zeros (( length , 2 )) for abs_index , rel_index in zip ( dims , range ( length )) : # Return interval for feature i ( can be a fixed value when set beforehand ) interval_i = self . _create_interval ( abs_index , solved_bounds , presetModel ) intervals [ rel_index ] = interval_i return intervals # TODO : add model model_state ( omega , bias ) to return value ##### compute_single_preset_relevance_bounds ``` python3 def ( self , i : int , signed_preset_i : [ < class ' float ' > , < class ' float ' > ] ) ``` Method to run method once for one restricted feature Parameters ---------- i : restricted feature signed_preset_i : restricted range of feature i ( set before optimization = preset ) ??? example \" View Source \" def compute_single_preset_relevance_bounds ( self , i : int , signed_preset_i : [ float , float ] ) : \"\"\" Method to run method once for one restricted feature Parameters ---------- i : restricted feature signed_preset_i : restricted range of feature i ( set before optimization = preset ) \"\"\" preset = { i : signed_preset_i } rangevector = self . compute_multi_preset_relevance_bounds ( preset ) return rangevector ##### get_normalized_intervals ``` python3 def ( self , presetModel = None ) ``` ??? example \" View Source \" def get_normalized_intervals ( self , presetModel = None ) : # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO : handle other data formats d = X . shape [ 1 ] # Depending on the preset model , we dont need to compute all bounds # e . g . in the case of fixed features we skip those dims = _get_necessary_dimensions ( d , presetModel ) with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : relevance_bounds = self . compute_relevance_bounds ( dims , parallel = parallel , presetModel = presetModel ) probe_values_upper = self . compute_probe_values ( dims , isUpper = True , parallel = parallel , presetModel = presetModel ) probe_values_lower = self . compute_probe_values ( dims , isUpper = False , parallel = parallel , presetModel = presetModel ) # Postprocess bounds norm_bounds = self . _postprocessing ( self . best_init_model . L1_factor , relevance_bounds ) norm_probe_values_upper = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_upper ) norm_probe_values_lower = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_lower ) feature_classes = feature_classification ( norm_probe_values_lower , norm_probe_values_upper , norm_bounds , verbose = self . verbose , ) return norm_bounds , feature_classes ##### get_normalized_lupi_intervals ``` python3 def ( self , lupi_features , presetModel = None ) ``` ??? example \" View Source \" def get_normalized_lupi_intervals ( self , lupi_features , presetModel = None ) : # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO : handle other data formats all_d = X . shape [ 1 ] normal_d = all_d - lupi_features # Compute relevance bounds and probes for normal features and LUPI with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : d_n = _get_necessary_dimensions ( normal_d , presetModel ) rb = self . compute_relevance_bounds ( d_n , parallel = parallel ) probe_upper = self . compute_probe_values ( d_n , True , parallel = parallel ) probe_lower = self . compute_probe_values ( d_n , False , parallel = parallel ) d_l = _get_necessary_dimensions ( all_d , presetModel , start = normal_d ) rb_l = self . compute_relevance_bounds ( d_l , parallel = parallel ) probe_priv_upper = self . compute_probe_values ( d_l , True , parallel = parallel ) probe_priv_lower = self . compute_probe_values ( d_l , False , parallel = parallel ) probes = [ probe_lower , probe_upper , probe_priv_lower , probe_priv_upper ] # # Postprocess # # Get Scaling Parameters l1 = self . init_constraints [ \" w_l1 \" ] l1_priv = self . init_constraints [ \" w_priv_l1 \" ] # Normalize Normal and Lupi features rb_norm = self . _postprocessing ( l1 , rb ) rb_l_norm = self . _postprocessing ( l1_priv , rb_l ) interval_ = np . concatenate ( [ rb_norm , rb_l_norm ] ) # Normalize Probes probe_lower = self . _postprocessing ( l1 , probe_lower ) probe_upper = self . _postprocessing ( l1 , probe_upper ) probe_priv_lower = self . _postprocessing ( l1_priv , probe_priv_lower ) probe_priv_upper = self . _postprocessing ( l1_priv , probe_priv_upper ) # # # Classify features fc = feature_classification ( probe_lower , probe_upper , rb_norm , verbose = self . verbose ) fc_l = feature_classification ( probe_priv_lower , probe_priv_upper , rb_l_norm , verbose = self . verbose ) fc_both = np . concatenate ( [ fc , fc_l ] ) return interval_ , fc_both View Source class RelevanceBoundsIntervals ( object ) : def __init__ ( self , data , problem_type : ProblemType , best_init_model : InitModel , random_state , n_resampling , n_jobs , verbose , normalize = True , ) : self . data = data self . problem_type = problem_type self . verbose = verbose self . n_jobs = n_jobs self . n_resampling = n_resampling self . random_state = random_state self . best_init_model = best_init_model self . best_hyperparameters = best_init_model . hyperparam self . normalize = normalize # Relax constraints to improve stability relaxed_constraints = problem_type . get_relaxed_constraints ( best_init_model . constraints ) self . init_constraints = relaxed_constraints def get_normalized_lupi_intervals ( self , lupi_features , presetModel = None ) : # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO : handle other data formats all_d = X . shape [ 1 ] normal_d = all_d - lupi_features # Compute relevance bounds and probes for normal features and LUPI with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : d_n = _get_necessary_dimensions ( normal_d , presetModel ) rb = self . compute_relevance_bounds ( d_n , parallel = parallel ) probe_upper = self . compute_probe_values ( d_n , True , parallel = parallel ) probe_lower = self . compute_probe_values ( d_n , False , parallel = parallel ) d_l = _get_necessary_dimensions ( all_d , presetModel , start = normal_d ) rb_l = self . compute_relevance_bounds ( d_l , parallel = parallel ) probe_priv_upper = self . compute_probe_values ( d_l , True , parallel = parallel ) probe_priv_lower = self . compute_probe_values ( d_l , False , parallel = parallel ) probes = [ probe_lower , probe_upper , probe_priv_lower , probe_priv_upper ] # # Postprocess # # Get Scaling Parameters l1 = self . init_constraints [ \" w_l1 \" ] l1_priv = self . init_constraints [ \" w_priv_l1 \" ] # Normalize Normal and Lupi features rb_norm = self . _postprocessing ( l1 , rb ) rb_l_norm = self . _postprocessing ( l1_priv , rb_l ) interval_ = np . concatenate ( [ rb_norm , rb_l_norm ] ) # Normalize Probes probe_lower = self . _postprocessing ( l1 , probe_lower ) probe_upper = self . _postprocessing ( l1 , probe_upper ) probe_priv_lower = self . _postprocessing ( l1_priv , probe_priv_lower ) probe_priv_upper = self . _postprocessing ( l1_priv , probe_priv_upper ) # # # Classify features fc = feature_classification ( probe_lower , probe_upper , rb_norm , verbose = self . verbose ) fc_l = feature_classification ( probe_priv_lower , probe_priv_upper , rb_l_norm , verbose = self . verbose ) fc_both = np . concatenate ( [ fc , fc_l ] ) return interval_ , fc_both def get_normalized_intervals ( self , presetModel = None ) : # We define a list of all the features we want to compute relevance bounds for X , _ = self . data # TODO : handle other data formats d = X . shape [ 1 ] # Depending on the preset model , we dont need to compute all bounds # e . g . in the case of fixed features we skip those dims = _get_necessary_dimensions ( d , presetModel ) with joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) as parallel : relevance_bounds = self . compute_relevance_bounds ( dims , parallel = parallel , presetModel = presetModel ) probe_values_upper = self . compute_probe_values ( dims , isUpper = True , parallel = parallel , presetModel = presetModel ) probe_values_lower = self . compute_probe_values ( dims , isUpper = False , parallel = parallel , presetModel = presetModel ) # Postprocess bounds norm_bounds = self . _postprocessing ( self . best_init_model . L1_factor , relevance_bounds ) norm_probe_values_upper = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_upper ) norm_probe_values_lower = self . _postprocessing ( self . best_init_model . L1_factor , probe_values_lower ) feature_classes = feature_classification ( norm_probe_values_lower , norm_probe_values_upper , norm_bounds , verbose = self . verbose , ) return norm_bounds , feature_classes def compute_relevance_bounds ( self , dims , parallel = None , presetModel = None , solverargs = None ) : init_model_state = self . best_init_model . model_state work_queue = self . _generate_relevance_bounds_tasks ( dims , self . data , presetModel , init_model_state ) # Solve relevance bounds in parallel ( when available ) if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) bound_results = parallel ( map ( joblib . delayed ( _start_solver_worker ) , work_queue )) # Retrieve results and aggregate values in dict solved_bounds = defaultdict ( list ) for finished_bound in bound_results : # Only add bounds with feasible solutions if finished_bound . is_solved : solved_bounds [ finished_bound . current_feature ]. append ( finished_bound ) # Initalize array for pair of bounds ( = intervals ) length = len ( dims ) intervals = np . zeros (( length , 2 )) for abs_index , rel_index in zip ( dims , range ( length )) : # Return interval for feature i ( can be a fixed value when set beforehand ) interval_i = self . _create_interval ( abs_index , solved_bounds , presetModel ) intervals [ rel_index ] = interval_i return intervals # TODO : add model model_state ( omega , bias ) to return value def compute_probe_values ( self , dims , isUpper = True , parallel = None , presetModel = None ) : # Get model parameters init_model_state = self . best_init_model . model_state # Prepare parallel framework if parallel is None : parallel = joblib . Parallel ( n_jobs = self . n_jobs , verbose = self . verbose ) # Generate probe_queue = self . _generate_probe_value_tasks ( self . data , dims , isUpper , self . n_resampling , self . random_state , presetModel , init_model_state , ) # Compute solution probe_results = parallel ( map ( joblib . delayed ( _start_solver_worker ) , probe_queue )) # probe_values . extend ( [ probe . objective . value for probe in probe_results if probe . is_solved ] ) candidates = defaultdict ( list ) for candidate in probe_results : # Only add bounds with feasible solutions if candidate . is_solved : candidates [ candidate . probeID ]. append ( candidate ) probe_values = [] for probes_for_ID in candidates . values () : if isUpper : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( probes_for_ID ) ) else : probe_values . append ( self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( probes_for_ID ) ) return np . array ( probe_values ) def _generate_relevance_bounds_tasks ( self , dims , data , preset_model = None , best_model_state = None ) : # Do not compute bounds for fixed features if preset_model is not None : dims = [ di for di in dims if di not in preset_model ] # Instantiate objects for computation later for di in dims : # Add Lower Bound problem ( s ) to work list yield from self . problem_type . get_cvxproblem_template . generate_lower_bound_problem ( self . best_hyperparameters , self . init_constraints , best_model_state , data , di , preset_model , ) # Add problem ( s ) for Upper bound yield from self . problem_type . get_cvxproblem_template . generate_upper_bound_problem ( self . best_hyperparameters , self . init_constraints , best_model_state , data , di , preset_model , ) def _generate_probe_value_tasks ( self , data , dims , isUpper , n_resampling , random_state , preset_model = None , best_model_state = None , ) : if isUpper : factory = ( self . problem_type . get_cvxproblem_template . generate_upper_bound_problem ) else : factory = ( self . problem_type . get_cvxproblem_template . generate_lower_bound_problem ) # Random sample n_resampling shadow features by permuting real features and computing upper bound random_choice = random_state . choice ( a = dims , size = n_resampling ) # Instantiate objects for i , di in enumerate ( random_choice ) : data_perm = permutate_feature_in_data ( data , di , random_state ) # We only use upper bounds as probe features yield from factory ( self . best_hyperparameters , self . init_constraints , best_model_state , data_perm , di , preset_model , probeID = i , ) def _create_interval ( self , feature : int , solved_bounds : dict , presetModel : dict = None ) : # Return preset values for fixed features if presetModel is not None : if feature in presetModel : return presetModel [ feature ] all_bounds = solved_bounds [ feature ] min_problems_candidates = [ p for p in all_bounds if p . isLowerBound ] max_problems_candidates = [ p for p in all_bounds if not p . isLowerBound ] if len ( all_bounds ) < 2 : logging . error ( f \" (Some) relevance bounds for feature {feature} were not solved. \" ) raise Exception ( \" Infeasible bound(s). \" ) lower_bound = self . problem_type . get_cvxproblem_template . aggregate_min_candidates ( min_problems_candidates ) upper_bound = self . problem_type . get_cvxproblem_template . aggregate_max_candidates ( max_problems_candidates ) return lower_bound , upper_bound def compute_single_preset_relevance_bounds ( self , i : int , signed_preset_i : [ float , float ] ) : \"\"\" Method to run method once for one restricted feature Parameters ---------- i : restricted feature signed_preset_i : restricted range of feature i ( set before optimization = preset ) \"\"\" preset = { i : signed_preset_i } rangevector = self . compute_multi_preset_relevance_bounds ( preset ) return rangevector def compute_multi_preset_relevance_bounds ( self , preset , lupi_features = 0 ) : \"\"\" Method to run method with preset values Parameters ---------- lupi_features \"\"\" X , y = self . data # The user is working with normalized values while we compute them unscaled if self . normalize : for k , v in preset . items () : preset [ k ] = np . asarray ( v ) * self . best_init_model . L1_factor # Add sign to presets preset = self . _add_sign_to_preset ( preset ) # Calculate all bounds with feature i set to min_i if lupi_features > 0 : rangevector , f_classes = self . get_normalized_lupi_intervals ( lupi_features , presetModel = preset ) else : rangevector , f_classes = self . get_normalized_intervals ( presetModel = preset ) return rangevector def _add_sign_to_preset ( self , unsigned_presets ) : \"\"\" We need signed presets for our convex problem definition later . We reuse the coefficients of the optimal model for this Parameters ---------- unsigned_presets : dict Returns ------- dict \"\"\" signed_presets = {} # Obtain optimal model parameters w = self . best_init_model . model_state [ \" w \" ] sum = 0 for i , preset in unsigned_presets . items () : unsigned_preset_i = np . sign ( w [ i ] ) * preset # accumulate maximal feature contribution sum += unsigned_preset_i [ 1 ] signed_presets [ i ] = unsigned_preset_i # Check if unsigned_presets makes sense l1 = self . init_constraints [ \" w_l1 \" ] if sum > l1 : print ( \" maximum L1 norm of presets: \" , sum ) print ( \" L1 allowed: \" , l1 ) print ( \" Presets are not feasible. Try lowering values. \" ) return return signed_presets def _postprocessing ( self , L1 , rangevector , round_to_zero = True ) : if self . normalize : assert L1 > 0 rangevector = rangevector . copy () / L1 if round_to_zero : rangevector [ rangevector <= 1 e - 11 ] = 0 return rangevector","title":"Methods"},{"location":"reference/fri/main/","text":"Module fri.main View Source from sklearn.base import BaseEstimator from sklearn.exceptions import NotFittedError from sklearn.feature_selection.base import SelectorMixin from sklearn.utils import check_random_state from sklearn.utils.validation import check_is_fitted from fri.compute import RelevanceBoundsIntervals from fri.model.base_type import ProblemType from fri.parameter_searcher import find_best_model RELEVANCE_MAPPING = { 0 : \"Irrelevant\" , 1 : \"Weak relevant\" , 2 : \"Strong relevant\" } class NotFeasibleForParameters ( Exception ): \"\"\" Problem was infeasible with the current parameter set. \"\"\" class FRIBase ( BaseEstimator , SelectorMixin ): def __init__ ( self , problem_type : ProblemType , random_state = None , n_jobs = 1 , verbose = 0 , n_param_search = 30 , n_probe_features = 40 , normalize = True , ** kwargs , ): \"\"\" Parameters ---------- problem_type : abc.ABCMeta random_state : Union[mtrand.RandomState, int, None, None, None, None, None, None, None] n_jobs : int verbose : int n_param_search : int n_probe_features : int normalize : bool kwargs : Attributes ---------- interval_ : array-like Feature relevance Intervals optim_model_ : `InitModel` Baseline model fitted on data relevance_classes_ : list(int) Classes of relevance encoded as int: 0 irrelevant, 1 weakly relevant, 2 strongly relevant relevance_classes_string_ : list(str) Classes of relevance encoded as string allrel_prediction_ : list(int) Relevance prediction encoded as boolean: 0 irrelevant, 1 relevant \"\"\" self . n_probe_features = n_probe_features self . n_param_search = n_param_search # assert issubclass(problem_type, ProblemType) self . problem_type_ = problem_type ( ** kwargs ) self . random_state = check_random_state ( random_state ) self . n_jobs = n_jobs self . verbose = verbose self . normalize = normalize self . interval_ = None self . optim_model_ = None self . relevance_classes_ = None self . relevance_classes_string_ = None self . allrel_prediction_ = None def fit ( self , X , y , lupi_features = 0 , ** kwargs ): \"\"\" Method to fit model on data. Parameters ---------- X : numpy.ndarray y : numpy.ndarray lupi_features : int Amount of features which are considered privileged information in `X`. The data is expected to be structured in a way that all lupi features are at the end of the set. For example `lupi_features=1` would denote the last column of `X` to be privileged. kwargs : dict Dictionary of additional keyword arguments depending on the `model`. Returns ------- `FRIBase` \"\"\" self . lupi_features_ = lupi_features self . n_samples_ = X . shape [ 0 ] self . n_features_ = X . shape [ 1 ] - lupi_features self . optim_model_ , best_score = self . _fit_baseline ( X , y , lupi_features , ** kwargs ) data = self . problem_type_ . preprocessing (( X , y ), lupi_features = lupi_features ) self . _relevance_bounds_computer = RelevanceBoundsIntervals ( data , self . problem_type_ , self . optim_model_ , self . random_state , self . n_probe_features , self . n_jobs , self . verbose , normalize = self . normalize , ) if lupi_features == 0 : self . interval_ , feature_classes = ( self . _relevance_bounds_computer . get_normalized_intervals () ) else : self . interval_ , feature_classes = self . _relevance_bounds_computer . get_normalized_lupi_intervals ( lupi_features = lupi_features ) self . _get_relevance_mask ( feature_classes ) # Return the classifier return self def _fit_baseline ( self , X , y , lupi_features = 0 , ** kwargs ): # Preprocessing data = self . problem_type_ . preprocessing (( X , y ), lupi_features = lupi_features ) # Get predefined template for our init. model init_model_template = self . problem_type_ . get_initmodel_template # Get hyperparameters which are predefined to our model template and can be seleted by user choice hyperparameters = self . problem_type_ . get_all_parameters () # search_samples = len(hyperparameters) * self.n_param_search # TODO: remove this search_samples = self . n_param_search # Find an optimal, fitted model using hyperparemeter search optimal_model , best_score = find_best_model ( init_model_template , hyperparameters , data , self . random_state , search_samples , self . n_jobs , self . verbose , lupi_features = lupi_features , ** kwargs , ) return optimal_model , best_score def _get_relevance_mask ( self , prediction ): \"\"\"Determines relevancy using feature relevance interval values Parameters ---------- fpr : float, optional false positive rate allowed under H_0 Returns ------- boolean array Relevancy prediction for each feature \"\"\" self . relevance_classes_ = prediction self . relevance_classes_string_ = [ RELEVANCE_MAPPING [ p ] for p in prediction ] self . allrel_prediction_ = prediction > 0 self . allrel_prediction_nonpriv_ = self . allrel_prediction_ [: self . n_features_ ] self . allrel_prediction_priv_ = self . allrel_prediction_ [ self . n_features_ :] self . relevance_classes_nonpriv_ = self . relevance_classes_ [: self . n_features_ ] self . relevance_classes_priv_ = self . relevance_classes_ [ self . n_features_ :] return self . allrel_prediction_ def _n_selected_features ( self ): \"\"\" Returns the number of selected features. ------- \"\"\" check_is_fitted ( self , \"allrel_prediction_\" ) return sum ( self . allrel_prediction_ ) def _get_support_mask ( self ): \"\"\"Method for SelectorMixin Returns ------- boolean array \"\"\" return self . allrel_prediction_ def score ( self , X , y ): \"\"\" Using fitted model predict points for `X` and compare to truth `y`. Parameters ---------- X : numpy.ndarray y : numpy.ndarray Returns ------- Model specific score (0 is worst, 1 is best) \"\"\" if self . optim_model_ : return self . optim_model_ . score ( X , y ) else : raise NotFittedError () def constrained_intervals ( self , preset : dict ): \"\"\" Method to return relevance intervals which are constrained using preset ranges or values. Parameters ---------- preset : dict like, {i:float} or {i:[float,float]} Keys denote feature index, values represent a fixed single value (float) or a range of allowed values (lower and upper bound). Example: To set feature 0 to a fixed value use >>> preset = {0: 0.1} or to use the minimum relevance bound >>> preset[1] = self.interval_[1, 0] Returns ------- array like Relevance bounds with user constraints \"\"\" # Do we have intervals? check_is_fitted ( self , \"interval_\" ) return self . _relevance_bounds_computer . compute_multi_preset_relevance_bounds ( preset = preset , lupi_features = self . lupi_features_ ) def print_interval_with_class ( self ): \"\"\" Pretty print the relevance intervals and determined feature relevance class \"\"\" output = \"\" if self . interval_ is None : output += \"Model is not fitted.\" output += \"############## Relevance bounds ############## \\n \" output += \"feature: [LB -- UB], relevance class \\n \" for i in range ( self . n_features_ + self . lupi_features_ ): if i == self . n_features_ : output += \"########## LUPI Relevance bounds \\n \" output += ( f \"{i:7}: [{self.interval_[i, 0]:1.1f} -- {self.interval_[i, 1]:1.1f}],\" ) output += f \" {self.relevance_classes_string_[i]} \\n \" return output Classes FRIBase class ( problem_type : fri . model . base_type . ProblemType , random_state = None , n_jobs = 1 , verbose = 0 , n_param_search = 30 , n_probe_features = 40 , normalize = True , ** kwargs ) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Parameters problem_type : abc.ABCMeta random_state : Union[mtrand.RandomState, int, None, None, None, None, None, None, None] n_jobs : int verbose : int n_param_search : int n_probe_features : int normalize : bool kwargs : Attributes interval_ : array-like Feature relevance Intervals optim_model_ : InitModel Baseline model fitted on data relevance_classes_ : list(int) Classes of relevance encoded as int: 0 irrelevant, 1 weakly relevant, 2 strongly relevant relevance_classes_string_ : list(str) Classes of relevance encoded as string allrel_prediction_ : list(int) Relevance prediction encoded as boolean: 0 irrelevant, 1 relevant Ancestors (in MRO) sklearn.base.BaseEstimator sklearn.feature_selection.base.SelectorMixin sklearn.base.TransformerMixin Descendants fri.FRI Methods ##### constrained_intervals ``` python3 def ( self , preset : dict ) ``` Method to return relevance intervals which are constrained using preset ranges or values . Parameters ---------- preset : dict like , { i : float } or { i :[ float , float ]} Keys denote feature index , values represent a fixed single value ( float ) or a range of allowed values ( lower and upper bound ) . Example : To set feature 0 to a fixed value use >>> preset = { 0 : 0 . 1 } or to use the minimum relevance bound >>> preset [ 1 ] = self . interval_ [ 1 , 0 ] Returns ------- array like Relevance bounds with user constraints ??? example \" View Source \" def constrained_intervals ( self , preset : dict ) : \"\"\" Method to return relevance intervals which are constrained using preset ranges or values . Parameters ---------- preset : dict like , { i : float } or { i :[ float , float ]} Keys denote feature index , values represent a fixed single value ( float ) or a range of allowed values ( lower and upper bound ) . Example : To set feature 0 to a fixed value use >>> preset = { 0 : 0 . 1 } or to use the minimum relevance bound >>> preset [ 1 ] = self . interval_ [ 1 , 0 ] Returns ------- array like Relevance bounds with user constraints \"\"\" # Do we have intervals ? check_is_fitted ( self , \" interval_ \" ) return self . _relevance_bounds_computer . compute_multi_preset_relevance_bounds ( preset = preset , lupi_features = self . lupi_features_ ) ##### fit ``` python3 def ( self , X , y , lupi_features = 0 , ** kwargs ) ``` Method to fit model on data . Parameters ---------- X : numpy . ndarray y : numpy . ndarray lupi_features : int Amount of features which are considered privileged information in ` X `. The data is expected to be structured in a way that all lupi features are at the end of the set . For example ` lupi_features = 1 ` would denote the last column of ` X ` to be privileged . kwargs : dict Dictionary of additional keyword arguments depending on the ` model `. Returns ------- ` FRIBase ` ??? example \" View Source \" def fit ( self , X , y , lupi_features = 0 , ** kwargs ) : \"\"\" Method to fit model on data . Parameters ---------- X : numpy . ndarray y : numpy . ndarray lupi_features : int Amount of features which are considered privileged information in ` X `. The data is expected to be structured in a way that all lupi features are at the end of the set . For example ` lupi_features = 1 ` would denote the last column of ` X ` to be privileged . kwargs : dict Dictionary of additional keyword arguments depending on the ` model `. Returns ------- ` FRIBase ` \"\"\" self . lupi_features_ = lupi_features self . n_samples_ = X . shape [ 0 ] self . n_features_ = X . shape [ 1 ] - lupi_features self . optim_model_ , best_score = self . _fit_baseline ( X , y , lupi_features , ** kwargs ) data = self . problem_type_ . preprocessing (( X , y ) , lupi_features = lupi_features ) self . _relevance_bounds_computer = RelevanceBoundsIntervals ( data , self . problem_type_ , self . optim_model_ , self . random_state , self . n_probe_features , self . n_jobs , self . verbose , normalize = self . normalize , ) if lupi_features == 0 : self . interval_ , feature_classes = ( self . _relevance_bounds_computer . get_normalized_intervals () ) else : self . interval_ , feature_classes = self . _relevance_bounds_computer . get_normalized_lupi_intervals ( lupi_features = lupi_features ) self . _get_relevance_mask ( feature_classes ) # Return the classifier return self ##### print_interval_with_class ``` python3 def ( self ) ``` Pretty print the relevance intervals and determined feature relevance class ??? example \" View Source \" def print_interval_with_class ( self ) : \"\"\" Pretty print the relevance intervals and determined feature relevance class \"\"\" output = \"\" if self . interval_ is None : output += \" Model is not fitted. \" output += \" ############## Relevance bounds ############## \\n \" output += \" feature: [LB -- UB], relevance class \\n \" for i in range ( self . n_features_ + self . lupi_features_ ) : if i == self . n_features_ : output += \" ########## LUPI Relevance bounds \\n \" output += ( f \" {i:7}: [{self.interval_[i, 0]:1.1f} -- {self.interval_[i, 1]:1.1f}], \" ) output += f \" {self.relevance_classes_string_[i]} \\n \" return output ##### score ``` python3 def ( self , X , y ) ``` Using fitted model predict points for ` X ` and compare to truth ` y `. Parameters ---------- X : numpy . ndarray y : numpy . ndarray Returns ------- Model specific score ( 0 is worst , 1 is best ) ??? example \" View Source \" def score ( self , X , y ) : \"\"\" Using fitted model predict points for ` X ` and compare to truth ` y `. Parameters ---------- X : numpy . ndarray y : numpy . ndarray Returns ------- Model specific score ( 0 is worst , 1 is best ) \"\"\" if self . optim_model_ : return self . optim_model_ . score ( X , y ) else : raise NotFittedError () View Source class FRIBase ( BaseEstimator , SelectorMixin ) : def __init__ ( self , problem_type : ProblemType , random_state = None , n_jobs = 1 , verbose = 0 , n_param_search = 30 , n_probe_features = 40 , normalize = True , ** kwargs , ) : \"\"\" Parameters ---------- problem_type : abc . ABCMeta random_state : Union [ mtrand . RandomState , int , None , None , None , None , None , None , None ] n_jobs : int verbose : int n_param_search : int n_probe_features : int normalize : bool kwargs : Attributes ---------- interval_ : array - like Feature relevance Intervals optim_model_ : ` InitModel ` Baseline model fitted on data relevance_classes_ : list ( int ) Classes of relevance encoded as int : 0 irrelevant , 1 weakly relevant , 2 strongly relevant relevance_classes_string_ : list ( str ) Classes of relevance encoded as string allrel_prediction_ : list ( int ) Relevance prediction encoded as boolean : 0 irrelevant , 1 relevant \"\"\" self . n_probe_features = n_probe_features self . n_param_search = n_param_search # assert issubclass ( problem_type , ProblemType ) self . problem_type_ = problem_type ( ** kwargs ) self . random_state = check_random_state ( random_state ) self . n_jobs = n_jobs self . verbose = verbose self . normalize = normalize self . interval_ = None self . optim_model_ = None self . relevance_classes_ = None self . relevance_classes_string_ = None self . allrel_prediction_ = None def fit ( self , X , y , lupi_features = 0 , ** kwargs ) : \"\"\" Method to fit model on data . Parameters ---------- X : numpy . ndarray y : numpy . ndarray lupi_features : int Amount of features which are considered privileged information in ` X `. The data is expected to be structured in a way that all lupi features are at the end of the set . For example ` lupi_features = 1 ` would denote the last column of ` X ` to be privileged . kwargs : dict Dictionary of additional keyword arguments depending on the ` model `. Returns ------- ` FRIBase ` \"\"\" self . lupi_features_ = lupi_features self . n_samples_ = X . shape [ 0 ] self . n_features_ = X . shape [ 1 ] - lupi_features self . optim_model_ , best_score = self . _fit_baseline ( X , y , lupi_features , ** kwargs ) data = self . problem_type_ . preprocessing (( X , y ) , lupi_features = lupi_features ) self . _relevance_bounds_computer = RelevanceBoundsIntervals ( data , self . problem_type_ , self . optim_model_ , self . random_state , self . n_probe_features , self . n_jobs , self . verbose , normalize = self . normalize , ) if lupi_features == 0 : self . interval_ , feature_classes = ( self . _relevance_bounds_computer . get_normalized_intervals () ) else : self . interval_ , feature_classes = self . _relevance_bounds_computer . get_normalized_lupi_intervals ( lupi_features = lupi_features ) self . _get_relevance_mask ( feature_classes ) # Return the classifier return self def _fit_baseline ( self , X , y , lupi_features = 0 , ** kwargs ) : # Preprocessing data = self . problem_type_ . preprocessing (( X , y ) , lupi_features = lupi_features ) # Get predefined template for our init . model init_model_template = self . problem_type_ . get_initmodel_template # Get hyperparameters which are predefined to our model template and can be seleted by user choice hyperparameters = self . problem_type_ . get_all_parameters () # search_samples = len ( hyperparameters ) * self . n_param_search # TODO : remove this search_samples = self . n_param_search # Find an optimal , fitted model using hyperparemeter search optimal_model , best_score = find_best_model ( init_model_template , hyperparameters , data , self . random_state , search_samples , self . n_jobs , self . verbose , lupi_features = lupi_features , ** kwargs , ) return optimal_model , best_score def _get_relevance_mask ( self , prediction ) : \"\"\" Determines relevancy using feature relevance interval values Parameters ---------- fpr : float , optional false positive rate allowed under H_0 Returns ------- boolean array Relevancy prediction for each feature \"\"\" self . relevance_classes_ = prediction self . relevance_classes_string_ = [ RELEVANCE_MAPPING [ p ] for p in prediction ] self . allrel_prediction_ = prediction > 0 self . allrel_prediction_nonpriv_ = self . allrel_prediction_ [: self . n_features_ ] self . allrel_prediction_priv_ = self . allrel_prediction_ [ self . n_features_ :] self . relevance_classes_nonpriv_ = self . relevance_classes_ [: self . n_features_ ] self . relevance_classes_priv_ = self . relevance_classes_ [ self . n_features_ :] return self . allrel_prediction_ def _n_selected_features ( self ) : \"\"\" Returns the number of selected features . ------- \"\"\" check_is_fitted ( self , \" allrel_prediction_ \" ) return sum ( self . allrel_prediction_ ) def _get_support_mask ( self ) : \"\"\" Method for SelectorMixin Returns ------- boolean array \"\"\" return self . allrel_prediction_ def score ( self , X , y ) : \"\"\" Using fitted model predict points for ` X ` and compare to truth ` y `. Parameters ---------- X : numpy . ndarray y : numpy . ndarray Returns ------- Model specific score ( 0 is worst , 1 is best ) \"\"\" if self . optim_model_ : return self . optim_model_ . score ( X , y ) else : raise NotFittedError () def constrained_intervals ( self , preset : dict ) : \"\"\" Method to return relevance intervals which are constrained using preset ranges or values . Parameters ---------- preset : dict like , { i : float } or { i :[ float , float ]} Keys denote feature index , values represent a fixed single value ( float ) or a range of allowed values ( lower and upper bound ) . Example : To set feature 0 to a fixed value use >>> preset = { 0 : 0 . 1 } or to use the minimum relevance bound >>> preset [ 1 ] = self . interval_ [ 1 , 0 ] Returns ------- array like Relevance bounds with user constraints \"\"\" # Do we have intervals ? check_is_fitted ( self , \" interval_ \" ) return self . _relevance_bounds_computer . compute_multi_preset_relevance_bounds ( preset = preset , lupi_features = self . lupi_features_ ) def print_interval_with_class ( self ) : \"\"\" Pretty print the relevance intervals and determined feature relevance class \"\"\" output = \"\" if self . interval_ is None : output += \" Model is not fitted. \" output += \" ############## Relevance bounds ############## \\n \" output += \" feature: [LB -- UB], relevance class \\n \" for i in range ( self . n_features_ + self . lupi_features_ ) : if i == self . n_features_ : output += \" ########## LUPI Relevance bounds \\n \" output += ( f \" {i:7}: [{self.interval_[i, 0]:1.1f} -- {self.interval_[i, 1]:1.1f}], \" ) output += f \" {self.relevance_classes_string_[i]} \\n \" return output NotFeasibleForParameters class ( * args , ** kwargs ) Problem was infeasible with the current parameter set. Ancestors (in MRO) builtins.Exception builtins.BaseException View Source class NotFeasibleForParameters ( Exception ): \"\"\" Problem was infeasible with the current parameter set. \"\"\"","title":"Main"},{"location":"reference/fri/main/#module-frimain","text":"View Source from sklearn.base import BaseEstimator from sklearn.exceptions import NotFittedError from sklearn.feature_selection.base import SelectorMixin from sklearn.utils import check_random_state from sklearn.utils.validation import check_is_fitted from fri.compute import RelevanceBoundsIntervals from fri.model.base_type import ProblemType from fri.parameter_searcher import find_best_model RELEVANCE_MAPPING = { 0 : \"Irrelevant\" , 1 : \"Weak relevant\" , 2 : \"Strong relevant\" } class NotFeasibleForParameters ( Exception ): \"\"\" Problem was infeasible with the current parameter set. \"\"\" class FRIBase ( BaseEstimator , SelectorMixin ): def __init__ ( self , problem_type : ProblemType , random_state = None , n_jobs = 1 , verbose = 0 , n_param_search = 30 , n_probe_features = 40 , normalize = True , ** kwargs , ): \"\"\" Parameters ---------- problem_type : abc.ABCMeta random_state : Union[mtrand.RandomState, int, None, None, None, None, None, None, None] n_jobs : int verbose : int n_param_search : int n_probe_features : int normalize : bool kwargs : Attributes ---------- interval_ : array-like Feature relevance Intervals optim_model_ : `InitModel` Baseline model fitted on data relevance_classes_ : list(int) Classes of relevance encoded as int: 0 irrelevant, 1 weakly relevant, 2 strongly relevant relevance_classes_string_ : list(str) Classes of relevance encoded as string allrel_prediction_ : list(int) Relevance prediction encoded as boolean: 0 irrelevant, 1 relevant \"\"\" self . n_probe_features = n_probe_features self . n_param_search = n_param_search # assert issubclass(problem_type, ProblemType) self . problem_type_ = problem_type ( ** kwargs ) self . random_state = check_random_state ( random_state ) self . n_jobs = n_jobs self . verbose = verbose self . normalize = normalize self . interval_ = None self . optim_model_ = None self . relevance_classes_ = None self . relevance_classes_string_ = None self . allrel_prediction_ = None def fit ( self , X , y , lupi_features = 0 , ** kwargs ): \"\"\" Method to fit model on data. Parameters ---------- X : numpy.ndarray y : numpy.ndarray lupi_features : int Amount of features which are considered privileged information in `X`. The data is expected to be structured in a way that all lupi features are at the end of the set. For example `lupi_features=1` would denote the last column of `X` to be privileged. kwargs : dict Dictionary of additional keyword arguments depending on the `model`. Returns ------- `FRIBase` \"\"\" self . lupi_features_ = lupi_features self . n_samples_ = X . shape [ 0 ] self . n_features_ = X . shape [ 1 ] - lupi_features self . optim_model_ , best_score = self . _fit_baseline ( X , y , lupi_features , ** kwargs ) data = self . problem_type_ . preprocessing (( X , y ), lupi_features = lupi_features ) self . _relevance_bounds_computer = RelevanceBoundsIntervals ( data , self . problem_type_ , self . optim_model_ , self . random_state , self . n_probe_features , self . n_jobs , self . verbose , normalize = self . normalize , ) if lupi_features == 0 : self . interval_ , feature_classes = ( self . _relevance_bounds_computer . get_normalized_intervals () ) else : self . interval_ , feature_classes = self . _relevance_bounds_computer . get_normalized_lupi_intervals ( lupi_features = lupi_features ) self . _get_relevance_mask ( feature_classes ) # Return the classifier return self def _fit_baseline ( self , X , y , lupi_features = 0 , ** kwargs ): # Preprocessing data = self . problem_type_ . preprocessing (( X , y ), lupi_features = lupi_features ) # Get predefined template for our init. model init_model_template = self . problem_type_ . get_initmodel_template # Get hyperparameters which are predefined to our model template and can be seleted by user choice hyperparameters = self . problem_type_ . get_all_parameters () # search_samples = len(hyperparameters) * self.n_param_search # TODO: remove this search_samples = self . n_param_search # Find an optimal, fitted model using hyperparemeter search optimal_model , best_score = find_best_model ( init_model_template , hyperparameters , data , self . random_state , search_samples , self . n_jobs , self . verbose , lupi_features = lupi_features , ** kwargs , ) return optimal_model , best_score def _get_relevance_mask ( self , prediction ): \"\"\"Determines relevancy using feature relevance interval values Parameters ---------- fpr : float, optional false positive rate allowed under H_0 Returns ------- boolean array Relevancy prediction for each feature \"\"\" self . relevance_classes_ = prediction self . relevance_classes_string_ = [ RELEVANCE_MAPPING [ p ] for p in prediction ] self . allrel_prediction_ = prediction > 0 self . allrel_prediction_nonpriv_ = self . allrel_prediction_ [: self . n_features_ ] self . allrel_prediction_priv_ = self . allrel_prediction_ [ self . n_features_ :] self . relevance_classes_nonpriv_ = self . relevance_classes_ [: self . n_features_ ] self . relevance_classes_priv_ = self . relevance_classes_ [ self . n_features_ :] return self . allrel_prediction_ def _n_selected_features ( self ): \"\"\" Returns the number of selected features. ------- \"\"\" check_is_fitted ( self , \"allrel_prediction_\" ) return sum ( self . allrel_prediction_ ) def _get_support_mask ( self ): \"\"\"Method for SelectorMixin Returns ------- boolean array \"\"\" return self . allrel_prediction_ def score ( self , X , y ): \"\"\" Using fitted model predict points for `X` and compare to truth `y`. Parameters ---------- X : numpy.ndarray y : numpy.ndarray Returns ------- Model specific score (0 is worst, 1 is best) \"\"\" if self . optim_model_ : return self . optim_model_ . score ( X , y ) else : raise NotFittedError () def constrained_intervals ( self , preset : dict ): \"\"\" Method to return relevance intervals which are constrained using preset ranges or values. Parameters ---------- preset : dict like, {i:float} or {i:[float,float]} Keys denote feature index, values represent a fixed single value (float) or a range of allowed values (lower and upper bound). Example: To set feature 0 to a fixed value use >>> preset = {0: 0.1} or to use the minimum relevance bound >>> preset[1] = self.interval_[1, 0] Returns ------- array like Relevance bounds with user constraints \"\"\" # Do we have intervals? check_is_fitted ( self , \"interval_\" ) return self . _relevance_bounds_computer . compute_multi_preset_relevance_bounds ( preset = preset , lupi_features = self . lupi_features_ ) def print_interval_with_class ( self ): \"\"\" Pretty print the relevance intervals and determined feature relevance class \"\"\" output = \"\" if self . interval_ is None : output += \"Model is not fitted.\" output += \"############## Relevance bounds ############## \\n \" output += \"feature: [LB -- UB], relevance class \\n \" for i in range ( self . n_features_ + self . lupi_features_ ): if i == self . n_features_ : output += \"########## LUPI Relevance bounds \\n \" output += ( f \"{i:7}: [{self.interval_[i, 0]:1.1f} -- {self.interval_[i, 1]:1.1f}],\" ) output += f \" {self.relevance_classes_string_[i]} \\n \" return output","title":"Module fri.main"},{"location":"reference/fri/main/#classes","text":"","title":"Classes"},{"location":"reference/fri/main/#fribase","text":"class ( problem_type : fri . model . base_type . ProblemType , random_state = None , n_jobs = 1 , verbose = 0 , n_param_search = 30 , n_probe_features = 40 , normalize = True , ** kwargs ) Base class for all estimators in scikit-learn","title":"FRIBase"},{"location":"reference/fri/main/#notes","text":"All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"Notes"},{"location":"reference/fri/main/#parameters","text":"problem_type : abc.ABCMeta random_state : Union[mtrand.RandomState, int, None, None, None, None, None, None, None] n_jobs : int verbose : int n_param_search : int n_probe_features : int normalize : bool kwargs :","title":"Parameters"},{"location":"reference/fri/main/#attributes","text":"interval_ : array-like Feature relevance Intervals optim_model_ : InitModel Baseline model fitted on data relevance_classes_ : list(int) Classes of relevance encoded as int: 0 irrelevant, 1 weakly relevant, 2 strongly relevant relevance_classes_string_ : list(str) Classes of relevance encoded as string allrel_prediction_ : list(int) Relevance prediction encoded as boolean: 0 irrelevant, 1 relevant","title":"Attributes"},{"location":"reference/fri/main/#ancestors-in-mro","text":"sklearn.base.BaseEstimator sklearn.feature_selection.base.SelectorMixin sklearn.base.TransformerMixin","title":"Ancestors (in MRO)"},{"location":"reference/fri/main/#descendants","text":"fri.FRI","title":"Descendants"},{"location":"reference/fri/main/#methods","text":"##### constrained_intervals ``` python3 def ( self , preset : dict ) ``` Method to return relevance intervals which are constrained using preset ranges or values . Parameters ---------- preset : dict like , { i : float } or { i :[ float , float ]} Keys denote feature index , values represent a fixed single value ( float ) or a range of allowed values ( lower and upper bound ) . Example : To set feature 0 to a fixed value use >>> preset = { 0 : 0 . 1 } or to use the minimum relevance bound >>> preset [ 1 ] = self . interval_ [ 1 , 0 ] Returns ------- array like Relevance bounds with user constraints ??? example \" View Source \" def constrained_intervals ( self , preset : dict ) : \"\"\" Method to return relevance intervals which are constrained using preset ranges or values . Parameters ---------- preset : dict like , { i : float } or { i :[ float , float ]} Keys denote feature index , values represent a fixed single value ( float ) or a range of allowed values ( lower and upper bound ) . Example : To set feature 0 to a fixed value use >>> preset = { 0 : 0 . 1 } or to use the minimum relevance bound >>> preset [ 1 ] = self . interval_ [ 1 , 0 ] Returns ------- array like Relevance bounds with user constraints \"\"\" # Do we have intervals ? check_is_fitted ( self , \" interval_ \" ) return self . _relevance_bounds_computer . compute_multi_preset_relevance_bounds ( preset = preset , lupi_features = self . lupi_features_ ) ##### fit ``` python3 def ( self , X , y , lupi_features = 0 , ** kwargs ) ``` Method to fit model on data . Parameters ---------- X : numpy . ndarray y : numpy . ndarray lupi_features : int Amount of features which are considered privileged information in ` X `. The data is expected to be structured in a way that all lupi features are at the end of the set . For example ` lupi_features = 1 ` would denote the last column of ` X ` to be privileged . kwargs : dict Dictionary of additional keyword arguments depending on the ` model `. Returns ------- ` FRIBase ` ??? example \" View Source \" def fit ( self , X , y , lupi_features = 0 , ** kwargs ) : \"\"\" Method to fit model on data . Parameters ---------- X : numpy . ndarray y : numpy . ndarray lupi_features : int Amount of features which are considered privileged information in ` X `. The data is expected to be structured in a way that all lupi features are at the end of the set . For example ` lupi_features = 1 ` would denote the last column of ` X ` to be privileged . kwargs : dict Dictionary of additional keyword arguments depending on the ` model `. Returns ------- ` FRIBase ` \"\"\" self . lupi_features_ = lupi_features self . n_samples_ = X . shape [ 0 ] self . n_features_ = X . shape [ 1 ] - lupi_features self . optim_model_ , best_score = self . _fit_baseline ( X , y , lupi_features , ** kwargs ) data = self . problem_type_ . preprocessing (( X , y ) , lupi_features = lupi_features ) self . _relevance_bounds_computer = RelevanceBoundsIntervals ( data , self . problem_type_ , self . optim_model_ , self . random_state , self . n_probe_features , self . n_jobs , self . verbose , normalize = self . normalize , ) if lupi_features == 0 : self . interval_ , feature_classes = ( self . _relevance_bounds_computer . get_normalized_intervals () ) else : self . interval_ , feature_classes = self . _relevance_bounds_computer . get_normalized_lupi_intervals ( lupi_features = lupi_features ) self . _get_relevance_mask ( feature_classes ) # Return the classifier return self ##### print_interval_with_class ``` python3 def ( self ) ``` Pretty print the relevance intervals and determined feature relevance class ??? example \" View Source \" def print_interval_with_class ( self ) : \"\"\" Pretty print the relevance intervals and determined feature relevance class \"\"\" output = \"\" if self . interval_ is None : output += \" Model is not fitted. \" output += \" ############## Relevance bounds ############## \\n \" output += \" feature: [LB -- UB], relevance class \\n \" for i in range ( self . n_features_ + self . lupi_features_ ) : if i == self . n_features_ : output += \" ########## LUPI Relevance bounds \\n \" output += ( f \" {i:7}: [{self.interval_[i, 0]:1.1f} -- {self.interval_[i, 1]:1.1f}], \" ) output += f \" {self.relevance_classes_string_[i]} \\n \" return output ##### score ``` python3 def ( self , X , y ) ``` Using fitted model predict points for ` X ` and compare to truth ` y `. Parameters ---------- X : numpy . ndarray y : numpy . ndarray Returns ------- Model specific score ( 0 is worst , 1 is best ) ??? example \" View Source \" def score ( self , X , y ) : \"\"\" Using fitted model predict points for ` X ` and compare to truth ` y `. Parameters ---------- X : numpy . ndarray y : numpy . ndarray Returns ------- Model specific score ( 0 is worst , 1 is best ) \"\"\" if self . optim_model_ : return self . optim_model_ . score ( X , y ) else : raise NotFittedError () View Source class FRIBase ( BaseEstimator , SelectorMixin ) : def __init__ ( self , problem_type : ProblemType , random_state = None , n_jobs = 1 , verbose = 0 , n_param_search = 30 , n_probe_features = 40 , normalize = True , ** kwargs , ) : \"\"\" Parameters ---------- problem_type : abc . ABCMeta random_state : Union [ mtrand . RandomState , int , None , None , None , None , None , None , None ] n_jobs : int verbose : int n_param_search : int n_probe_features : int normalize : bool kwargs : Attributes ---------- interval_ : array - like Feature relevance Intervals optim_model_ : ` InitModel ` Baseline model fitted on data relevance_classes_ : list ( int ) Classes of relevance encoded as int : 0 irrelevant , 1 weakly relevant , 2 strongly relevant relevance_classes_string_ : list ( str ) Classes of relevance encoded as string allrel_prediction_ : list ( int ) Relevance prediction encoded as boolean : 0 irrelevant , 1 relevant \"\"\" self . n_probe_features = n_probe_features self . n_param_search = n_param_search # assert issubclass ( problem_type , ProblemType ) self . problem_type_ = problem_type ( ** kwargs ) self . random_state = check_random_state ( random_state ) self . n_jobs = n_jobs self . verbose = verbose self . normalize = normalize self . interval_ = None self . optim_model_ = None self . relevance_classes_ = None self . relevance_classes_string_ = None self . allrel_prediction_ = None def fit ( self , X , y , lupi_features = 0 , ** kwargs ) : \"\"\" Method to fit model on data . Parameters ---------- X : numpy . ndarray y : numpy . ndarray lupi_features : int Amount of features which are considered privileged information in ` X `. The data is expected to be structured in a way that all lupi features are at the end of the set . For example ` lupi_features = 1 ` would denote the last column of ` X ` to be privileged . kwargs : dict Dictionary of additional keyword arguments depending on the ` model `. Returns ------- ` FRIBase ` \"\"\" self . lupi_features_ = lupi_features self . n_samples_ = X . shape [ 0 ] self . n_features_ = X . shape [ 1 ] - lupi_features self . optim_model_ , best_score = self . _fit_baseline ( X , y , lupi_features , ** kwargs ) data = self . problem_type_ . preprocessing (( X , y ) , lupi_features = lupi_features ) self . _relevance_bounds_computer = RelevanceBoundsIntervals ( data , self . problem_type_ , self . optim_model_ , self . random_state , self . n_probe_features , self . n_jobs , self . verbose , normalize = self . normalize , ) if lupi_features == 0 : self . interval_ , feature_classes = ( self . _relevance_bounds_computer . get_normalized_intervals () ) else : self . interval_ , feature_classes = self . _relevance_bounds_computer . get_normalized_lupi_intervals ( lupi_features = lupi_features ) self . _get_relevance_mask ( feature_classes ) # Return the classifier return self def _fit_baseline ( self , X , y , lupi_features = 0 , ** kwargs ) : # Preprocessing data = self . problem_type_ . preprocessing (( X , y ) , lupi_features = lupi_features ) # Get predefined template for our init . model init_model_template = self . problem_type_ . get_initmodel_template # Get hyperparameters which are predefined to our model template and can be seleted by user choice hyperparameters = self . problem_type_ . get_all_parameters () # search_samples = len ( hyperparameters ) * self . n_param_search # TODO : remove this search_samples = self . n_param_search # Find an optimal , fitted model using hyperparemeter search optimal_model , best_score = find_best_model ( init_model_template , hyperparameters , data , self . random_state , search_samples , self . n_jobs , self . verbose , lupi_features = lupi_features , ** kwargs , ) return optimal_model , best_score def _get_relevance_mask ( self , prediction ) : \"\"\" Determines relevancy using feature relevance interval values Parameters ---------- fpr : float , optional false positive rate allowed under H_0 Returns ------- boolean array Relevancy prediction for each feature \"\"\" self . relevance_classes_ = prediction self . relevance_classes_string_ = [ RELEVANCE_MAPPING [ p ] for p in prediction ] self . allrel_prediction_ = prediction > 0 self . allrel_prediction_nonpriv_ = self . allrel_prediction_ [: self . n_features_ ] self . allrel_prediction_priv_ = self . allrel_prediction_ [ self . n_features_ :] self . relevance_classes_nonpriv_ = self . relevance_classes_ [: self . n_features_ ] self . relevance_classes_priv_ = self . relevance_classes_ [ self . n_features_ :] return self . allrel_prediction_ def _n_selected_features ( self ) : \"\"\" Returns the number of selected features . ------- \"\"\" check_is_fitted ( self , \" allrel_prediction_ \" ) return sum ( self . allrel_prediction_ ) def _get_support_mask ( self ) : \"\"\" Method for SelectorMixin Returns ------- boolean array \"\"\" return self . allrel_prediction_ def score ( self , X , y ) : \"\"\" Using fitted model predict points for ` X ` and compare to truth ` y `. Parameters ---------- X : numpy . ndarray y : numpy . ndarray Returns ------- Model specific score ( 0 is worst , 1 is best ) \"\"\" if self . optim_model_ : return self . optim_model_ . score ( X , y ) else : raise NotFittedError () def constrained_intervals ( self , preset : dict ) : \"\"\" Method to return relevance intervals which are constrained using preset ranges or values . Parameters ---------- preset : dict like , { i : float } or { i :[ float , float ]} Keys denote feature index , values represent a fixed single value ( float ) or a range of allowed values ( lower and upper bound ) . Example : To set feature 0 to a fixed value use >>> preset = { 0 : 0 . 1 } or to use the minimum relevance bound >>> preset [ 1 ] = self . interval_ [ 1 , 0 ] Returns ------- array like Relevance bounds with user constraints \"\"\" # Do we have intervals ? check_is_fitted ( self , \" interval_ \" ) return self . _relevance_bounds_computer . compute_multi_preset_relevance_bounds ( preset = preset , lupi_features = self . lupi_features_ ) def print_interval_with_class ( self ) : \"\"\" Pretty print the relevance intervals and determined feature relevance class \"\"\" output = \"\" if self . interval_ is None : output += \" Model is not fitted. \" output += \" ############## Relevance bounds ############## \\n \" output += \" feature: [LB -- UB], relevance class \\n \" for i in range ( self . n_features_ + self . lupi_features_ ) : if i == self . n_features_ : output += \" ########## LUPI Relevance bounds \\n \" output += ( f \" {i:7}: [{self.interval_[i, 0]:1.1f} -- {self.interval_[i, 1]:1.1f}], \" ) output += f \" {self.relevance_classes_string_[i]} \\n \" return output","title":"Methods"},{"location":"reference/fri/main/#notfeasibleforparameters","text":"class ( * args , ** kwargs ) Problem was infeasible with the current parameter set.","title":"NotFeasibleForParameters"},{"location":"reference/fri/main/#ancestors-in-mro_1","text":"builtins.Exception builtins.BaseException View Source class NotFeasibleForParameters ( Exception ): \"\"\" Problem was infeasible with the current parameter set. \"\"\"","title":"Ancestors (in MRO)"},{"location":"reference/fri/parameter_searcher/","text":"Module fri.parameter_searcher View Source import warnings from sklearn.exceptions import FitFailedWarning warnings . filterwarnings ( action = \"ignore\" , category = FitFailedWarning ) from pprint import pprint from typing import Tuple import numpy as np from sklearn.model_selection import RandomizedSearchCV from fri.model.base_initmodel import InitModel def find_best_model ( model_template : InitModel , hyperparameters : dict , data : Tuple [ np . ndarray , np . ndarray ], random_state : np . random . RandomState , n_iter : int , n_jobs : int , verbose : int = 0 , lupi_features = None , kwargs : dict = None , ) -> Tuple [ InitModel , float ]: model = model_template () scorer , metric = model . make_scorer () if scorer is None : refit = True else : refit = metric searcher = RandomizedSearchCV ( model , hyperparameters , scoring = scorer , random_state = random_state , refit = refit , cv = 3 , n_iter = n_iter , n_jobs = n_jobs , error_score = np . nan , verbose = verbose , ) X , y = data # Ignore warnings for extremely bad model_state (when precision=0) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) searcher . fit ( X , y , lupi_features = lupi_features ) best_model : InitModel = searcher . best_estimator_ best_score = best_model . score ( X , y ) if verbose > 0 : print ( \"*\" * 20 , \"Best found baseline model\" , \"*\" * 20 ) pprint ( best_model ) print ( \"score: \" , best_score ) for k , v in best_model . constraints . items (): pprint (( f \"{k}: {v}\" )) for k , v in best_model . model_state . items (): if hasattr ( v , \"shape\" ): pprint (( f \"{k}: shape {v.shape}\" )) else : if \"slack\" in k : continue pprint (( f \"{k}: {v}\" )) print ( \"*\" * 30 ) return best_model , best_score Functions find_best_model def ( model_template : fri . model . base_initmodel . InitModel , hyperparameters : dict , data : Tuple [ numpy . ndarray , numpy . ndarray ], random_state : numpy . random . mtrand . RandomState , n_iter : int , n_jobs : int , verbose : int = 0 , lupi_features = None , kwargs : dict = None ) -> Tuple [ fri . model . base_initmodel . InitModel , float ] View Source def find_best_model ( model_template : InitModel , hyperparameters : dict , data : Tuple [ np . ndarray , np . ndarray ], random_state : np . random . RandomState , n_iter : int , n_jobs : int , verbose : int = 0 , lupi_features = None , kwargs : dict = None , ) -> Tuple [ InitModel , float ]: model = model_template () scorer , metric = model . make_scorer () if scorer is None : refit = True else : refit = metric searcher = RandomizedSearchCV ( model , hyperparameters , scoring = scorer , random_state = random_state , refit = refit , cv = 3 , n_iter = n_iter , n_jobs = n_jobs , error_score = np . nan , verbose = verbose , ) X , y = data # Ignore warnings for extremely bad model_state ( when precision = 0 ) with warnings . catch_warnings () : warnings . simplefilter ( \" ignore \" ) searcher . fit ( X , y , lupi_features = lupi_features ) best_model : InitModel = searcher . best_estimator_ best_score = best_model . score ( X , y ) if verbose > 0 : print ( \" * \" * 20 , \" Best found baseline model \" , \" * \" * 20 ) pprint ( best_model ) print ( \" score: \" , best_score ) for k , v in best_model . constraints . items () : pprint (( f \" {k}: {v} \" )) for k , v in best_model . model_state . items () : if hasattr ( v , \" shape \" ) : pprint (( f \" {k}: shape {v.shape} \" )) else : if \" slack \" in k : continue pprint (( f \" {k}: {v} \" )) print ( \" * \" * 30 ) return best_model , best_score","title":"Parameter Searcher"},{"location":"reference/fri/parameter_searcher/#module-friparameter_searcher","text":"View Source import warnings from sklearn.exceptions import FitFailedWarning warnings . filterwarnings ( action = \"ignore\" , category = FitFailedWarning ) from pprint import pprint from typing import Tuple import numpy as np from sklearn.model_selection import RandomizedSearchCV from fri.model.base_initmodel import InitModel def find_best_model ( model_template : InitModel , hyperparameters : dict , data : Tuple [ np . ndarray , np . ndarray ], random_state : np . random . RandomState , n_iter : int , n_jobs : int , verbose : int = 0 , lupi_features = None , kwargs : dict = None , ) -> Tuple [ InitModel , float ]: model = model_template () scorer , metric = model . make_scorer () if scorer is None : refit = True else : refit = metric searcher = RandomizedSearchCV ( model , hyperparameters , scoring = scorer , random_state = random_state , refit = refit , cv = 3 , n_iter = n_iter , n_jobs = n_jobs , error_score = np . nan , verbose = verbose , ) X , y = data # Ignore warnings for extremely bad model_state (when precision=0) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) searcher . fit ( X , y , lupi_features = lupi_features ) best_model : InitModel = searcher . best_estimator_ best_score = best_model . score ( X , y ) if verbose > 0 : print ( \"*\" * 20 , \"Best found baseline model\" , \"*\" * 20 ) pprint ( best_model ) print ( \"score: \" , best_score ) for k , v in best_model . constraints . items (): pprint (( f \"{k}: {v}\" )) for k , v in best_model . model_state . items (): if hasattr ( v , \"shape\" ): pprint (( f \"{k}: shape {v.shape}\" )) else : if \"slack\" in k : continue pprint (( f \"{k}: {v}\" )) print ( \"*\" * 30 ) return best_model , best_score","title":"Module fri.parameter_searcher"},{"location":"reference/fri/parameter_searcher/#functions","text":"","title":"Functions"},{"location":"reference/fri/parameter_searcher/#find_best_model","text":"def ( model_template : fri . model . base_initmodel . InitModel , hyperparameters : dict , data : Tuple [ numpy . ndarray , numpy . ndarray ], random_state : numpy . random . mtrand . RandomState , n_iter : int , n_jobs : int , verbose : int = 0 , lupi_features = None , kwargs : dict = None ) -> Tuple [ fri . model . base_initmodel . InitModel , float ] View Source def find_best_model ( model_template : InitModel , hyperparameters : dict , data : Tuple [ np . ndarray , np . ndarray ], random_state : np . random . RandomState , n_iter : int , n_jobs : int , verbose : int = 0 , lupi_features = None , kwargs : dict = None , ) -> Tuple [ InitModel , float ]: model = model_template () scorer , metric = model . make_scorer () if scorer is None : refit = True else : refit = metric searcher = RandomizedSearchCV ( model , hyperparameters , scoring = scorer , random_state = random_state , refit = refit , cv = 3 , n_iter = n_iter , n_jobs = n_jobs , error_score = np . nan , verbose = verbose , ) X , y = data # Ignore warnings for extremely bad model_state ( when precision = 0 ) with warnings . catch_warnings () : warnings . simplefilter ( \" ignore \" ) searcher . fit ( X , y , lupi_features = lupi_features ) best_model : InitModel = searcher . best_estimator_ best_score = best_model . score ( X , y ) if verbose > 0 : print ( \" * \" * 20 , \" Best found baseline model \" , \" * \" * 20 ) pprint ( best_model ) print ( \" score: \" , best_score ) for k , v in best_model . constraints . items () : pprint (( f \" {k}: {v} \" )) for k , v in best_model . model_state . items () : if hasattr ( v , \" shape \" ) : pprint (( f \" {k}: shape {v.shape} \" )) else : if \" slack \" in k : continue pprint (( f \" {k}: {v} \" )) print ( \" * \" * 30 ) return best_model , best_score","title":"find_best_model"},{"location":"reference/fri/plot/","text":"Module fri.plot View Source import matplotlib matplotlib . use ( \"TkAgg\" ) import matplotlib.patches as mpatches import matplotlib.pyplot as plt import numpy as np from scipy.cluster.hierarchy import dendrogram import matplotlib.cm as cm # Get a color for each relevance type color_palette_3 = cm . Set1 ([ 0 , 1 , 2 ], alpha = 0.8 ) def plot_relevance_bars ( ax , ranges , ticklabels = None , classes = None , numbering = True , tick_rotation = 30 ): \"\"\" Parameters ---------- ax: axis which the bars get drawn on ranges: the 2d array of floating values determining the lower and upper bounds of the bars ticklabels: (optional) labels for each feature classes: (optional) relevance class for each feature, determines color numbering: bool Add feature index when using ticklabels tick_rotation: int Amonut of rotation of ticklabels for easier readability. \"\"\" N = len ( ranges ) # Ticklabels if ticklabels is None : ticks = np . arange ( N ) + 1 else : ticks = list ( ticklabels ) if numbering : for i in range ( N ): ticks [ i ] += \" - {}\" . format ( i + 1 ) # Interval sizes ind = np . arange ( N ) + 1 width = 0.6 upper_vals = ranges [:, 1 ] lower_vals = ranges [:, 0 ] height = upper_vals - lower_vals # Minimal height to make very small intervals visible height [ height < 0.001 ] = 0.001 # Bar colors if classes is None : new_classes = np . zeros ( N ) . astype ( int ) color = [ color_palette_3 [ c . astpye ( int )] for c in new_classes ] else : color = [ color_palette_3 [ c . astype ( int )] for c in classes ] # Plot the bars bars = ax . bar ( ind , height , width , bottom = lower_vals , tick_label = ticks , align = \"center\" , edgecolor = [ \"black\" ] * N , linewidth = 1.3 , color = color , ) ax . set_xticklabels ( ticks ) if ticklabels is not None : ax . set_xticklabels ( ax . get_xticklabels (), rotation = tick_rotation , ha = \"right\" ) # ax.tick_params(rotation=\"auto\") # Limit the y range to 0,1 or 0,L1 ax . set_ylim ([ 0 , max ( ranges [:, 1 ]) * 1.1 ]) ax . set_ylabel ( \"relevance\" ) ax . set_xlabel ( \"feature\" ) if classes is not None : relevance_classes = [ \"Irrelevant\" , \"Weakly relevant\" , \"Strongly relevant\" ] patches = [] for i , rc in enumerate ( relevance_classes ): patch = mpatches . Patch ( color = color_palette_3 [ i ], label = rc ) patches . append ( patch ) ax . legend ( handles = patches ) return bars def plotIntervals ( ranges , ticklabels = None , invert = False , classes = None ): # Figure Parameters fig = plt . figure () ax = fig . add_subplot ( 111 ) out = plot_relevance_bars ( ax , ranges , ticklabels = ticklabels , classes = classes ) fig . autofmt_xdate () # Invert the xaxis for cases in which the comparison with other tools if invert : plt . gca () . invert_xaxis () return fig def plot_dendrogram_and_intervals ( intervals , linkage , figsize = ( 13 , 7 ), ticklabels = None , classes = None , ** kwargs ): fig , ( ax2 , ax ) = plt . subplots ( 2 , 1 , figsize = figsize ) # Top dendrogram plot d = dendrogram ( linkage , color_threshold = 0 , leaf_rotation = 0.0 , # rotates the x axis labels leaf_font_size = 12.0 , # font size for the x axis labels ax = ax2 , ) # Get index determined through linkage method and dendrogram rearranged_index = d [ \"leaves\" ] ranges = intervals [ rearranged_index ] if ticklabels is None : ticks = np . array ( rearranged_index ) ticks += 1 # Index starting at 1 else : ticks = list ( ticklabels [ rearranged_index ]) for i in range ( len ( intervals )): ticks [ i ] += \" - {}\" . format ( rearranged_index [ i ] + 1 ) plot_relevance_bars ( ax , ranges , ticklabels = ticks , classes = classes [ rearranged_index ] if classes is not None else None , numbering = False , ** kwargs ) fig . subplots_adjust ( hspace = 0 ) ax . margins ( x = 0 ) ax2 . set_xticks ([]) ax2 . margins ( x = 0 ) plt . tight_layout () return fig def plot_intervals ( model , ticklabels = None ): \"\"\"Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) \"\"\" if model . interval_ is not None : plotIntervals ( model . interval_ , ticklabels = ticklabels , classes = model . relevance_classes_ ) else : print ( \"Intervals not computed. Try running fit() function first.\" ) def plot_lupi_intervals ( model , ticklabels = None , lupi_ticklabels = None ): \"\"\"Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) lupi_ticklabels : list of str, optional Strs for lupi ticklabels on x-axis (lupi features) \"\"\" n_features = model . interval_ . shape [ 0 ] - model . lupi_features_ data_interval_ = model . interval_ [ 0 : n_features , :] lupi_interval_ = model . interval_ [ n_features :, :] data_relevance_classes_ = model . relevance_classes_ [ 0 : n_features ] lupi_relevance_classes_ = model . relevance_classes_ [ n_features :] if model . interval_ is not None : plotIntervals ( data_interval_ , ticklabels = ticklabels , classes = data_relevance_classes_ ) plotIntervals ( lupi_interval_ , ticklabels = lupi_ticklabels , classes = lupi_relevance_classes_ ) else : print ( \"Intervals not computed. Try running fit() function first.\" ) # # def interactive_scatter_embed(embedding, mode=\"markers\", txt=None): # # TODO: extend method # import plotly.graph_objs as go # from plotly.offline import init_notebook_mode, iplot # init_notebook_mode(connected=True) # # Create a trace # trace = go.Scatter( # x=embedding[:, 0], # y=embedding[:, 1], # mode=mode, # text=txt if mode is \"text\" else None # ) # # data = [trace] # # # Plot and embed in ipython notebook! # iplot(data) Functions plotIntervals def ( ranges , ticklabels = None , invert = False , classes = None ) View Source def plotIntervals ( ranges , ticklabels = None , invert = False , classes = None ) : # Figure Parameters fig = plt . figure () ax = fig . add_subplot ( 111 ) out = plot_relevance_bars ( ax , ranges , ticklabels = ticklabels , classes = classes ) fig . autofmt_xdate () # Invert the xaxis for cases in which the comparison with other tools if invert : plt . gca () . invert_xaxis () return fig plot_dendrogram_and_intervals def ( intervals , linkage , figsize = ( 13 , 7 ), ticklabels = None , classes = None , ** kwargs ) View Source def plot_dendrogram_and_intervals ( intervals , linkage , figsize = ( 13 , 7 ) , ticklabels = None , classes = None , ** kwargs ) : fig , ( ax2 , ax ) = plt . subplots ( 2 , 1 , figsize = figsize ) # Top dendrogram plot d = dendrogram ( linkage , color_threshold = 0 , leaf_rotation = 0 . 0 , # rotates the x axis labels leaf_font_size = 12 . 0 , # font size for the x axis labels ax = ax2 , ) # Get index determined through linkage method and dendrogram rearranged_index = d [ \" leaves \" ] ranges = intervals [ rearranged_index ] if ticklabels is None : ticks = np . array ( rearranged_index ) ticks += 1 # Index starting at 1 else : ticks = list ( ticklabels [ rearranged_index ] ) for i in range ( len ( intervals )) : ticks [ i ] += \" - {} \" . format ( rearranged_index [ i ] + 1 ) plot_relevance_bars ( ax , ranges , ticklabels = ticks , classes = classes [ rearranged_index ] if classes is not None else None , numbering = False , ** kwargs ) fig . subplots_adjust ( hspace = 0 ) ax . margins ( x = 0 ) ax2 . set_xticks ( [] ) ax2 . margins ( x = 0 ) plt . tight_layout () return fig plot_intervals def ( model , ticklabels = None ) Plot the relevance intervals. Parameters model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) View Source def plot_intervals ( model , ticklabels = None ) : \"\"\" Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before . ticklabels : list of str , optional Strs for ticklabels on x - axis ( features ) \"\"\" if model . interval_ is not None : plotIntervals ( model . interval_ , ticklabels = ticklabels , classes = model . relevance_classes_ ) else : print ( \" Intervals not computed. Try running fit() function first. \" ) plot_lupi_intervals def ( model , ticklabels = None , lupi_ticklabels = None ) Plot the relevance intervals. Parameters model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) lupi_ticklabels : list of str, optional Strs for lupi ticklabels on x-axis (lupi features) View Source def plot_lupi_intervals ( model , ticklabels = None , lupi_ticklabels = None ) : \"\"\" Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before . ticklabels : list of str , optional Strs for ticklabels on x - axis ( features ) lupi_ticklabels : list of str , optional Strs for lupi ticklabels on x - axis ( lupi features ) \"\"\" n_features = model . interval_ . shape [ 0 ] - model . lupi_features_ data_interval_ = model . interval_ [ 0 : n_features , :] lupi_interval_ = model . interval_ [ n_features :, :] data_relevance_classes_ = model . relevance_classes_ [ 0 : n_features ] lupi_relevance_classes_ = model . relevance_classes_ [ n_features :] if model . interval_ is not None : plotIntervals ( data_interval_ , ticklabels = ticklabels , classes = data_relevance_classes_ ) plotIntervals ( lupi_interval_ , ticklabels = lupi_ticklabels , classes = lupi_relevance_classes_ ) else : print ( \" Intervals not computed. Try running fit() function first. \" ) plot_relevance_bars def ( ax , ranges , ticklabels = None , classes = None , numbering = True , tick_rotation = 30 ) Parameters ax: axis which the bars get drawn on ranges: the 2d array of floating values determining the lower and upper bounds of the bars ticklabels: (optional) labels for each feature classes: (optional) relevance class for each feature, determines color numbering: bool Add feature index when using ticklabels tick_rotation: int Amonut of rotation of ticklabels for easier readability. View Source def plot_relevance_bars ( ax , ranges , ticklabels = None , classes = None , numbering = True , tick_rotation = 30 ) : \"\"\" Parameters ---------- ax : axis which the bars get drawn on ranges : the 2 d array of floating values determining the lower and upper bounds of the bars ticklabels : ( optional ) labels for each feature classes : ( optional ) relevance class for each feature , determines color numbering : bool Add feature index when using ticklabels tick_rotation : int Amonut of rotation of ticklabels for easier readability . \"\"\" N = len ( ranges ) # Ticklabels if ticklabels is None : ticks = np . arange ( N ) + 1 else : ticks = list ( ticklabels ) if numbering : for i in range ( N ) : ticks [ i ] += \" - {} \" . format ( i + 1 ) # Interval sizes ind = np . arange ( N ) + 1 width = 0 . 6 upper_vals = ranges [:, 1 ] lower_vals = ranges [:, 0 ] height = upper_vals - lower_vals # Minimal height to make very small intervals visible height [ height < 0 . 001 ] = 0 . 001 # Bar colors if classes is None : new_classes = np . zeros ( N ) . astype ( int ) color = [ color_palette_3 [ c . astpye ( int ) ] for c in new_classes ] else : color = [ color_palette_3 [ c . astype ( int ) ] for c in classes ] # Plot the bars bars = ax . bar ( ind , height , width , bottom = lower_vals , tick_label = ticks , align = \" center \" , edgecolor = [ \" black \" ] * N , linewidth = 1 . 3 , color = color , ) ax . set_xticklabels ( ticks ) if ticklabels is not None : ax . set_xticklabels ( ax . get_xticklabels () , rotation = tick_rotation , ha = \" right \" ) # ax . tick_params ( rotation = \" auto \" ) # Limit the y range to 0 , 1 or 0 , L1 ax . set_ylim ( [ 0 , max ( ranges [:, 1 ] ) * 1 . 1 ] ) ax . set_ylabel ( \" relevance \" ) ax . set_xlabel ( \" feature \" ) if classes is not None : relevance_classes = [ \" Irrelevant \" , \" Weakly relevant \" , \" Strongly relevant \" ] patches = [] for i , rc in enumerate ( relevance_classes ) : patch = mpatches . Patch ( color = color_palette_3 [ i ], label = rc ) patches . append ( patch ) ax . legend ( handles = patches ) return bars","title":"Plot"},{"location":"reference/fri/plot/#module-friplot","text":"View Source import matplotlib matplotlib . use ( \"TkAgg\" ) import matplotlib.patches as mpatches import matplotlib.pyplot as plt import numpy as np from scipy.cluster.hierarchy import dendrogram import matplotlib.cm as cm # Get a color for each relevance type color_palette_3 = cm . Set1 ([ 0 , 1 , 2 ], alpha = 0.8 ) def plot_relevance_bars ( ax , ranges , ticklabels = None , classes = None , numbering = True , tick_rotation = 30 ): \"\"\" Parameters ---------- ax: axis which the bars get drawn on ranges: the 2d array of floating values determining the lower and upper bounds of the bars ticklabels: (optional) labels for each feature classes: (optional) relevance class for each feature, determines color numbering: bool Add feature index when using ticklabels tick_rotation: int Amonut of rotation of ticklabels for easier readability. \"\"\" N = len ( ranges ) # Ticklabels if ticklabels is None : ticks = np . arange ( N ) + 1 else : ticks = list ( ticklabels ) if numbering : for i in range ( N ): ticks [ i ] += \" - {}\" . format ( i + 1 ) # Interval sizes ind = np . arange ( N ) + 1 width = 0.6 upper_vals = ranges [:, 1 ] lower_vals = ranges [:, 0 ] height = upper_vals - lower_vals # Minimal height to make very small intervals visible height [ height < 0.001 ] = 0.001 # Bar colors if classes is None : new_classes = np . zeros ( N ) . astype ( int ) color = [ color_palette_3 [ c . astpye ( int )] for c in new_classes ] else : color = [ color_palette_3 [ c . astype ( int )] for c in classes ] # Plot the bars bars = ax . bar ( ind , height , width , bottom = lower_vals , tick_label = ticks , align = \"center\" , edgecolor = [ \"black\" ] * N , linewidth = 1.3 , color = color , ) ax . set_xticklabels ( ticks ) if ticklabels is not None : ax . set_xticklabels ( ax . get_xticklabels (), rotation = tick_rotation , ha = \"right\" ) # ax.tick_params(rotation=\"auto\") # Limit the y range to 0,1 or 0,L1 ax . set_ylim ([ 0 , max ( ranges [:, 1 ]) * 1.1 ]) ax . set_ylabel ( \"relevance\" ) ax . set_xlabel ( \"feature\" ) if classes is not None : relevance_classes = [ \"Irrelevant\" , \"Weakly relevant\" , \"Strongly relevant\" ] patches = [] for i , rc in enumerate ( relevance_classes ): patch = mpatches . Patch ( color = color_palette_3 [ i ], label = rc ) patches . append ( patch ) ax . legend ( handles = patches ) return bars def plotIntervals ( ranges , ticklabels = None , invert = False , classes = None ): # Figure Parameters fig = plt . figure () ax = fig . add_subplot ( 111 ) out = plot_relevance_bars ( ax , ranges , ticklabels = ticklabels , classes = classes ) fig . autofmt_xdate () # Invert the xaxis for cases in which the comparison with other tools if invert : plt . gca () . invert_xaxis () return fig def plot_dendrogram_and_intervals ( intervals , linkage , figsize = ( 13 , 7 ), ticklabels = None , classes = None , ** kwargs ): fig , ( ax2 , ax ) = plt . subplots ( 2 , 1 , figsize = figsize ) # Top dendrogram plot d = dendrogram ( linkage , color_threshold = 0 , leaf_rotation = 0.0 , # rotates the x axis labels leaf_font_size = 12.0 , # font size for the x axis labels ax = ax2 , ) # Get index determined through linkage method and dendrogram rearranged_index = d [ \"leaves\" ] ranges = intervals [ rearranged_index ] if ticklabels is None : ticks = np . array ( rearranged_index ) ticks += 1 # Index starting at 1 else : ticks = list ( ticklabels [ rearranged_index ]) for i in range ( len ( intervals )): ticks [ i ] += \" - {}\" . format ( rearranged_index [ i ] + 1 ) plot_relevance_bars ( ax , ranges , ticklabels = ticks , classes = classes [ rearranged_index ] if classes is not None else None , numbering = False , ** kwargs ) fig . subplots_adjust ( hspace = 0 ) ax . margins ( x = 0 ) ax2 . set_xticks ([]) ax2 . margins ( x = 0 ) plt . tight_layout () return fig def plot_intervals ( model , ticklabels = None ): \"\"\"Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) \"\"\" if model . interval_ is not None : plotIntervals ( model . interval_ , ticklabels = ticklabels , classes = model . relevance_classes_ ) else : print ( \"Intervals not computed. Try running fit() function first.\" ) def plot_lupi_intervals ( model , ticklabels = None , lupi_ticklabels = None ): \"\"\"Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) lupi_ticklabels : list of str, optional Strs for lupi ticklabels on x-axis (lupi features) \"\"\" n_features = model . interval_ . shape [ 0 ] - model . lupi_features_ data_interval_ = model . interval_ [ 0 : n_features , :] lupi_interval_ = model . interval_ [ n_features :, :] data_relevance_classes_ = model . relevance_classes_ [ 0 : n_features ] lupi_relevance_classes_ = model . relevance_classes_ [ n_features :] if model . interval_ is not None : plotIntervals ( data_interval_ , ticklabels = ticklabels , classes = data_relevance_classes_ ) plotIntervals ( lupi_interval_ , ticklabels = lupi_ticklabels , classes = lupi_relevance_classes_ ) else : print ( \"Intervals not computed. Try running fit() function first.\" ) # # def interactive_scatter_embed(embedding, mode=\"markers\", txt=None): # # TODO: extend method # import plotly.graph_objs as go # from plotly.offline import init_notebook_mode, iplot # init_notebook_mode(connected=True) # # Create a trace # trace = go.Scatter( # x=embedding[:, 0], # y=embedding[:, 1], # mode=mode, # text=txt if mode is \"text\" else None # ) # # data = [trace] # # # Plot and embed in ipython notebook! # iplot(data)","title":"Module fri.plot"},{"location":"reference/fri/plot/#functions","text":"","title":"Functions"},{"location":"reference/fri/plot/#plotintervals","text":"def ( ranges , ticklabels = None , invert = False , classes = None ) View Source def plotIntervals ( ranges , ticklabels = None , invert = False , classes = None ) : # Figure Parameters fig = plt . figure () ax = fig . add_subplot ( 111 ) out = plot_relevance_bars ( ax , ranges , ticklabels = ticklabels , classes = classes ) fig . autofmt_xdate () # Invert the xaxis for cases in which the comparison with other tools if invert : plt . gca () . invert_xaxis () return fig","title":"plotIntervals"},{"location":"reference/fri/plot/#plot_dendrogram_and_intervals","text":"def ( intervals , linkage , figsize = ( 13 , 7 ), ticklabels = None , classes = None , ** kwargs ) View Source def plot_dendrogram_and_intervals ( intervals , linkage , figsize = ( 13 , 7 ) , ticklabels = None , classes = None , ** kwargs ) : fig , ( ax2 , ax ) = plt . subplots ( 2 , 1 , figsize = figsize ) # Top dendrogram plot d = dendrogram ( linkage , color_threshold = 0 , leaf_rotation = 0 . 0 , # rotates the x axis labels leaf_font_size = 12 . 0 , # font size for the x axis labels ax = ax2 , ) # Get index determined through linkage method and dendrogram rearranged_index = d [ \" leaves \" ] ranges = intervals [ rearranged_index ] if ticklabels is None : ticks = np . array ( rearranged_index ) ticks += 1 # Index starting at 1 else : ticks = list ( ticklabels [ rearranged_index ] ) for i in range ( len ( intervals )) : ticks [ i ] += \" - {} \" . format ( rearranged_index [ i ] + 1 ) plot_relevance_bars ( ax , ranges , ticklabels = ticks , classes = classes [ rearranged_index ] if classes is not None else None , numbering = False , ** kwargs ) fig . subplots_adjust ( hspace = 0 ) ax . margins ( x = 0 ) ax2 . set_xticks ( [] ) ax2 . margins ( x = 0 ) plt . tight_layout () return fig","title":"plot_dendrogram_and_intervals"},{"location":"reference/fri/plot/#plot_intervals","text":"def ( model , ticklabels = None ) Plot the relevance intervals.","title":"plot_intervals"},{"location":"reference/fri/plot/#parameters","text":"model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) View Source def plot_intervals ( model , ticklabels = None ) : \"\"\" Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before . ticklabels : list of str , optional Strs for ticklabels on x - axis ( features ) \"\"\" if model . interval_ is not None : plotIntervals ( model . interval_ , ticklabels = ticklabels , classes = model . relevance_classes_ ) else : print ( \" Intervals not computed. Try running fit() function first. \" )","title":"Parameters"},{"location":"reference/fri/plot/#plot_lupi_intervals","text":"def ( model , ticklabels = None , lupi_ticklabels = None ) Plot the relevance intervals.","title":"plot_lupi_intervals"},{"location":"reference/fri/plot/#parameters_1","text":"model : FRI model Needs to be fitted before. ticklabels : list of str, optional Strs for ticklabels on x-axis (features) lupi_ticklabels : list of str, optional Strs for lupi ticklabels on x-axis (lupi features) View Source def plot_lupi_intervals ( model , ticklabels = None , lupi_ticklabels = None ) : \"\"\" Plot the relevance intervals. Parameters ---------- model : FRI model Needs to be fitted before . ticklabels : list of str , optional Strs for ticklabels on x - axis ( features ) lupi_ticklabels : list of str , optional Strs for lupi ticklabels on x - axis ( lupi features ) \"\"\" n_features = model . interval_ . shape [ 0 ] - model . lupi_features_ data_interval_ = model . interval_ [ 0 : n_features , :] lupi_interval_ = model . interval_ [ n_features :, :] data_relevance_classes_ = model . relevance_classes_ [ 0 : n_features ] lupi_relevance_classes_ = model . relevance_classes_ [ n_features :] if model . interval_ is not None : plotIntervals ( data_interval_ , ticklabels = ticklabels , classes = data_relevance_classes_ ) plotIntervals ( lupi_interval_ , ticklabels = lupi_ticklabels , classes = lupi_relevance_classes_ ) else : print ( \" Intervals not computed. Try running fit() function first. \" )","title":"Parameters"},{"location":"reference/fri/plot/#plot_relevance_bars","text":"def ( ax , ranges , ticklabels = None , classes = None , numbering = True , tick_rotation = 30 )","title":"plot_relevance_bars"},{"location":"reference/fri/plot/#parameters_2","text":"ax: axis which the bars get drawn on ranges: the 2d array of floating values determining the lower and upper bounds of the bars ticklabels: (optional) labels for each feature classes: (optional) relevance class for each feature, determines color numbering: bool Add feature index when using ticklabels tick_rotation: int Amonut of rotation of ticklabels for easier readability. View Source def plot_relevance_bars ( ax , ranges , ticklabels = None , classes = None , numbering = True , tick_rotation = 30 ) : \"\"\" Parameters ---------- ax : axis which the bars get drawn on ranges : the 2 d array of floating values determining the lower and upper bounds of the bars ticklabels : ( optional ) labels for each feature classes : ( optional ) relevance class for each feature , determines color numbering : bool Add feature index when using ticklabels tick_rotation : int Amonut of rotation of ticklabels for easier readability . \"\"\" N = len ( ranges ) # Ticklabels if ticklabels is None : ticks = np . arange ( N ) + 1 else : ticks = list ( ticklabels ) if numbering : for i in range ( N ) : ticks [ i ] += \" - {} \" . format ( i + 1 ) # Interval sizes ind = np . arange ( N ) + 1 width = 0 . 6 upper_vals = ranges [:, 1 ] lower_vals = ranges [:, 0 ] height = upper_vals - lower_vals # Minimal height to make very small intervals visible height [ height < 0 . 001 ] = 0 . 001 # Bar colors if classes is None : new_classes = np . zeros ( N ) . astype ( int ) color = [ color_palette_3 [ c . astpye ( int ) ] for c in new_classes ] else : color = [ color_palette_3 [ c . astype ( int ) ] for c in classes ] # Plot the bars bars = ax . bar ( ind , height , width , bottom = lower_vals , tick_label = ticks , align = \" center \" , edgecolor = [ \" black \" ] * N , linewidth = 1 . 3 , color = color , ) ax . set_xticklabels ( ticks ) if ticklabels is not None : ax . set_xticklabels ( ax . get_xticklabels () , rotation = tick_rotation , ha = \" right \" ) # ax . tick_params ( rotation = \" auto \" ) # Limit the y range to 0 , 1 or 0 , L1 ax . set_ylim ( [ 0 , max ( ranges [:, 1 ] ) * 1 . 1 ] ) ax . set_ylabel ( \" relevance \" ) ax . set_xlabel ( \" feature \" ) if classes is not None : relevance_classes = [ \" Irrelevant \" , \" Weakly relevant \" , \" Strongly relevant \" ] patches = [] for i , rc in enumerate ( relevance_classes ) : patch = mpatches . Patch ( color = color_palette_3 [ i ], label = rc ) patches . append ( patch ) ax . legend ( handles = patches ) return bars","title":"Parameters"},{"location":"reference/fri/utils/","text":"Module fri.utils View Source import numpy as np def distance ( u , v ): \"\"\" Distance measure custom made for feature comparison. Parameters ---------- u: first feature v: second feature Returns ------- \"\"\" u = np . asarray ( u ) v = np . asarray ( v ) # Euclidean differences diff = ( u - v ) ** 2 # Nullify pairwise contribution diff [ u == 0 ] = 0 diff [ v == 0 ] = 0 return np . sqrt ( np . sum ( diff )) def permutate_feature_in_data ( data , feature_i , random_state ): X , y = data X_copy = np . copy ( X ) # Permute selected feature permutated_feature = random_state . permutation ( X_copy [:, feature_i ]) # Add permutation back to dataset X_copy [:, feature_i ] = permutated_feature return X_copy , y Functions distance def ( u , v ) Distance measure custom made for feature comparison. Parameters u: first feature v: second feature Returns View Source def distance ( u , v ) : \"\"\" Distance measure custom made for feature comparison . Parameters ---------- u : first feature v : second feature Returns ------- \"\"\" u = np . asarray ( u ) v = np . asarray ( v ) # Euclidean differences diff = ( u - v ) ** 2 # Nullify pairwise contribution diff [ u == 0 ] = 0 diff [ v == 0 ] = 0 return np . sqrt ( np . sum ( diff )) permutate_feature_in_data def ( data , feature_i , random_state ) View Source def permutate_feature_in_data ( data , feature_i , random_state ) : X , y = data X_copy = np . copy ( X ) # Permute selected feature permutated_feature = random_state . permutation ( X_copy [:, feature_i ] ) # Add permutation back to dataset X_copy [:, feature_i ] = permutated_feature return X_copy , y","title":"Utils"},{"location":"reference/fri/utils/#module-friutils","text":"View Source import numpy as np def distance ( u , v ): \"\"\" Distance measure custom made for feature comparison. Parameters ---------- u: first feature v: second feature Returns ------- \"\"\" u = np . asarray ( u ) v = np . asarray ( v ) # Euclidean differences diff = ( u - v ) ** 2 # Nullify pairwise contribution diff [ u == 0 ] = 0 diff [ v == 0 ] = 0 return np . sqrt ( np . sum ( diff )) def permutate_feature_in_data ( data , feature_i , random_state ): X , y = data X_copy = np . copy ( X ) # Permute selected feature permutated_feature = random_state . permutation ( X_copy [:, feature_i ]) # Add permutation back to dataset X_copy [:, feature_i ] = permutated_feature return X_copy , y","title":"Module fri.utils"},{"location":"reference/fri/utils/#functions","text":"","title":"Functions"},{"location":"reference/fri/utils/#distance","text":"def ( u , v ) Distance measure custom made for feature comparison.","title":"distance"},{"location":"reference/fri/utils/#parameters","text":"u: first feature v: second feature","title":"Parameters"},{"location":"reference/fri/utils/#returns","text":"View Source def distance ( u , v ) : \"\"\" Distance measure custom made for feature comparison . Parameters ---------- u : first feature v : second feature Returns ------- \"\"\" u = np . asarray ( u ) v = np . asarray ( v ) # Euclidean differences diff = ( u - v ) ** 2 # Nullify pairwise contribution diff [ u == 0 ] = 0 diff [ v == 0 ] = 0 return np . sqrt ( np . sum ( diff ))","title":"Returns"},{"location":"reference/fri/utils/#permutate_feature_in_data","text":"def ( data , feature_i , random_state ) View Source def permutate_feature_in_data ( data , feature_i , random_state ) : X , y = data X_copy = np . copy ( X ) # Permute selected feature permutated_feature = random_state . permutation ( X_copy [:, feature_i ] ) # Add permutation back to dataset X_copy [:, feature_i ] = permutated_feature return X_copy , y","title":"permutate_feature_in_data"},{"location":"reference/fri/model/","text":"Module fri.model View Source from .classification import Classification from .lupi_classification import LUPI_Classification from .lupi_ordinal_regression import LUPI_OrdinalRegression from .lupi_regression import LUPI_Regression from .ordinal_regression import OrdinalRegression from .regression import Regression __all__ = [ \"Classification\" , \"Regression\" , \"OrdinalRegression\" , \"LUPI_Classification\" , \"LUPI_Regression\" , \"LUPI_OrdinalRegression\" , ] Sub-modules fri.model.base_cvxproblem fri.model.base_initmodel fri.model.base_lupi fri.model.base_type fri.model.classification fri.model.lupi_classification fri.model.lupi_ordinal_regression fri.model.lupi_regression fri.model.ordinal_regression fri.model.regression Classes Classification class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods ##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" ] Instance variables ``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : Methods ##### preprocessing ``` python3 def ( self , data , ** kwargs ) ``` ??? example \" View Source \" def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \" Only binary class data supported \" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class Classification ( ProblemType ) : @ classmethod def parameters ( cls ) : return [ \" C \" ] @ property def get_initmodel_template ( cls ) : return Classification_SVM @ property def get_cvxproblem_template ( cls ) : return Classification_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \" Only binary class data supported \" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y LUPI_Classification class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods ##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] Instance variables ``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : ``` python3 lupi_features ``` : Methods ##### preprocessing ``` python3 def ( self , data , lupi_features = None ) ``` ??? example \" View Source \" def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \" Only binary class data supported \" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class LUPI_Classification ( ProblemType ) : def __init__ ( self , ** kwargs ) : super () . __init__ ( ** kwargs ) self . _lupi_features = None @ property def lupi_features ( self ) : return self . _lupi_features @ classmethod def parameters ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] @ property def get_initmodel_template ( cls ) : return LUPI_Classification_SVM @ property def get_cvxproblem_template ( cls ) : return LUPI_Classification_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \" Only binary class data supported \" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y LUPI_OrdinalRegression class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods ##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] Instance variables ``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : ``` python3 lupi_features ``` : Methods ##### preprocessing ``` python3 def ( self , data , lupi_features = None ) ``` ??? example \" View Source \" def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \" First ordinal class has index > 0. Shifting index... \" ) y = y - np . min ( y ) return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class LUPI_OrdinalRegression ( ProblemType ) : def __init__ ( self , ** kwargs ) : super () . __init__ ( ** kwargs ) self . _lupi_features = None @ property def lupi_features ( self ) : return self . _lupi_features @ classmethod def parameters ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] @ property def get_initmodel_template ( cls ) : return LUPI_OrdinalRegression_SVM @ property def get_cvxproblem_template ( cls ) : return LUPI_OrdinalRegression_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \" First ordinal class has index > 0. Shifting index... \" ) y = y - np . min ( y ) return X , y LUPI_Regression class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods ##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" , \" epsilon \" , \" scaling_lupi_w \" ] Instance variables ``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : ``` python3 lupi_features ``` : Methods ##### preprocessing ``` python3 def ( self , data , lupi_features = None ) ``` ??? example \" View Source \" def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class LUPI_Regression ( ProblemType ) : def __init__ ( self , ** kwargs ) : super () . __init__ ( ** kwargs ) self . _lupi_features = None @ property def lupi_features ( self ) : return self . _lupi_features @ classmethod def parameters ( cls ) : return [ \" C \" , \" epsilon \" , \" scaling_lupi_w \" ] @ property def get_initmodel_template ( cls ) : return LUPI_Regression_SVM @ property def get_cvxproblem_template ( cls ) : return LUPI_Regression_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y OrdinalRegression class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods ##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" ] Instance variables ``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : Methods ##### preprocessing ``` python3 def ( self , data , ** kwargs ) ``` ??? example \" View Source \" def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \" First ordinal class has index > 0. Shifting index... \" ) y = y - np . min ( y ) return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class OrdinalRegression ( ProblemType ) : @ classmethod def parameters ( cls ) : return [ \" C \" ] @ property def get_initmodel_template ( cls ) : return OrdinalRegression_SVM @ property def get_cvxproblem_template ( cls ) : return OrdinalRegression_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \" First ordinal class has index > 0. Shifting index... \" ) y = y - np . min ( y ) return X , y Regression class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods ##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" , \" epsilon \" ] Instance variables ``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : Methods ##### preprocessing ``` python3 def ( self , data , ** kwargs ) ``` ??? example \" View Source \" def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class Regression ( ProblemType ) : @ classmethod def parameters ( cls ) : return [ \" C \" , \" epsilon \" ] @ property def get_initmodel_template ( cls ) : return Regression_SVR @ property def get_cvxproblem_template ( cls ) : return Regression_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y","title":"Index"},{"location":"reference/fri/model/#module-frimodel","text":"View Source from .classification import Classification from .lupi_classification import LUPI_Classification from .lupi_ordinal_regression import LUPI_OrdinalRegression from .lupi_regression import LUPI_Regression from .ordinal_regression import OrdinalRegression from .regression import Regression __all__ = [ \"Classification\" , \"Regression\" , \"OrdinalRegression\" , \"LUPI_Classification\" , \"LUPI_Regression\" , \"LUPI_OrdinalRegression\" , ]","title":"Module fri.model"},{"location":"reference/fri/model/#sub-modules","text":"fri.model.base_cvxproblem fri.model.base_initmodel fri.model.base_lupi fri.model.base_type fri.model.classification fri.model.lupi_classification fri.model.lupi_ordinal_regression fri.model.lupi_regression fri.model.ordinal_regression fri.model.regression","title":"Sub-modules"},{"location":"reference/fri/model/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/#classification","text":"class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"Classification"},{"location":"reference/fri/model/#ancestors-in-mro","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/#static-methods","text":"##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" ]","title":"Static methods"},{"location":"reference/fri/model/#instance-variables","text":"``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` :","title":"Instance variables"},{"location":"reference/fri/model/#methods","text":"##### preprocessing ``` python3 def ( self , data , ** kwargs ) ``` ??? example \" View Source \" def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \" Only binary class data supported \" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class Classification ( ProblemType ) : @ classmethod def parameters ( cls ) : return [ \" C \" ] @ property def get_initmodel_template ( cls ) : return Classification_SVM @ property def get_cvxproblem_template ( cls ) : return Classification_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \" Only binary class data supported \" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y","title":"Methods"},{"location":"reference/fri/model/#lupi_classification","text":"class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"LUPI_Classification"},{"location":"reference/fri/model/#ancestors-in-mro_1","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/#static-methods_1","text":"##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ]","title":"Static methods"},{"location":"reference/fri/model/#instance-variables_1","text":"``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : ``` python3 lupi_features ``` :","title":"Instance variables"},{"location":"reference/fri/model/#methods_1","text":"##### preprocessing ``` python3 def ( self , data , lupi_features = None ) ``` ??? example \" View Source \" def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \" Only binary class data supported \" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class LUPI_Classification ( ProblemType ) : def __init__ ( self , ** kwargs ) : super () . __init__ ( ** kwargs ) self . _lupi_features = None @ property def lupi_features ( self ) : return self . _lupi_features @ classmethod def parameters ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] @ property def get_initmodel_template ( cls ) : return LUPI_Classification_SVM @ property def get_cvxproblem_template ( cls ) : return LUPI_Classification_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \" Only binary class data supported \" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y","title":"Methods"},{"location":"reference/fri/model/#lupi_ordinalregression","text":"class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"LUPI_OrdinalRegression"},{"location":"reference/fri/model/#ancestors-in-mro_2","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/#static-methods_2","text":"##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ]","title":"Static methods"},{"location":"reference/fri/model/#instance-variables_2","text":"``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : ``` python3 lupi_features ``` :","title":"Instance variables"},{"location":"reference/fri/model/#methods_2","text":"##### preprocessing ``` python3 def ( self , data , lupi_features = None ) ``` ??? example \" View Source \" def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \" First ordinal class has index > 0. Shifting index... \" ) y = y - np . min ( y ) return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class LUPI_OrdinalRegression ( ProblemType ) : def __init__ ( self , ** kwargs ) : super () . __init__ ( ** kwargs ) self . _lupi_features = None @ property def lupi_features ( self ) : return self . _lupi_features @ classmethod def parameters ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] @ property def get_initmodel_template ( cls ) : return LUPI_OrdinalRegression_SVM @ property def get_cvxproblem_template ( cls ) : return LUPI_OrdinalRegression_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \" First ordinal class has index > 0. Shifting index... \" ) y = y - np . min ( y ) return X , y","title":"Methods"},{"location":"reference/fri/model/#lupi_regression","text":"class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"LUPI_Regression"},{"location":"reference/fri/model/#ancestors-in-mro_3","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/#static-methods_3","text":"##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" , \" epsilon \" , \" scaling_lupi_w \" ]","title":"Static methods"},{"location":"reference/fri/model/#instance-variables_3","text":"``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : ``` python3 lupi_features ``` :","title":"Instance variables"},{"location":"reference/fri/model/#methods_3","text":"##### preprocessing ``` python3 def ( self , data , lupi_features = None ) ``` ??? example \" View Source \" def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class LUPI_Regression ( ProblemType ) : def __init__ ( self , ** kwargs ) : super () . __init__ ( ** kwargs ) self . _lupi_features = None @ property def lupi_features ( self ) : return self . _lupi_features @ classmethod def parameters ( cls ) : return [ \" C \" , \" epsilon \" , \" scaling_lupi_w \" ] @ property def get_initmodel_template ( cls ) : return LUPI_Regression_SVM @ property def get_cvxproblem_template ( cls ) : return LUPI_Regression_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y","title":"Methods"},{"location":"reference/fri/model/#ordinalregression","text":"class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"OrdinalRegression"},{"location":"reference/fri/model/#ancestors-in-mro_4","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/#static-methods_4","text":"##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" ]","title":"Static methods"},{"location":"reference/fri/model/#instance-variables_4","text":"``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` :","title":"Instance variables"},{"location":"reference/fri/model/#methods_4","text":"##### preprocessing ``` python3 def ( self , data , ** kwargs ) ``` ??? example \" View Source \" def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \" First ordinal class has index > 0. Shifting index... \" ) y = y - np . min ( y ) return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class OrdinalRegression ( ProblemType ) : @ classmethod def parameters ( cls ) : return [ \" C \" ] @ property def get_initmodel_template ( cls ) : return OrdinalRegression_SVM @ property def get_cvxproblem_template ( cls ) : return OrdinalRegression_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \" First ordinal class has index > 0. Shifting index... \" ) y = y - np . min ( y ) return X , y","title":"Methods"},{"location":"reference/fri/model/#regression","text":"class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"Regression"},{"location":"reference/fri/model/#ancestors-in-mro_5","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/#static-methods_5","text":"##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" , \" epsilon \" ]","title":"Static methods"},{"location":"reference/fri/model/#instance-variables_5","text":"``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` :","title":"Instance variables"},{"location":"reference/fri/model/#methods_5","text":"##### preprocessing ``` python3 def ( self , data , ** kwargs ) ``` ??? example \" View Source \" def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class Regression ( ProblemType ) : @ classmethod def parameters ( cls ) : return [ \" C \" , \" epsilon \" ] @ property def get_initmodel_template ( cls ) : return Regression_SVR @ property def get_cvxproblem_template ( cls ) : return Regression_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y","title":"Methods"},{"location":"reference/fri/model/base_cvxproblem/","text":"Module fri.model.base_cvxproblem View Source from abc import ABC , abstractmethod import cvxpy as cvx import numpy as np from cvxpy import SolverError class Relevance_CVXProblem ( ABC ): def __str__ ( self ) -> str : if self . isLowerBound : lower = \"Lower\" else : lower = \"Upper\" name = f \"{lower}_{self.current_feature}_{self.__class__.__name__}\" state = \"\" for s in self . init_hyperparameters . items (): state += f \"{s[0]}:{s[1]}, \" for s in self . init_model_constraints . items (): state += f \"{s[0]}:{s[1]}, \" state = \"(\" + state [: - 2 ] + \")\" if self . isProbe : prefix = f \"Probe_{self.probeID}\" else : prefix = \"\" return prefix + name + state def __init__ ( self , current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs , ) -> None : self . _probeID = probeID self . _feature_relevance = None self . isLowerBound = None # General data self . current_feature = current_feature self . preset_model = preset_model self . best_model_state = best_model_state self . preprocessing_data ( data , best_model_state ) # Initialize constraints self . _constraints = [] self . _objective = None self . w = None self . _init_constraints ( hyperparameters , best_model_constraints ) if self . preset_model is not None : self . _add_preset_constraints ( self . preset_model , best_model_constraints ) self . init_hyperparameters = hyperparameters self . init_model_constraints = best_model_constraints def preprocessing_data ( self , data , best_model_state ): X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y ) @property def constraints ( self ): return self . _constraints def add_constraint ( self , new ): self . _constraints . append ( new ) @property def objective ( self ): return self . _objective @property def solved_relevance ( self ): if self . is_solved : return self . objective . value else : raise Exception ( \"Problem not solved. No feature relevance computed.\" ) @property def probeID ( self ): return self . _probeID @property def isProbe ( self ): return self . probeID >= 0 @abstractmethod def _init_constraints ( self , parameters , init_model_constraints ): pass @abstractmethod def init_objective_UB ( self , ** kwargs ): pass @abstractmethod def init_objective_LB ( self , ** kwargs ): pass @property def cvx_problem ( self ): return self . _cvx_problem @property def is_solved ( self ): if self . _solver_status in self . accepted_status : return True else : return False @property def accepted_status ( self ): return [ \"optimal\" , \"optimal_inaccurate\" ] def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here, worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print(\"Solve\", self) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors, which are common with our framework: # We solve multiple problems per bound and choose a feasible solution later (see '_create_interval') pass self . _solver_status = self . _cvx_problem . status # self._cvx_problem = None return self def _retrieve_result ( self ): return self . current_feature , self . objective @property def solver_kwargs ( self ): return { \"verbose\" : False , \"solver\" : \"ECOS\" } def _add_preset_constraints ( self , preset_model : dict , best_model_constraints ): for feature , current_preset in preset_model . items (): # Skip current feature if feature == self . current_feature : continue # Skip unset values if all ( np . isnan ( current_preset )): continue # a weight bigger than the optimal model L1 makes no sense assert abs ( current_preset [ 0 ]) <= best_model_constraints [ \"w_l1\" ] assert abs ( current_preset [ 1 ]) <= best_model_constraints [ \"w_l1\" ] # We add a pair of constraints depending on sign of known coefficient # this makes it possible to solve this as a convex problem if current_preset [ 0 ] >= 0 : self . add_constraint ( self . w [ feature ] >= current_preset [ 0 ]) self . add_constraint ( self . w [ feature ] <= current_preset [ 1 ]) else : self . add_constraint ( self . w [ feature ] <= current_preset [ 0 ]) self . add_constraint ( self . w [ feature ] >= current_preset [ 1 ]) @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): for sign in [ - 1 , 1 ]: problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ): vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ): vals = [ candidate . solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value Classes Relevance_CVXProblem class ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) abc.ABC Descendants fri.model.classification.Classification_Relevance_Bound fri.model.base_lupi.LUPI_Relevance_CVXProblem fri.model.ordinal_regression.OrdinalRegression_Relevance_Bound fri.model.regression.Regression_Relevance_Bound Static methods ##### aggregate_max_candidates ``` python3 def ( max_problems_candidates ) ``` ??? example \" View Source \" @ classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate . solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value ##### aggregate_min_candidates ``` python3 def ( min_problems_candidates ) ``` ??? example \" View Source \" @ classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value ##### generate_lower_bound_problem ``` python3 def ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) ``` ??? example \" View Source \" @ classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem ##### generate_upper_bound_problem ``` python3 def ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) ``` ??? example \" View Source \" @ classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : for sign in [ - 1 , 1 ]: problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem Instance variables ``` python3 accepted_status ``` : ``` python3 constraints ``` : ``` python3 cvx_problem ``` : ``` python3 isProbe ``` : ``` python3 is_solved ``` : ``` python3 objective ``` : ``` python3 probeID ``` : ``` python3 solved_relevance ``` : ``` python3 solver_kwargs ``` : Methods ##### add_constraint ``` python3 def ( self , new ) ``` ??? example \" View Source \" def add_constraint ( self , new ) : self . _constraints . append ( new ) ##### init_objective_LB ``` python3 def ( self , ** kwargs ) ``` ??? example \" View Source \" @ abstractmethod def init_objective_LB ( self , ** kwargs ) : pass ##### init_objective_UB ``` python3 def ( self , ** kwargs ) ``` ??? example \" View Source \" @ abstractmethod def init_objective_UB ( self , ** kwargs ) : pass ##### preprocessing_data ``` python3 def ( self , data , best_model_state ) ``` ??? example \" View Source \" def preprocessing_data ( self , data , best_model_state ) : X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y ) ##### solve ``` python3 def ( self ) -> object ``` ??? example \" View Source \" def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \" Solve \" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see ' _create_interval ' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self View Source class Relevance_CVXProblem ( ABC ) : def __ str__ ( self ) -> str : if self . isLowerBound: lower = \"Lower\" else : lower = \"Upper\" name = f \"{lower}_{self.current_feature}_{self.__class__.__name__}\" state = \"\" for s in self . init_hyperparameters . items () : state += f \"{s[0]}:{s[1]}, \" for s in self . init_model_constraints . items () : state += f \"{s[0]}:{s[1]}, \" state = \"(\" + state [:- 2 ] + \")\" if self . isProbe: prefix = f \"Probe_{self.probeID}\" else : prefix = \"\" return prefix + name + state def __ init__ ( self , current_feature: int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID=- 1 , **kwargs , ) -> None : self . _ probeID = probeID self . _ feature_relevance = None self . isLowerBound = None # General data self . current_feature = current_feature self . preset_model = preset_model self . best_model_state = best_model_state self . preprocessing_data ( data , best_model_state ) # Initialize constraints self . _ constraints = [] self . _ objective = None self . w = None self . _ init_constraints ( hyperparameters , best_model_constraints ) if self . preset_model is not None : self . _ add_preset_constraints ( self . preset_model , best_model_constraints ) self . init_hyperparameters = hyperparameters self . init_model_constraints = best_model_constraints def preprocessing_data ( self , data , best_model_state ) : X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y ) @property def constraints ( self ) : return self . _ constraints def add_constraint ( self , new ) : self . _ constraints . append ( new ) @property def objective ( self ) : return self . _ objective @property def solved_relevance ( self ) : if self . is_solved: return self . objective . value else : raise Exception ( \"Problem not solved. No feature relevance computed.\" ) @property def probeID ( self ) : return self . _ probeID @property def isProbe ( self ) : return self . probeID >= 0 @abstractmethod def _ init_constraints ( self , parameters , init_model_constraints ) : pass @abstractmethod def init_objective_UB ( self , **kwargs ) : pass @abstractmethod def init_objective_LB ( self , **kwargs ) : pass @property def cvx_problem ( self ) : return self . _ cvx_problem @property def is_solved ( self ) : if self . _ solver_status in self . accepted_status: return True else : return False @property def accepted_status ( self ) : return [ \"optimal\" , \"optimal_inaccurate\" ] def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _ cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _ cvx_problem . solve ( **self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _ solver_status = self . _ cvx_problem . status # self . _ cvx_problem = None return self def _ retrieve_result ( self ) : return self . current_feature , self . objective @property def solver_kwargs ( self ) : return { \"verbose\" : False , \"solver\" : \"ECOS\" } def _ add_preset_constraints ( self , preset_model: dict , best_model_constraints ) : for feature , current_preset in preset_model . items () : # Skip current feature if feature == self . current_feature: continue # Skip unset values if all ( np . isnan ( current_preset )) : continue # a weight bigger than the optimal model L1 makes no sense assert abs ( current_preset [ 0 ]) <= best_model_constraints [ \"w_l1\" ] assert abs ( current_preset [ 1 ]) <= best_model_constraints [ \"w_l1\" ] # We add a pair of constraints depending on sign of known coefficient # this makes it possible to solve this as a convex problem if current_preset [ 0 ] >= 0 : self . add_constraint ( self . w [ feature ] >= current_preset [ 0 ]) self . add_constraint ( self . w [ feature ] <= current_preset [ 1 ]) else : self . add_constraint ( self . w [ feature ] <= current_preset [ 0 ]) self . add_constraint ( self . w [ feature ] >= current_preset [ 1 ]) @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID=- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID=- 1 , ) : for sign in [ - 1 , 1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate . solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value","title":"Base Cvxproblem"},{"location":"reference/fri/model/base_cvxproblem/#module-frimodelbase_cvxproblem","text":"View Source from abc import ABC , abstractmethod import cvxpy as cvx import numpy as np from cvxpy import SolverError class Relevance_CVXProblem ( ABC ): def __str__ ( self ) -> str : if self . isLowerBound : lower = \"Lower\" else : lower = \"Upper\" name = f \"{lower}_{self.current_feature}_{self.__class__.__name__}\" state = \"\" for s in self . init_hyperparameters . items (): state += f \"{s[0]}:{s[1]}, \" for s in self . init_model_constraints . items (): state += f \"{s[0]}:{s[1]}, \" state = \"(\" + state [: - 2 ] + \")\" if self . isProbe : prefix = f \"Probe_{self.probeID}\" else : prefix = \"\" return prefix + name + state def __init__ ( self , current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs , ) -> None : self . _probeID = probeID self . _feature_relevance = None self . isLowerBound = None # General data self . current_feature = current_feature self . preset_model = preset_model self . best_model_state = best_model_state self . preprocessing_data ( data , best_model_state ) # Initialize constraints self . _constraints = [] self . _objective = None self . w = None self . _init_constraints ( hyperparameters , best_model_constraints ) if self . preset_model is not None : self . _add_preset_constraints ( self . preset_model , best_model_constraints ) self . init_hyperparameters = hyperparameters self . init_model_constraints = best_model_constraints def preprocessing_data ( self , data , best_model_state ): X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y ) @property def constraints ( self ): return self . _constraints def add_constraint ( self , new ): self . _constraints . append ( new ) @property def objective ( self ): return self . _objective @property def solved_relevance ( self ): if self . is_solved : return self . objective . value else : raise Exception ( \"Problem not solved. No feature relevance computed.\" ) @property def probeID ( self ): return self . _probeID @property def isProbe ( self ): return self . probeID >= 0 @abstractmethod def _init_constraints ( self , parameters , init_model_constraints ): pass @abstractmethod def init_objective_UB ( self , ** kwargs ): pass @abstractmethod def init_objective_LB ( self , ** kwargs ): pass @property def cvx_problem ( self ): return self . _cvx_problem @property def is_solved ( self ): if self . _solver_status in self . accepted_status : return True else : return False @property def accepted_status ( self ): return [ \"optimal\" , \"optimal_inaccurate\" ] def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here, worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print(\"Solve\", self) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors, which are common with our framework: # We solve multiple problems per bound and choose a feasible solution later (see '_create_interval') pass self . _solver_status = self . _cvx_problem . status # self._cvx_problem = None return self def _retrieve_result ( self ): return self . current_feature , self . objective @property def solver_kwargs ( self ): return { \"verbose\" : False , \"solver\" : \"ECOS\" } def _add_preset_constraints ( self , preset_model : dict , best_model_constraints ): for feature , current_preset in preset_model . items (): # Skip current feature if feature == self . current_feature : continue # Skip unset values if all ( np . isnan ( current_preset )): continue # a weight bigger than the optimal model L1 makes no sense assert abs ( current_preset [ 0 ]) <= best_model_constraints [ \"w_l1\" ] assert abs ( current_preset [ 1 ]) <= best_model_constraints [ \"w_l1\" ] # We add a pair of constraints depending on sign of known coefficient # this makes it possible to solve this as a convex problem if current_preset [ 0 ] >= 0 : self . add_constraint ( self . w [ feature ] >= current_preset [ 0 ]) self . add_constraint ( self . w [ feature ] <= current_preset [ 1 ]) else : self . add_constraint ( self . w [ feature ] <= current_preset [ 0 ]) self . add_constraint ( self . w [ feature ] >= current_preset [ 1 ]) @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): for sign in [ - 1 , 1 ]: problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ): vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ): vals = [ candidate . solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value","title":"Module fri.model.base_cvxproblem"},{"location":"reference/fri/model/base_cvxproblem/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/base_cvxproblem/#relevance_cvxproblem","text":"class ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"Relevance_CVXProblem"},{"location":"reference/fri/model/base_cvxproblem/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/base_cvxproblem/#descendants","text":"fri.model.classification.Classification_Relevance_Bound fri.model.base_lupi.LUPI_Relevance_CVXProblem fri.model.ordinal_regression.OrdinalRegression_Relevance_Bound fri.model.regression.Regression_Relevance_Bound","title":"Descendants"},{"location":"reference/fri/model/base_cvxproblem/#static-methods","text":"##### aggregate_max_candidates ``` python3 def ( max_problems_candidates ) ``` ??? example \" View Source \" @ classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate . solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value ##### aggregate_min_candidates ``` python3 def ( min_problems_candidates ) ``` ??? example \" View Source \" @ classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value ##### generate_lower_bound_problem ``` python3 def ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) ``` ??? example \" View Source \" @ classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem ##### generate_upper_bound_problem ``` python3 def ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) ``` ??? example \" View Source \" @ classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : for sign in [ - 1 , 1 ]: problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem","title":"Static methods"},{"location":"reference/fri/model/base_cvxproblem/#instance-variables","text":"``` python3 accepted_status ``` : ``` python3 constraints ``` : ``` python3 cvx_problem ``` : ``` python3 isProbe ``` : ``` python3 is_solved ``` : ``` python3 objective ``` : ``` python3 probeID ``` : ``` python3 solved_relevance ``` : ``` python3 solver_kwargs ``` :","title":"Instance variables"},{"location":"reference/fri/model/base_cvxproblem/#methods","text":"##### add_constraint ``` python3 def ( self , new ) ``` ??? example \" View Source \" def add_constraint ( self , new ) : self . _constraints . append ( new ) ##### init_objective_LB ``` python3 def ( self , ** kwargs ) ``` ??? example \" View Source \" @ abstractmethod def init_objective_LB ( self , ** kwargs ) : pass ##### init_objective_UB ``` python3 def ( self , ** kwargs ) ``` ??? example \" View Source \" @ abstractmethod def init_objective_UB ( self , ** kwargs ) : pass ##### preprocessing_data ``` python3 def ( self , data , best_model_state ) ``` ??? example \" View Source \" def preprocessing_data ( self , data , best_model_state ) : X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y ) ##### solve ``` python3 def ( self ) -> object ``` ??? example \" View Source \" def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \" Solve \" , self ) self . _cvx_problem . solve ( ** self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see ' _create_interval ' ) pass self . _solver_status = self . _cvx_problem . status # self . _cvx_problem = None return self View Source class Relevance_CVXProblem ( ABC ) : def __ str__ ( self ) -> str : if self . isLowerBound: lower = \"Lower\" else : lower = \"Upper\" name = f \"{lower}_{self.current_feature}_{self.__class__.__name__}\" state = \"\" for s in self . init_hyperparameters . items () : state += f \"{s[0]}:{s[1]}, \" for s in self . init_model_constraints . items () : state += f \"{s[0]}:{s[1]}, \" state = \"(\" + state [:- 2 ] + \")\" if self . isProbe: prefix = f \"Probe_{self.probeID}\" else : prefix = \"\" return prefix + name + state def __ init__ ( self , current_feature: int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID=- 1 , **kwargs , ) -> None : self . _ probeID = probeID self . _ feature_relevance = None self . isLowerBound = None # General data self . current_feature = current_feature self . preset_model = preset_model self . best_model_state = best_model_state self . preprocessing_data ( data , best_model_state ) # Initialize constraints self . _ constraints = [] self . _ objective = None self . w = None self . _ init_constraints ( hyperparameters , best_model_constraints ) if self . preset_model is not None : self . _ add_preset_constraints ( self . preset_model , best_model_constraints ) self . init_hyperparameters = hyperparameters self . init_model_constraints = best_model_constraints def preprocessing_data ( self , data , best_model_state ) : X , y = data self . n = X . shape [ 0 ] self . d = X . shape [ 1 ] self . X = X self . y = np . array ( y ) @property def constraints ( self ) : return self . _ constraints def add_constraint ( self , new ) : self . _ constraints . append ( new ) @property def objective ( self ) : return self . _ objective @property def solved_relevance ( self ) : if self . is_solved: return self . objective . value else : raise Exception ( \"Problem not solved. No feature relevance computed.\" ) @property def probeID ( self ) : return self . _ probeID @property def isProbe ( self ) : return self . probeID >= 0 @abstractmethod def _ init_constraints ( self , parameters , init_model_constraints ) : pass @abstractmethod def init_objective_UB ( self , **kwargs ) : pass @abstractmethod def init_objective_LB ( self , **kwargs ) : pass @property def cvx_problem ( self ) : return self . _ cvx_problem @property def is_solved ( self ) : if self . _ solver_status in self . accepted_status: return True else : return False @property def accepted_status ( self ) : return [ \"optimal\" , \"optimal_inaccurate\" ] def solve ( self ) -> object : # We init cvx problem here because pickling LP solver objects is problematic # by deferring it to here , worker threads do the problem building themselves and we spare the serialization self . _ cvx_problem = cvx . Problem ( objective = self . objective , constraints = self . constraints ) try : # print ( \"Solve\" , self ) self . _ cvx_problem . solve ( **self . solver_kwargs ) except SolverError : # We ignore Solver Errors , which are common with our framework : # We solve multiple problems per bound and choose a feasible solution later ( see '_create_interval' ) pass self . _ solver_status = self . _ cvx_problem . status # self . _ cvx_problem = None return self def _ retrieve_result ( self ) : return self . current_feature , self . objective @property def solver_kwargs ( self ) : return { \"verbose\" : False , \"solver\" : \"ECOS\" } def _ add_preset_constraints ( self , preset_model: dict , best_model_constraints ) : for feature , current_preset in preset_model . items () : # Skip current feature if feature == self . current_feature: continue # Skip unset values if all ( np . isnan ( current_preset )) : continue # a weight bigger than the optimal model L1 makes no sense assert abs ( current_preset [ 0 ]) <= best_model_constraints [ \"w_l1\" ] assert abs ( current_preset [ 1 ]) <= best_model_constraints [ \"w_l1\" ] # We add a pair of constraints depending on sign of known coefficient # this makes it possible to solve this as a convex problem if current_preset [ 0 ] >= 0 : self . add_constraint ( self . w [ feature ] >= current_preset [ 0 ]) self . add_constraint ( self . w [ feature ] <= current_preset [ 1 ]) else : self . add_constraint ( self . w [ feature ] <= current_preset [ 0 ]) self . add_constraint ( self . w [ feature ] >= current_preset [ 1 ]) @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID=- 1 , ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB () problem . isLowerBound = True yield problem @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID=- 1 , ) : for sign in [ - 1 , 1 ] : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign ) problem . isLowerBound = False yield problem @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] min_value = min ( vals ) return min_value @classmethod def aggregate_max_candidates ( cls , max_problems_candidates ) : vals = [ candidate . solved_relevance for candidate in max_problems_candidates ] max_value = max ( vals ) return max_value","title":"Methods"},{"location":"reference/fri/model/base_initmodel/","text":"Module fri.model.base_initmodel View Source from abc import ABC , abstractmethod from sklearn.base import BaseEstimator class InitModel ( ABC , BaseEstimator ): def __init__ ( self , ** parameters ): if parameters is None : parameters = {} self . hyperparam = parameters self . _model_state = {} def _get_param_names ( cls ): return sorted ( cls . hyperparameter ()) @classmethod @abstractmethod def hyperparameter ( cls ): raise NotImplementedError def get_params ( self , deep = True ): return self . hyperparam def set_params ( self , ** params ): for p , value in params . items (): self . hyperparam [ p ] = value return self @abstractmethod def fit ( self , X , y , ** kwargs ): pass @abstractmethod def predict ( self , X ): pass @abstractmethod def score ( self , X , y , ** kwargs ): pass @classmethod def make_scorer ( self ): return None , None @property def constraints ( self ): return self . _constraints @constraints.setter def constraints ( self , constraints ): self . _constraints = constraints @property def model_state ( self ): return self . _model_state @model_state.setter def model_state ( self , params ): self . _model_state = params @property def L1_factor ( self ): try : return self . constraints [ \"w_l1\" ] except : raise NotImplementedError ( \"Baseline model does not provide (L1) normalization constant. Expected l1 norm of model weights (e.g. w).\" ) @property def solver_params ( cls ): return { \"solver\" : \"ECOS\" } class LUPI_InitModel ( InitModel ): @property def L1_factor_priv ( self ): try : return self . constraints [ \"w_priv_l1\" ] except : raise NotImplementedError ( \"Baseline model does not provide LUPI (L1) normalization constant. Expected l1 norm of LUPI model weights (e.g. w_priv).\" ) Classes InitModel class ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) abc.ABC sklearn.base.BaseEstimator Descendants fri.model.base_initmodel.LUPI_InitModel fri.model.classification.Classification_SVM fri.model.lupi_classification.LUPI_Classification_SVM fri.model.ordinal_regression.OrdinalRegression_SVM fri.model.regression.Regression_SVR Static methods ##### hyperparameter ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod @ abstractmethod def hyperparameter ( cls ) : raise NotImplementedError ##### make_scorer ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def make_scorer ( self ) : return None , None Instance variables ``` python3 L1_factor ``` : ``` python3 constraints ``` : ``` python3 model_state ``` : ``` python3 solver_params ``` : Methods ##### fit ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" @ abstractmethod def fit ( self , X , y , ** kwargs ) : pass ##### get_params ``` python3 def ( self , deep = True ) ``` Get parameters for this estimator . Parameters ---------- deep : boolean , optional If True , will return the parameters for this estimator and contained subobjects that are estimators . Returns ------- params : mapping of string to any Parameter names mapped to their values . ??? example \" View Source \" def get_params ( self , deep = True ) : return self . hyperparam ##### predict ``` python3 def ( self , X ) ``` ??? example \" View Source \" @ abstractmethod def predict ( self , X ) : pass ##### score ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" @ abstractmethod def score ( self , X , y , ** kwargs ) : pass ##### set_params ``` python3 def ( self , ** params ) ``` Set the parameters of this estimator . The method works on simple estimators as well as on nested objects ( such as pipelines ) . The latter have parameters of the form `` < component > __ < parameter > `` so that it ' s possible to update each component of a nested object . Returns ------- self ??? example \" View Source \" def set_params ( self , ** params ) : for p , value in params . items () : self . hyperparam [ p ] = value return self View Source class InitModel ( ABC , BaseEstimator ) : def __init__ ( self , ** parameters ) : if parameters is None : parameters = {} self . hyperparam = parameters self . _model_state = {} def _get_param_names ( cls ) : return sorted ( cls . hyperparameter ()) @ classmethod @ abstractmethod def hyperparameter ( cls ) : raise NotImplementedError def get_params ( self , deep = True ) : return self . hyperparam def set_params ( self , ** params ) : for p , value in params . items () : self . hyperparam [ p ] = value return self @ abstractmethod def fit ( self , X , y , ** kwargs ) : pass @ abstractmethod def predict ( self , X ) : pass @ abstractmethod def score ( self , X , y , ** kwargs ) : pass @ classmethod def make_scorer ( self ) : return None , None @ property def constraints ( self ) : return self . _constraints @ constraints . setter def constraints ( self , constraints ) : self . _constraints = constraints @ property def model_state ( self ) : return self . _model_state @ model_state . setter def model_state ( self , params ) : self . _model_state = params @ property def L1_factor ( self ) : try : return self . constraints [ \" w_l1 \" ] except : raise NotImplementedError ( \" Baseline model does not provide (L1) normalization constant. Expected l1 norm of model weights (e.g. w). \" ) @ property def solver_params ( cls ) : return { \" solver \" : \" ECOS \" } LUPI_InitModel class ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator Descendants fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression_SVM fri.model.lupi_regression.LUPI_Regression_SVM Instance variables ``` python3 L1_factor_priv ``` : View Source class LUPI_InitModel ( InitModel ) : @ property def L1_factor_priv ( self ) : try : return self . constraints [ \" w_priv_l1 \" ] except : raise NotImplementedError ( \" Baseline model does not provide LUPI (L1) normalization constant. Expected l1 norm of LUPI model weights (e.g. w_priv). \" )","title":"Base Initmodel"},{"location":"reference/fri/model/base_initmodel/#module-frimodelbase_initmodel","text":"View Source from abc import ABC , abstractmethod from sklearn.base import BaseEstimator class InitModel ( ABC , BaseEstimator ): def __init__ ( self , ** parameters ): if parameters is None : parameters = {} self . hyperparam = parameters self . _model_state = {} def _get_param_names ( cls ): return sorted ( cls . hyperparameter ()) @classmethod @abstractmethod def hyperparameter ( cls ): raise NotImplementedError def get_params ( self , deep = True ): return self . hyperparam def set_params ( self , ** params ): for p , value in params . items (): self . hyperparam [ p ] = value return self @abstractmethod def fit ( self , X , y , ** kwargs ): pass @abstractmethod def predict ( self , X ): pass @abstractmethod def score ( self , X , y , ** kwargs ): pass @classmethod def make_scorer ( self ): return None , None @property def constraints ( self ): return self . _constraints @constraints.setter def constraints ( self , constraints ): self . _constraints = constraints @property def model_state ( self ): return self . _model_state @model_state.setter def model_state ( self , params ): self . _model_state = params @property def L1_factor ( self ): try : return self . constraints [ \"w_l1\" ] except : raise NotImplementedError ( \"Baseline model does not provide (L1) normalization constant. Expected l1 norm of model weights (e.g. w).\" ) @property def solver_params ( cls ): return { \"solver\" : \"ECOS\" } class LUPI_InitModel ( InitModel ): @property def L1_factor_priv ( self ): try : return self . constraints [ \"w_priv_l1\" ] except : raise NotImplementedError ( \"Baseline model does not provide LUPI (L1) normalization constant. Expected l1 norm of LUPI model weights (e.g. w_priv).\" )","title":"Module fri.model.base_initmodel"},{"location":"reference/fri/model/base_initmodel/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/base_initmodel/#initmodel","text":"class ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance.","title":"InitModel"},{"location":"reference/fri/model/base_initmodel/#ancestors-in-mro","text":"abc.ABC sklearn.base.BaseEstimator","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/base_initmodel/#descendants","text":"fri.model.base_initmodel.LUPI_InitModel fri.model.classification.Classification_SVM fri.model.lupi_classification.LUPI_Classification_SVM fri.model.ordinal_regression.OrdinalRegression_SVM fri.model.regression.Regression_SVR","title":"Descendants"},{"location":"reference/fri/model/base_initmodel/#static-methods","text":"##### hyperparameter ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod @ abstractmethod def hyperparameter ( cls ) : raise NotImplementedError ##### make_scorer ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def make_scorer ( self ) : return None , None","title":"Static methods"},{"location":"reference/fri/model/base_initmodel/#instance-variables","text":"``` python3 L1_factor ``` : ``` python3 constraints ``` : ``` python3 model_state ``` : ``` python3 solver_params ``` :","title":"Instance variables"},{"location":"reference/fri/model/base_initmodel/#methods","text":"##### fit ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" @ abstractmethod def fit ( self , X , y , ** kwargs ) : pass ##### get_params ``` python3 def ( self , deep = True ) ``` Get parameters for this estimator . Parameters ---------- deep : boolean , optional If True , will return the parameters for this estimator and contained subobjects that are estimators . Returns ------- params : mapping of string to any Parameter names mapped to their values . ??? example \" View Source \" def get_params ( self , deep = True ) : return self . hyperparam ##### predict ``` python3 def ( self , X ) ``` ??? example \" View Source \" @ abstractmethod def predict ( self , X ) : pass ##### score ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" @ abstractmethod def score ( self , X , y , ** kwargs ) : pass ##### set_params ``` python3 def ( self , ** params ) ``` Set the parameters of this estimator . The method works on simple estimators as well as on nested objects ( such as pipelines ) . The latter have parameters of the form `` < component > __ < parameter > `` so that it ' s possible to update each component of a nested object . Returns ------- self ??? example \" View Source \" def set_params ( self , ** params ) : for p , value in params . items () : self . hyperparam [ p ] = value return self View Source class InitModel ( ABC , BaseEstimator ) : def __init__ ( self , ** parameters ) : if parameters is None : parameters = {} self . hyperparam = parameters self . _model_state = {} def _get_param_names ( cls ) : return sorted ( cls . hyperparameter ()) @ classmethod @ abstractmethod def hyperparameter ( cls ) : raise NotImplementedError def get_params ( self , deep = True ) : return self . hyperparam def set_params ( self , ** params ) : for p , value in params . items () : self . hyperparam [ p ] = value return self @ abstractmethod def fit ( self , X , y , ** kwargs ) : pass @ abstractmethod def predict ( self , X ) : pass @ abstractmethod def score ( self , X , y , ** kwargs ) : pass @ classmethod def make_scorer ( self ) : return None , None @ property def constraints ( self ) : return self . _constraints @ constraints . setter def constraints ( self , constraints ) : self . _constraints = constraints @ property def model_state ( self ) : return self . _model_state @ model_state . setter def model_state ( self , params ) : self . _model_state = params @ property def L1_factor ( self ) : try : return self . constraints [ \" w_l1 \" ] except : raise NotImplementedError ( \" Baseline model does not provide (L1) normalization constant. Expected l1 norm of model weights (e.g. w). \" ) @ property def solver_params ( cls ) : return { \" solver \" : \" ECOS \" }","title":"Methods"},{"location":"reference/fri/model/base_initmodel/#lupi_initmodel","text":"class ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance.","title":"LUPI_InitModel"},{"location":"reference/fri/model/base_initmodel/#ancestors-in-mro_1","text":"fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/base_initmodel/#descendants_1","text":"fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression_SVM fri.model.lupi_regression.LUPI_Regression_SVM","title":"Descendants"},{"location":"reference/fri/model/base_initmodel/#instance-variables_1","text":"``` python3 L1_factor_priv ``` : View Source class LUPI_InitModel ( InitModel ) : @ property def L1_factor_priv ( self ) : try : return self . constraints [ \" w_priv_l1 \" ] except : raise NotImplementedError ( \" Baseline model does not provide LUPI (L1) normalization constant. Expected l1 norm of LUPI model weights (e.g. w_priv). \" )","title":"Instance variables"},{"location":"reference/fri/model/base_lupi/","text":"Module fri.model.base_lupi View Source from abc import abstractmethod from . base_cvxproblem import Relevance_CVXProblem class LUPI_Relevance_CVXProblem ( Relevance_CVXProblem ) : def __ init__ ( self , current_feature: int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID=- 1 , ) -> None : super (). __ init__ ( current_feature , data , hyperparameters , best_model_constraints , preset_model , best_model_state , probeID , ) def preprocessing_data ( self , data , best_model_state ) : lupi_features = best_model_state [ \"lupi_features\" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super (). preprocessing_data (( X , y ), best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False def init_objective_UB ( self , **kwargs ) : # We have two models basically with different indexes if self . isPriv: self . _ init_objective_UB_LUPI ( **kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_UB ( **kwargs ) def init_objective_LB ( self , **kwargs ) : # We have two models basically with different indexes if self . isPriv: self . _ init_objective_LB_LUPI ( **kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_LB ( **kwargs ) @abstractmethod def _ init_objective_LB_LUPI ( self , **kwargs ) : pass @abstractmethod def _ init_objective_UB_LUPI ( self , **kwargs ) : pass def split_dataset ( X_combined , lupi_features ) : assert X_combined . shape [ 1 ] > lupi_features X = X_combined [ : , :- lupi_features ] X_priv = X_combined [ : , - lupi_features: ] return X , X_priv def is_lupi_feature ( di , data , best_model_state ) : lupi_features = best_model_state [ \"lupi_features\" ] X_combined , _ = data d = X_combined . shape [ 1 ] - lupi_features lupi_index = di - d return lupi_index >= 0 Functions is_lupi_feature def ( di , data , best_model_state ) View Source def is_lupi_feature ( di , data , best_model_state ) : lupi_features = best_model_state [ \" lupi_features \" ] X_combined , _ = data d = X_combined . shape [ 1 ] - lupi_features lupi_index = di - d return lupi_index >= 0 split_dataset def ( X_combined , lupi_features ) View Source def split_dataset ( X_combined , lupi_features ) : assert X_combined . shape [ 1 ] > lupi_features X = X_combined [ : , :- lupi_features ] X_priv = X_combined [ : , - lupi_features: ] return X , X_priv Classes LUPI_Relevance_CVXProblem class ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC Descendants fri.model.lupi_classification.LUPI_Classification_Relevance_Bound fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression_Relevance_Bound fri.model.lupi_regression.LUPI_Regression_Relevance_Bound Methods ##### init_objective_LB ``` python3 def ( self , ** kwargs ) ``` ??? example \" View Source \" def init_objective_LB ( self , ** kwargs ) : # We have two models basically with different indexes if self . isPriv : self . _init_objective_LB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super () . init_objective_LB ( ** kwargs ) ##### init_objective_UB ``` python3 def ( self , ** kwargs ) ``` ??? example \" View Source \" def init_objective_UB ( self , ** kwargs ) : # We have two models basically with different indexes if self . isPriv : self . _init_objective_UB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super () . init_objective_UB ( ** kwargs ) ##### preprocessing_data ``` python3 def ( self , data , best_model_state ) ``` ??? example \" View Source \" def preprocessing_data ( self , data , best_model_state ) : lupi_features = best_model_state [ \" lupi_features \" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super () . preprocessing_data (( X , y ) , best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False View Source class LUPI_Relevance_CVXProblem ( Relevance_CVXProblem ) : def __init__ ( self , current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ) -> None : super () . __init__ ( current_feature , data , hyperparameters , best_model_constraints , preset_model , best_model_state , probeID , ) def preprocessing_data ( self , data , best_model_state ) : lupi_features = best_model_state [ \" lupi_features \" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super () . preprocessing_data (( X , y ) , best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False def init_objective_UB ( self , ** kwargs ) : # We have two models basically with different indexes if self . isPriv : self . _init_objective_UB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super () . init_objective_UB ( ** kwargs ) def init_objective_LB ( self , ** kwargs ) : # We have two models basically with different indexes if self . isPriv : self . _init_objective_LB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super () . init_objective_LB ( ** kwargs ) @ abstractmethod def _init_objective_LB_LUPI ( self , ** kwargs ) : pass @ abstractmethod def _init_objective_UB_LUPI ( self , ** kwargs ) : pass","title":"Base Lupi"},{"location":"reference/fri/model/base_lupi/#module-frimodelbase_lupi","text":"View Source from abc import abstractmethod from . base_cvxproblem import Relevance_CVXProblem class LUPI_Relevance_CVXProblem ( Relevance_CVXProblem ) : def __ init__ ( self , current_feature: int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID=- 1 , ) -> None : super (). __ init__ ( current_feature , data , hyperparameters , best_model_constraints , preset_model , best_model_state , probeID , ) def preprocessing_data ( self , data , best_model_state ) : lupi_features = best_model_state [ \"lupi_features\" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super (). preprocessing_data (( X , y ), best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False def init_objective_UB ( self , **kwargs ) : # We have two models basically with different indexes if self . isPriv: self . _ init_objective_UB_LUPI ( **kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_UB ( **kwargs ) def init_objective_LB ( self , **kwargs ) : # We have two models basically with different indexes if self . isPriv: self . _ init_objective_LB_LUPI ( **kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super (). init_objective_LB ( **kwargs ) @abstractmethod def _ init_objective_LB_LUPI ( self , **kwargs ) : pass @abstractmethod def _ init_objective_UB_LUPI ( self , **kwargs ) : pass def split_dataset ( X_combined , lupi_features ) : assert X_combined . shape [ 1 ] > lupi_features X = X_combined [ : , :- lupi_features ] X_priv = X_combined [ : , - lupi_features: ] return X , X_priv def is_lupi_feature ( di , data , best_model_state ) : lupi_features = best_model_state [ \"lupi_features\" ] X_combined , _ = data d = X_combined . shape [ 1 ] - lupi_features lupi_index = di - d return lupi_index >= 0","title":"Module fri.model.base_lupi"},{"location":"reference/fri/model/base_lupi/#functions","text":"","title":"Functions"},{"location":"reference/fri/model/base_lupi/#is_lupi_feature","text":"def ( di , data , best_model_state ) View Source def is_lupi_feature ( di , data , best_model_state ) : lupi_features = best_model_state [ \" lupi_features \" ] X_combined , _ = data d = X_combined . shape [ 1 ] - lupi_features lupi_index = di - d return lupi_index >= 0","title":"is_lupi_feature"},{"location":"reference/fri/model/base_lupi/#split_dataset","text":"def ( X_combined , lupi_features ) View Source def split_dataset ( X_combined , lupi_features ) : assert X_combined . shape [ 1 ] > lupi_features X = X_combined [ : , :- lupi_features ] X_priv = X_combined [ : , - lupi_features: ] return X , X_priv","title":"split_dataset"},{"location":"reference/fri/model/base_lupi/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/base_lupi/#lupi_relevance_cvxproblem","text":"class ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 ) Helper class that provides a standard way to create an ABC using inheritance.","title":"LUPI_Relevance_CVXProblem"},{"location":"reference/fri/model/base_lupi/#ancestors-in-mro","text":"fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/base_lupi/#descendants","text":"fri.model.lupi_classification.LUPI_Classification_Relevance_Bound fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression_Relevance_Bound fri.model.lupi_regression.LUPI_Regression_Relevance_Bound","title":"Descendants"},{"location":"reference/fri/model/base_lupi/#methods","text":"##### init_objective_LB ``` python3 def ( self , ** kwargs ) ``` ??? example \" View Source \" def init_objective_LB ( self , ** kwargs ) : # We have two models basically with different indexes if self . isPriv : self . _init_objective_LB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super () . init_objective_LB ( ** kwargs ) ##### init_objective_UB ``` python3 def ( self , ** kwargs ) ``` ??? example \" View Source \" def init_objective_UB ( self , ** kwargs ) : # We have two models basically with different indexes if self . isPriv : self . _init_objective_UB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super () . init_objective_UB ( ** kwargs ) ##### preprocessing_data ``` python3 def ( self , data , best_model_state ) ``` ??? example \" View Source \" def preprocessing_data ( self , data , best_model_state ) : lupi_features = best_model_state [ \" lupi_features \" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super () . preprocessing_data (( X , y ) , best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False View Source class LUPI_Relevance_CVXProblem ( Relevance_CVXProblem ) : def __init__ ( self , current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ) -> None : super () . __init__ ( current_feature , data , hyperparameters , best_model_constraints , preset_model , best_model_state , probeID , ) def preprocessing_data ( self , data , best_model_state ) : lupi_features = best_model_state [ \" lupi_features \" ] X_combined , y = data X , X_priv = split_dataset ( X_combined , lupi_features ) self . X_priv = X_priv super () . preprocessing_data (( X , y ) , best_model_state ) assert lupi_features == X_priv . shape [ 1 ] self . d_priv = lupi_features # LUPI model , we need to offset the index self . lupi_index = self . current_feature - self . d if self . lupi_index >= 0 : self . isPriv = True else : self . isPriv = False def init_objective_UB ( self , ** kwargs ) : # We have two models basically with different indexes if self . isPriv : self . _init_objective_UB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super () . init_objective_UB ( ** kwargs ) def init_objective_LB ( self , ** kwargs ) : # We have two models basically with different indexes if self . isPriv : self . _init_objective_LB_LUPI ( ** kwargs ) else : # We call sibling class of our lupi class , which is the normal problem super () . init_objective_LB ( ** kwargs ) @ abstractmethod def _init_objective_LB_LUPI ( self , ** kwargs ) : pass @ abstractmethod def _init_objective_UB_LUPI ( self , ** kwargs ) : pass","title":"Methods"},{"location":"reference/fri/model/base_type/","text":"Module fri.model.base_type View Source from abc import ABC , abstractmethod import scipy.stats class ProblemType ( ABC ): def __init__ ( self , ** kwargs ): self . chosen_parameters_ = {} for p in self . parameters (): if p in kwargs : if kwargs [ p ] is not None : self . chosen_parameters_ [ p ] = kwargs [ p ] self . relax_factors_ = {} for p in self . relax_factors (): if p in kwargs : if kwargs [ p ] is not None : self . relax_factors_ [ p ] = kwargs [ p ] @classmethod @abstractmethod def parameters ( cls ): raise NotImplementedError def get_chosen_parameter ( self , p ): try : return [ self . chosen_parameters_ [ p ] ] # We return list for param search function except : # # TODO: rewrite the parameter logic # # TODO: move this to subclass if p == \"scaling_lupi_w\" : # return [0.1, 1, 10, 100, 1000] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\": # # value 0>p<1 causes standard svm solution # # p>1 encourages usage of lupi function # return scipy.stats.reciprocal(a=1e-15, b=1e15) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0 , 0.001 , 0.01 , 0.1 , 1 , 10 , 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters ()} @classmethod @abstractmethod def relax_factors ( cls ): raise NotImplementedError def get_chosen_relax_factors ( self , p ): try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors ()} @property @abstractmethod def get_initmodel_template ( self ): pass @property @abstractmethod def get_cvxproblem_template ( self ): pass @abstractmethod def preprocessing ( self , data , lupi_features = None ): return data def postprocessing ( self , bounds ): return bounds def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items ()} def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key )) Classes ProblemType class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) abc.ABC Descendants fri.model.classification.Classification fri.model.lupi_classification.LUPI_Classification fri.model.ordinal_regression.OrdinalRegression fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression fri.model.regression.Regression fri.model.lupi_regression.LUPI_Regression Static methods ##### parameters ``` python3 def ( ) ``` ??? example \"View Source\" @classmethod @abstractmethod def parameters ( cls ) : raise NotImplementedError ##### relax_factors ``` python3 def ( ) ``` ??? example \"View Source\" @classmethod @abstractmethod def relax_factors ( cls ) : raise NotImplementedError Instance variables ``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : Methods ##### get_all_parameters ``` python3 def ( self ) ``` ??? example \" View Source \" def get_all_parameters ( self ) : return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } ##### get_all_relax_factors ``` python3 def ( self ) ``` ??? example \" View Source \" def get_all_relax_factors ( self ) : return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } ##### get_chosen_parameter ``` python3 def ( self , p ) ``` ??? example \" View Source \" def get_chosen_parameter ( self , p ) : try : return [ self . chosen_parameters_ [ p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \" scaling_lupi_w \" : # return [ 0 . 1 , 1 , 10 , 100 , 1000 ] return scipy . stats . reciprocal ( a = 1 e - 15 , b = 1 e10 ) # if p == \" scaling_lupi_loss \" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1 e - 15 , b = 1 e15 ) if p == \" C \" : return scipy . stats . reciprocal ( a = 1 e - 5 , b = 1 e5 ) if p == \" epsilon \" : return [ 0 , 0 . 001 , 0 . 01 , 0 . 1 , 1 , 10 , 100 ] else : return scipy . stats . reciprocal ( a = 1 e - 10 , b = 1 e10 ) ##### get_chosen_relax_factors ``` python3 def ( self , p ) ``` ??? example \" View Source \" def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \" _slack \" ] except KeyError : factor = 0 . 1 if factor < 0 : raise ValueError ( \" Slack Factor multiplier is positive! \" ) return factor ##### get_relaxed_constraints ``` python3 def ( self , constraints ) ``` ??? example \" View Source \" def get_relaxed_constraints ( self , constraints ) : return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } ##### postprocessing ``` python3 def ( self , bounds ) ``` ??? example \" View Source \" def postprocessing ( self , bounds ) : return bounds ##### preprocessing ``` python3 def ( self , data , lupi_features = None ) ``` ??? example \" View Source \" @ abstractmethod def preprocessing ( self , data , lupi_features = None ) : return data ##### relax_constraint ``` python3 def ( self , key , value ) ``` ??? example \" View Source \" def relax_constraint ( self , key , value ) : return value * ( 1 + self . get_chosen_relax_factors ( key )) View Source class ProblemType ( ABC ) : def __init__ ( self , ** kwargs ) : self . chosen_parameters_ = {} for p in self . parameters () : if p in kwargs : if kwargs [ p ] is not None : self . chosen_parameters_ [ p ] = kwargs [ p ] self . relax_factors_ = {} for p in self . relax_factors () : if p in kwargs : if kwargs [ p ] is not None : self . relax_factors_ [ p ] = kwargs [ p ] @ classmethod @ abstractmethod def parameters ( cls ) : raise NotImplementedError def get_chosen_parameter ( self , p ) : try : return [ self . chosen_parameters_ [ p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \" scaling_lupi_w \" : # return [ 0 . 1 , 1 , 10 , 100 , 1000 ] return scipy . stats . reciprocal ( a = 1 e - 15 , b = 1 e10 ) # if p == \" scaling_lupi_loss \" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1 e - 15 , b = 1 e15 ) if p == \" C \" : return scipy . stats . reciprocal ( a = 1 e - 5 , b = 1 e5 ) if p == \" epsilon \" : return [ 0 , 0 . 001 , 0 . 01 , 0 . 1 , 1 , 10 , 100 ] else : return scipy . stats . reciprocal ( a = 1 e - 10 , b = 1 e10 ) def get_all_parameters ( self ) : return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } @ classmethod @ abstractmethod def relax_factors ( cls ) : raise NotImplementedError def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \" _slack \" ] except KeyError : factor = 0 . 1 if factor < 0 : raise ValueError ( \" Slack Factor multiplier is positive! \" ) return factor def get_all_relax_factors ( self ) : return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } @ property @ abstractmethod def get_initmodel_template ( self ) : pass @ property @ abstractmethod def get_cvxproblem_template ( self ) : pass @ abstractmethod def preprocessing ( self , data , lupi_features = None ) : return data def postprocessing ( self , bounds ) : return bounds def get_relaxed_constraints ( self , constraints ) : return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } def relax_constraint ( self , key , value ) : return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"Base Type"},{"location":"reference/fri/model/base_type/#module-frimodelbase_type","text":"View Source from abc import ABC , abstractmethod import scipy.stats class ProblemType ( ABC ): def __init__ ( self , ** kwargs ): self . chosen_parameters_ = {} for p in self . parameters (): if p in kwargs : if kwargs [ p ] is not None : self . chosen_parameters_ [ p ] = kwargs [ p ] self . relax_factors_ = {} for p in self . relax_factors (): if p in kwargs : if kwargs [ p ] is not None : self . relax_factors_ [ p ] = kwargs [ p ] @classmethod @abstractmethod def parameters ( cls ): raise NotImplementedError def get_chosen_parameter ( self , p ): try : return [ self . chosen_parameters_ [ p ] ] # We return list for param search function except : # # TODO: rewrite the parameter logic # # TODO: move this to subclass if p == \"scaling_lupi_w\" : # return [0.1, 1, 10, 100, 1000] return scipy . stats . reciprocal ( a = 1e-15 , b = 1e10 ) # if p == \"scaling_lupi_loss\": # # value 0>p<1 causes standard svm solution # # p>1 encourages usage of lupi function # return scipy.stats.reciprocal(a=1e-15, b=1e15) if p == \"C\" : return scipy . stats . reciprocal ( a = 1e-5 , b = 1e5 ) if p == \"epsilon\" : return [ 0 , 0.001 , 0.01 , 0.1 , 1 , 10 , 100 ] else : return scipy . stats . reciprocal ( a = 1e-10 , b = 1e10 ) def get_all_parameters ( self ): return { p : self . get_chosen_parameter ( p ) for p in self . parameters ()} @classmethod @abstractmethod def relax_factors ( cls ): raise NotImplementedError def get_chosen_relax_factors ( self , p ): try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \"_slack\" ] except KeyError : factor = 0.1 if factor < 0 : raise ValueError ( \"Slack Factor multiplier is positive!\" ) return factor def get_all_relax_factors ( self ): return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors ()} @property @abstractmethod def get_initmodel_template ( self ): pass @property @abstractmethod def get_cvxproblem_template ( self ): pass @abstractmethod def preprocessing ( self , data , lupi_features = None ): return data def postprocessing ( self , bounds ): return bounds def get_relaxed_constraints ( self , constraints ): return { c : self . relax_constraint ( c , v ) for c , v in constraints . items ()} def relax_constraint ( self , key , value ): return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"Module fri.model.base_type"},{"location":"reference/fri/model/base_type/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/base_type/#problemtype","text":"class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"ProblemType"},{"location":"reference/fri/model/base_type/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/base_type/#descendants","text":"fri.model.classification.Classification fri.model.lupi_classification.LUPI_Classification fri.model.ordinal_regression.OrdinalRegression fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression fri.model.regression.Regression fri.model.lupi_regression.LUPI_Regression","title":"Descendants"},{"location":"reference/fri/model/base_type/#static-methods","text":"##### parameters ``` python3 def ( ) ``` ??? example \"View Source\" @classmethod @abstractmethod def parameters ( cls ) : raise NotImplementedError ##### relax_factors ``` python3 def ( ) ``` ??? example \"View Source\" @classmethod @abstractmethod def relax_factors ( cls ) : raise NotImplementedError","title":"Static methods"},{"location":"reference/fri/model/base_type/#instance-variables","text":"``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` :","title":"Instance variables"},{"location":"reference/fri/model/base_type/#methods","text":"##### get_all_parameters ``` python3 def ( self ) ``` ??? example \" View Source \" def get_all_parameters ( self ) : return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } ##### get_all_relax_factors ``` python3 def ( self ) ``` ??? example \" View Source \" def get_all_relax_factors ( self ) : return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } ##### get_chosen_parameter ``` python3 def ( self , p ) ``` ??? example \" View Source \" def get_chosen_parameter ( self , p ) : try : return [ self . chosen_parameters_ [ p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \" scaling_lupi_w \" : # return [ 0 . 1 , 1 , 10 , 100 , 1000 ] return scipy . stats . reciprocal ( a = 1 e - 15 , b = 1 e10 ) # if p == \" scaling_lupi_loss \" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1 e - 15 , b = 1 e15 ) if p == \" C \" : return scipy . stats . reciprocal ( a = 1 e - 5 , b = 1 e5 ) if p == \" epsilon \" : return [ 0 , 0 . 001 , 0 . 01 , 0 . 1 , 1 , 10 , 100 ] else : return scipy . stats . reciprocal ( a = 1 e - 10 , b = 1 e10 ) ##### get_chosen_relax_factors ``` python3 def ( self , p ) ``` ??? example \" View Source \" def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \" _slack \" ] except KeyError : factor = 0 . 1 if factor < 0 : raise ValueError ( \" Slack Factor multiplier is positive! \" ) return factor ##### get_relaxed_constraints ``` python3 def ( self , constraints ) ``` ??? example \" View Source \" def get_relaxed_constraints ( self , constraints ) : return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } ##### postprocessing ``` python3 def ( self , bounds ) ``` ??? example \" View Source \" def postprocessing ( self , bounds ) : return bounds ##### preprocessing ``` python3 def ( self , data , lupi_features = None ) ``` ??? example \" View Source \" @ abstractmethod def preprocessing ( self , data , lupi_features = None ) : return data ##### relax_constraint ``` python3 def ( self , key , value ) ``` ??? example \" View Source \" def relax_constraint ( self , key , value ) : return value * ( 1 + self . get_chosen_relax_factors ( key )) View Source class ProblemType ( ABC ) : def __init__ ( self , ** kwargs ) : self . chosen_parameters_ = {} for p in self . parameters () : if p in kwargs : if kwargs [ p ] is not None : self . chosen_parameters_ [ p ] = kwargs [ p ] self . relax_factors_ = {} for p in self . relax_factors () : if p in kwargs : if kwargs [ p ] is not None : self . relax_factors_ [ p ] = kwargs [ p ] @ classmethod @ abstractmethod def parameters ( cls ) : raise NotImplementedError def get_chosen_parameter ( self , p ) : try : return [ self . chosen_parameters_ [ p ] ] # We return list for param search function except : # # TODO : rewrite the parameter logic # # TODO : move this to subclass if p == \" scaling_lupi_w \" : # return [ 0 . 1 , 1 , 10 , 100 , 1000 ] return scipy . stats . reciprocal ( a = 1 e - 15 , b = 1 e10 ) # if p == \" scaling_lupi_loss \" : # # value 0 > p < 1 causes standard svm solution # # p > 1 encourages usage of lupi function # return scipy . stats . reciprocal ( a = 1 e - 15 , b = 1 e15 ) if p == \" C \" : return scipy . stats . reciprocal ( a = 1 e - 5 , b = 1 e5 ) if p == \" epsilon \" : return [ 0 , 0 . 001 , 0 . 01 , 0 . 1 , 1 , 10 , 100 ] else : return scipy . stats . reciprocal ( a = 1 e - 10 , b = 1 e10 ) def get_all_parameters ( self ) : return { p : self . get_chosen_parameter ( p ) for p in self . parameters () } @ classmethod @ abstractmethod def relax_factors ( cls ) : raise NotImplementedError def get_chosen_relax_factors ( self , p ) : try : factor = self . relax_factors_ [ p ] except KeyError : try : factor = self . relax_factors_ [ p + \" _slack \" ] except KeyError : factor = 0 . 1 if factor < 0 : raise ValueError ( \" Slack Factor multiplier is positive! \" ) return factor def get_all_relax_factors ( self ) : return { p : self . get_chosen_relax_factors ( p ) for p in self . relax_factors () } @ property @ abstractmethod def get_initmodel_template ( self ) : pass @ property @ abstractmethod def get_cvxproblem_template ( self ) : pass @ abstractmethod def preprocessing ( self , data , lupi_features = None ) : return data def postprocessing ( self , bounds ) : return bounds def get_relaxed_constraints ( self , constraints ) : return { c : self . relax_constraint ( c , v ) for c , v in constraints . items () } def relax_constraint ( self , key , value ) : return value * ( 1 + self . get_chosen_relax_factors ( key ))","title":"Methods"},{"location":"reference/fri/model/classification/","text":"Module fri.model.classification View Source import cvxpy as cvx import numpy as np from sklearn import preprocessing from sklearn.metrics import fbeta_score , classification_report from sklearn.preprocessing import LabelEncoder from sklearn.utils import check_X_y from sklearn.utils.multiclass import unique_labels from fri.model.base_cvxproblem import Relevance_CVXProblem from fri.model.base_initmodel import InitModel from .base_type import ProblemType class Classification ( ProblemType ): @classmethod def parameters ( cls ): return [ \"C\" ] @property def get_initmodel_template ( cls ): return Classification_SVM @property def get_cvxproblem_template ( cls ): return Classification_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to -1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y class Classification_SVM ( InitModel ): @classmethod def hyperparameter ( cls ): return [ \"C\" ] def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . hyperparam [ \"C\" ] w = cvx . Variable ( shape = ( d ), name = \"w\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) b = cvx . Variable ( name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . multiply ( y . T , X * w + b ) >= 1 - slack , slack >= 0 ] # Solve problem. solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b = b . value slack = np . asarray ( slack . value ) . flatten () self . model_state = { \"w\" : w , \"b\" : b , \"slack\" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ): w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b >= 0 y = y . astype ( int ) y [ y == 0 ] = - 1 return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) # Negative class is set to -1 for decision surface y = LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \"weighted\" ) if \"verbose\" in kwargs : return classification_report ( y , prediction ) return score class Classification_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] # New Variables self . w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) self . b = cvx . Variable ( name = \"b\" ) self . slack = cvx . Variable ( shape = ( self . n ), nonneg = True , name = \"slack\" ) # New Constraints distance_from_plane = cvx . multiply ( self . y , self . X * self . w + self . b ) self . loss = cvx . sum ( self . slack ) self . weight_norm = cvx . norm ( self . w , 1 ) self . add_constraint ( distance_from_plane >= 1 - self . slack ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Classes Classification class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods ##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" ] Instance variables ``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : Methods ##### preprocessing ``` python3 def ( self , data , ** kwargs ) ``` ??? example \" View Source \" def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \" Only binary class data supported \" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class Classification ( ProblemType ) : @ classmethod def parameters ( cls ) : return [ \" C \" ] @ property def get_initmodel_template ( cls ) : return Classification_SVM @ property def get_cvxproblem_template ( cls ) : return Classification_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \" Only binary class data supported \" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y Classification_Relevance_Bound class ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC Descendants fri.model.lupi_classification.LUPI_Classification_Relevance_Bound Methods ##### init_objective_LB ``` python3 def ( self , ** kwargs ) ``` ??? example \"View Source\" def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) ##### init_objective_UB ``` python3 def ( self , sign = None , ** kwargs ) ``` ??? example \"View Source\" def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) View Source class Classification_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] # New Variables self . w = cvx . Variable ( shape =( self . d ), name = \"w\" ) self . b = cvx . Variable ( name = \"b\" ) self . slack = cvx . Variable ( shape =( self . n ), nonneg = True , name = \"slack\" ) # New Constraints distance_from_plane = cvx . multiply ( self . y , self . X * self . w + self . b ) self . loss = cvx . sum ( self . slack ) self . weight_norm = cvx . norm ( self . w , 1 ) self . add_constraint ( distance_from_plane >= 1 - self . slack ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Classification_SVM class ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator Static methods ##### hyperparameter ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def hyperparameter ( cls ) : return [ \" C \" ] Methods ##### fit ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" def fit ( self , X , y , ** kwargs ) : ( n , d ) = X . shape C = self . hyperparam [ \" C \" ] w = cvx . Variable ( shape = ( d ) , name = \" w \" ) slack = cvx . Variable ( shape = ( n ) , name = \" slack \" ) b = cvx . Variable ( name = \" bias \" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . multiply ( y . T , X * w + b ) >= 1 - slack , slack >= 0 ] # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b = b . value slack = np . asarray ( slack . value ) . flatten () self . model_state = { \" w \" : w , \" b \" : b , \" slack \" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \" loss \" : loss , \" w_l1 \" : w_l1 } return self ##### predict ``` python3 def ( self , X ) ``` ??? example \" View Source \" def predict ( self , X ) : w = self . model_state [ \" w \" ] b = self . model_state [ \" b \" ] y = np . dot ( X , w ) + b >= 0 y = y . astype ( int ) y [ y == 0 ] = - 1 return y ##### score ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) # Negative class is set to - 1 for decision surface y = LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \" weighted \" ) if \" verbose \" in kwargs : return classification_report ( y , prediction ) return score View Source class Classification_SVM ( InitModel ) : @ classmethod def hyperparameter ( cls ) : return [ \" C \" ] def fit ( self , X , y , ** kwargs ) : ( n , d ) = X . shape C = self . hyperparam [ \" C \" ] w = cvx . Variable ( shape = ( d ) , name = \" w \" ) slack = cvx . Variable ( shape = ( n ) , name = \" slack \" ) b = cvx . Variable ( name = \" bias \" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . multiply ( y . T , X * w + b ) >= 1 - slack , slack >= 0 ] # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b = b . value slack = np . asarray ( slack . value ) . flatten () self . model_state = { \" w \" : w , \" b \" : b , \" slack \" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \" loss \" : loss , \" w_l1 \" : w_l1 } return self def predict ( self , X ) : w = self . model_state [ \" w \" ] b = self . model_state [ \" b \" ] y = np . dot ( X , w ) + b >= 0 y = y . astype ( int ) y [ y == 0 ] = - 1 return y def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) # Negative class is set to - 1 for decision surface y = LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \" weighted \" ) if \" verbose \" in kwargs : return classification_report ( y , prediction ) return score","title":"Classification"},{"location":"reference/fri/model/classification/#module-frimodelclassification","text":"View Source import cvxpy as cvx import numpy as np from sklearn import preprocessing from sklearn.metrics import fbeta_score , classification_report from sklearn.preprocessing import LabelEncoder from sklearn.utils import check_X_y from sklearn.utils.multiclass import unique_labels from fri.model.base_cvxproblem import Relevance_CVXProblem from fri.model.base_initmodel import InitModel from .base_type import ProblemType class Classification ( ProblemType ): @classmethod def parameters ( cls ): return [ \"C\" ] @property def get_initmodel_template ( cls ): return Classification_SVM @property def get_cvxproblem_template ( cls ): return Classification_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to -1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y class Classification_SVM ( InitModel ): @classmethod def hyperparameter ( cls ): return [ \"C\" ] def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . hyperparam [ \"C\" ] w = cvx . Variable ( shape = ( d ), name = \"w\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) b = cvx . Variable ( name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . multiply ( y . T , X * w + b ) >= 1 - slack , slack >= 0 ] # Solve problem. solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b = b . value slack = np . asarray ( slack . value ) . flatten () self . model_state = { \"w\" : w , \"b\" : b , \"slack\" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ): w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b >= 0 y = y . astype ( int ) y [ y == 0 ] = - 1 return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) # Negative class is set to -1 for decision surface y = LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \"weighted\" ) if \"verbose\" in kwargs : return classification_report ( y , prediction ) return score class Classification_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] # New Variables self . w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) self . b = cvx . Variable ( name = \"b\" ) self . slack = cvx . Variable ( shape = ( self . n ), nonneg = True , name = \"slack\" ) # New Constraints distance_from_plane = cvx . multiply ( self . y , self . X * self . w + self . b ) self . loss = cvx . sum ( self . slack ) self . weight_norm = cvx . norm ( self . w , 1 ) self . add_constraint ( distance_from_plane >= 1 - self . slack ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"Module fri.model.classification"},{"location":"reference/fri/model/classification/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/classification/#classification","text":"class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"Classification"},{"location":"reference/fri/model/classification/#ancestors-in-mro","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/classification/#static-methods","text":"##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" ]","title":"Static methods"},{"location":"reference/fri/model/classification/#instance-variables","text":"``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` :","title":"Instance variables"},{"location":"reference/fri/model/classification/#methods","text":"##### preprocessing ``` python3 def ( self , data , ** kwargs ) ``` ??? example \" View Source \" def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \" Only binary class data supported \" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class Classification ( ProblemType ) : @ classmethod def parameters ( cls ) : return [ \" C \" ] @ property def get_initmodel_template ( cls ) : return Classification_SVM @ property def get_cvxproblem_template ( cls ) : return Classification_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \" Only binary class data supported \" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y","title":"Methods"},{"location":"reference/fri/model/classification/#classification_relevance_bound","text":"class ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"Classification_Relevance_Bound"},{"location":"reference/fri/model/classification/#ancestors-in-mro_1","text":"fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/classification/#descendants","text":"fri.model.lupi_classification.LUPI_Classification_Relevance_Bound","title":"Descendants"},{"location":"reference/fri/model/classification/#methods_1","text":"##### init_objective_LB ``` python3 def ( self , ** kwargs ) ``` ??? example \"View Source\" def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) ##### init_objective_UB ``` python3 def ( self , sign = None , ** kwargs ) ``` ??? example \"View Source\" def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) View Source class Classification_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] # New Variables self . w = cvx . Variable ( shape =( self . d ), name = \"w\" ) self . b = cvx . Variable ( name = \"b\" ) self . slack = cvx . Variable ( shape =( self . n ), nonneg = True , name = \"slack\" ) # New Constraints distance_from_plane = cvx . multiply ( self . y , self . X * self . w + self . b ) self . loss = cvx . sum ( self . slack ) self . weight_norm = cvx . norm ( self . w , 1 ) self . add_constraint ( distance_from_plane >= 1 - self . slack ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"Methods"},{"location":"reference/fri/model/classification/#classification_svm","text":"class ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance.","title":"Classification_SVM"},{"location":"reference/fri/model/classification/#ancestors-in-mro_2","text":"fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/classification/#static-methods_1","text":"##### hyperparameter ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def hyperparameter ( cls ) : return [ \" C \" ]","title":"Static methods"},{"location":"reference/fri/model/classification/#methods_2","text":"##### fit ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" def fit ( self , X , y , ** kwargs ) : ( n , d ) = X . shape C = self . hyperparam [ \" C \" ] w = cvx . Variable ( shape = ( d ) , name = \" w \" ) slack = cvx . Variable ( shape = ( n ) , name = \" slack \" ) b = cvx . Variable ( name = \" bias \" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . multiply ( y . T , X * w + b ) >= 1 - slack , slack >= 0 ] # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b = b . value slack = np . asarray ( slack . value ) . flatten () self . model_state = { \" w \" : w , \" b \" : b , \" slack \" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \" loss \" : loss , \" w_l1 \" : w_l1 } return self ##### predict ``` python3 def ( self , X ) ``` ??? example \" View Source \" def predict ( self , X ) : w = self . model_state [ \" w \" ] b = self . model_state [ \" b \" ] y = np . dot ( X , w ) + b >= 0 y = y . astype ( int ) y [ y == 0 ] = - 1 return y ##### score ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) # Negative class is set to - 1 for decision surface y = LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \" weighted \" ) if \" verbose \" in kwargs : return classification_report ( y , prediction ) return score View Source class Classification_SVM ( InitModel ) : @ classmethod def hyperparameter ( cls ) : return [ \" C \" ] def fit ( self , X , y , ** kwargs ) : ( n , d ) = X . shape C = self . hyperparam [ \" C \" ] w = cvx . Variable ( shape = ( d ) , name = \" w \" ) slack = cvx . Variable ( shape = ( n ) , name = \" slack \" ) b = cvx . Variable ( name = \" bias \" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . multiply ( y . T , X * w + b ) >= 1 - slack , slack >= 0 ] # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b = b . value slack = np . asarray ( slack . value ) . flatten () self . model_state = { \" w \" : w , \" b \" : b , \" slack \" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \" loss \" : loss , \" w_l1 \" : w_l1 } return self def predict ( self , X ) : w = self . model_state [ \" w \" ] b = self . model_state [ \" b \" ] y = np . dot ( X , w ) + b >= 0 y = y . astype ( int ) y [ y == 0 ] = - 1 return y def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) # Negative class is set to - 1 for decision surface y = LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \" weighted \" ) if \" verbose \" in kwargs : return classification_report ( y , prediction ) return score","title":"Methods"},{"location":"reference/fri/model/lupi_classification/","text":"Module fri.model.lupi_classification View Source import cvxpy as cvx import numpy as np from sklearn import preprocessing from sklearn.metrics import fbeta_score , classification_report from sklearn.preprocessing import LabelEncoder from sklearn.utils import check_X_y from sklearn.utils.multiclass import unique_labels from .base_initmodel import InitModel from .base_lupi import LUPI_Relevance_CVXProblem , split_dataset from .base_type import ProblemType from .classification import Classification_Relevance_Bound class LUPI_Classification ( ProblemType ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ): return self . _lupi_features @classmethod def parameters ( cls ): return [ \"C\" , \"scaling_lupi_w\" ] @property def get_initmodel_template ( cls ): return LUPI_Classification_SVM @property def get_cvxproblem_template ( cls ): return LUPI_Classification_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to -1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y class LUPI_Classification_SVM ( InitModel ): @classmethod def hyperparameter ( cls ): return [ \"C\" , \"scaling_lupi_w\" ] def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : raise ValueError ( \"No lupi_features argument given.\" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \"C\" ] scaling_lupi_w = self . hyperparam [ \"scaling_lupi_w\" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) w_priv = cvx . Variable ( lupi_features , name = \"w_priv\" ) b = cvx . Variable ( name = \"bias\" ) b_priv = cvx . Variable ( name = \"bias_priv\" ) # Define functions for better readability function = X * w + b priv_function = X_priv * w_priv + b_priv # Combined loss of lupi function and normal slacks, scaled by two constants loss = cvx . sum ( priv_function ) # L1 norm regularization of both functions with 1 scaling constant w_l1 = cvx . norm ( w , 1 ) w_priv_l1 = cvx . norm ( w_priv , 1 ) weight_regularization = 0.5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [ cvx . multiply ( y . T , function ) >= 1 - cvx . multiply ( y . T , priv_function ), priv_function >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem. solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value w_priv = w_priv . value b = b . value b_priv = b_priv . value self . model_state = { \"w\" : w , \"w_priv\" : w_priv , \"b\" : b , \"b_priv\" : b_priv , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO: Move this somewhere else } loss = loss . value w_l1 = w_l1 . value w_priv_l1 = w_priv_l1 . value self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 , \"w_priv_l1\" : w_priv_l1 } return self def predict ( self , X ): X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] # Simple hyperplane classification rule f = np . dot ( X , w ) + b y = f >= 0 y = y . astype ( int ) # Format binary as signed unit vector y [ y == 0 ] = - 1 return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) # Negative class is set to -1 for decision surface y = LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \"weighted\" ) if \"verbose\" in kwargs : return classification_report ( y , prediction ) return score class LUPI_Classification_Relevance_Bound ( LUPI_Relevance_CVXProblem , Classification_Relevance_Bound ): def _init_objective_UB_LUPI ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w_priv [ self . lupi_index ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_objective_LB_LUPI ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w_priv [ self . lupi_index ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from best initial model l1_w = init_model_constraints [ \"w_l1\" ] l1_priv_w = init_model_constraints [ \"w_priv_l1\" ] init_loss = init_model_constraints [ \"loss\" ] # New Variables w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) w_priv = cvx . Variable ( shape = ( self . d_priv ), name = \"w_priv\" ) b = cvx . Variable ( name = \"b\" ) b_priv = cvx . Variable ( name = \"b_priv\" ) # New Constraints function = cvx . multiply ( self . y . T , self . X * w + b ) priv_function = self . X_priv * w_priv + b_priv loss = cvx . sum ( priv_function ) weight_norm = cvx . norm ( w , 1 ) weight_norm_priv = cvx . norm ( w_priv , 1 ) self . add_constraint ( function >= 1 - cvx . multiply ( self . y . T , priv_function )) self . add_constraint ( priv_function >= 0 ) self . add_constraint ( loss <= init_loss ) self . add_constraint ( weight_norm <= l1_w ) self . add_constraint ( weight_norm_priv <= l1_priv_w ) # Save values for object use later self . w = w self . w_priv = w_priv self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Classes LUPI_Classification class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods ##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] Instance variables ``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : ``` python3 lupi_features ``` : Methods ##### preprocessing ``` python3 def ( self , data , lupi_features = None ) ``` ??? example \" View Source \" def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \" Only binary class data supported \" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class LUPI_Classification ( ProblemType ) : def __init__ ( self , ** kwargs ) : super () . __init__ ( ** kwargs ) self . _lupi_features = None @ property def lupi_features ( self ) : return self . _lupi_features @ classmethod def parameters ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] @ property def get_initmodel_template ( cls ) : return LUPI_Classification_SVM @ property def get_cvxproblem_template ( cls ) : return LUPI_Classification_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \" Only binary class data supported \" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y LUPI_Classification_Relevance_Bound class ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_lupi.LUPI_Relevance_CVXProblem fri.model.classification.Classification_Relevance_Bound fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC View Source class LUPI_Classification_Relevance_Bound ( LUPI_Relevance_CVXProblem , Classification_Relevance_Bound ) : def _init_objective_UB_LUPI ( self , sign = None , ** kwargs ) : self . add_constraint ( self . feature_relevance <= sign * self . w_priv [ self . lupi_index ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_objective_LB_LUPI ( self , ** kwargs ) : self . add_constraint ( cvx . abs ( self . w_priv [ self . lupi_index ] ) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ) : # Upper constraints from best initial model l1_w = init_model_constraints [ \" w_l1 \" ] l1_priv_w = init_model_constraints [ \" w_priv_l1 \" ] init_loss = init_model_constraints [ \" loss \" ] # New Variables w = cvx . Variable ( shape = ( self . d ) , name = \" w \" ) w_priv = cvx . Variable ( shape = ( self . d_priv ) , name = \" w_priv \" ) b = cvx . Variable ( name = \" b \" ) b_priv = cvx . Variable ( name = \" b_priv \" ) # New Constraints function = cvx . multiply ( self . y . T , self . X * w + b ) priv_function = self . X_priv * w_priv + b_priv loss = cvx . sum ( priv_function ) weight_norm = cvx . norm ( w , 1 ) weight_norm_priv = cvx . norm ( w_priv , 1 ) self . add_constraint ( function >= 1 - cvx . multiply ( self . y . T , priv_function )) self . add_constraint ( priv_function >= 0 ) self . add_constraint ( loss <= init_loss ) self . add_constraint ( weight_norm <= l1_w ) self . add_constraint ( weight_norm_priv <= l1_priv_w ) # Save values for object use later self . w = w self . w_priv = w_priv self . feature_relevance = cvx . Variable ( nonneg = True , name = \" Feature Relevance \" ) LUPI_Classification_SVM class ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator Static methods ##### hyperparameter ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def hyperparameter ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] Methods ##### fit ``` python3 def ( self , X_combined , y , lupi_features = None ) ``` Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . ??? example \" View Source \" def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . \"\"\" if lupi_features is None : raise ValueError ( \" No lupi_features argument given. \" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \" C \" ] scaling_lupi_w = self . hyperparam [ \" scaling_lupi_w \" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ) , name = \" w \" ) w_priv = cvx . Variable ( lupi_features , name = \" w_priv \" ) b = cvx . Variable ( name = \" bias \" ) b_priv = cvx . Variable ( name = \" bias_priv \" ) # Define functions for better readability function = X * w + b priv_function = X_priv * w_priv + b_priv # Combined loss of lupi function and normal slacks , scaled by two constants loss = cvx . sum ( priv_function ) # L1 norm regularization of both functions with 1 scaling constant w_l1 = cvx . norm ( w , 1 ) w_priv_l1 = cvx . norm ( w_priv , 1 ) weight_regularization = 0 . 5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [ cvx . multiply ( y . T , function ) >= 1 - cvx . multiply ( y . T , priv_function ) , priv_function >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value w_priv = w_priv . value b = b . value b_priv = b_priv . value self . model_state = { \" w \" : w , \" w_priv \" : w_priv , \" b \" : b , \" b_priv \" : b_priv , \" lupi_features \" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else } loss = loss . value w_l1 = w_l1 . value w_priv_l1 = w_priv_l1 . value self . constraints = { \" loss \" : loss , \" w_l1 \" : w_l1 , \" w_priv_l1 \" : w_priv_l1 } return self ##### predict ``` python3 def ( self , X ) ``` ??? example \" View Source \" def predict ( self , X ) : X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \" w \" ] b = self . model_state [ \" b \" ] # Simple hyperplane classification rule f = np . dot ( X , w ) + b y = f >= 0 y = y . astype ( int ) # Format binary as signed unit vector y [ y == 0 ] = - 1 return y ##### score ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) # Negative class is set to - 1 for decision surface y = LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \" weighted \" ) if \" verbose \" in kwargs : return classification_report ( y , prediction ) return score View Source class LUPI_Classification_SVM ( InitModel ) : @ classmethod def hyperparameter ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . \"\"\" if lupi_features is None : raise ValueError ( \" No lupi_features argument given. \" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \" C \" ] scaling_lupi_w = self . hyperparam [ \" scaling_lupi_w \" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ) , name = \" w \" ) w_priv = cvx . Variable ( lupi_features , name = \" w_priv \" ) b = cvx . Variable ( name = \" bias \" ) b_priv = cvx . Variable ( name = \" bias_priv \" ) # Define functions for better readability function = X * w + b priv_function = X_priv * w_priv + b_priv # Combined loss of lupi function and normal slacks , scaled by two constants loss = cvx . sum ( priv_function ) # L1 norm regularization of both functions with 1 scaling constant w_l1 = cvx . norm ( w , 1 ) w_priv_l1 = cvx . norm ( w_priv , 1 ) weight_regularization = 0 . 5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [ cvx . multiply ( y . T , function ) >= 1 - cvx . multiply ( y . T , priv_function ) , priv_function >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value w_priv = w_priv . value b = b . value b_priv = b_priv . value self . model_state = { \" w \" : w , \" w_priv \" : w_priv , \" b \" : b , \" b_priv \" : b_priv , \" lupi_features \" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else } loss = loss . value w_l1 = w_l1 . value w_priv_l1 = w_priv_l1 . value self . constraints = { \" loss \" : loss , \" w_l1 \" : w_l1 , \" w_priv_l1 \" : w_priv_l1 } return self def predict ( self , X ) : X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \" w \" ] b = self . model_state [ \" b \" ] # Simple hyperplane classification rule f = np . dot ( X , w ) + b y = f >= 0 y = y . astype ( int ) # Format binary as signed unit vector y [ y == 0 ] = - 1 return y def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) # Negative class is set to - 1 for decision surface y = LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \" weighted \" ) if \" verbose \" in kwargs : return classification_report ( y , prediction ) return score","title":"Lupi Classification"},{"location":"reference/fri/model/lupi_classification/#module-frimodellupi_classification","text":"View Source import cvxpy as cvx import numpy as np from sklearn import preprocessing from sklearn.metrics import fbeta_score , classification_report from sklearn.preprocessing import LabelEncoder from sklearn.utils import check_X_y from sklearn.utils.multiclass import unique_labels from .base_initmodel import InitModel from .base_lupi import LUPI_Relevance_CVXProblem , split_dataset from .base_type import ProblemType from .classification import Classification_Relevance_Bound class LUPI_Classification ( ProblemType ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ): return self . _lupi_features @classmethod def parameters ( cls ): return [ \"C\" , \"scaling_lupi_w\" ] @property def get_initmodel_template ( cls ): return LUPI_Classification_SVM @property def get_cvxproblem_template ( cls ): return LUPI_Classification_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \"Only binary class data supported\" ) # Negative class is set to -1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y class LUPI_Classification_SVM ( InitModel ): @classmethod def hyperparameter ( cls ): return [ \"C\" , \"scaling_lupi_w\" ] def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : raise ValueError ( \"No lupi_features argument given.\" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \"C\" ] scaling_lupi_w = self . hyperparam [ \"scaling_lupi_w\" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) w_priv = cvx . Variable ( lupi_features , name = \"w_priv\" ) b = cvx . Variable ( name = \"bias\" ) b_priv = cvx . Variable ( name = \"bias_priv\" ) # Define functions for better readability function = X * w + b priv_function = X_priv * w_priv + b_priv # Combined loss of lupi function and normal slacks, scaled by two constants loss = cvx . sum ( priv_function ) # L1 norm regularization of both functions with 1 scaling constant w_l1 = cvx . norm ( w , 1 ) w_priv_l1 = cvx . norm ( w_priv , 1 ) weight_regularization = 0.5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [ cvx . multiply ( y . T , function ) >= 1 - cvx . multiply ( y . T , priv_function ), priv_function >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem. solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value w_priv = w_priv . value b = b . value b_priv = b_priv . value self . model_state = { \"w\" : w , \"w_priv\" : w_priv , \"b\" : b , \"b_priv\" : b_priv , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO: Move this somewhere else } loss = loss . value w_l1 = w_l1 . value w_priv_l1 = w_priv_l1 . value self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 , \"w_priv_l1\" : w_priv_l1 } return self def predict ( self , X ): X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] # Simple hyperplane classification rule f = np . dot ( X , w ) + b y = f >= 0 y = y . astype ( int ) # Format binary as signed unit vector y [ y == 0 ] = - 1 return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) # Negative class is set to -1 for decision surface y = LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \"weighted\" ) if \"verbose\" in kwargs : return classification_report ( y , prediction ) return score class LUPI_Classification_Relevance_Bound ( LUPI_Relevance_CVXProblem , Classification_Relevance_Bound ): def _init_objective_UB_LUPI ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w_priv [ self . lupi_index ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_objective_LB_LUPI ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w_priv [ self . lupi_index ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from best initial model l1_w = init_model_constraints [ \"w_l1\" ] l1_priv_w = init_model_constraints [ \"w_priv_l1\" ] init_loss = init_model_constraints [ \"loss\" ] # New Variables w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) w_priv = cvx . Variable ( shape = ( self . d_priv ), name = \"w_priv\" ) b = cvx . Variable ( name = \"b\" ) b_priv = cvx . Variable ( name = \"b_priv\" ) # New Constraints function = cvx . multiply ( self . y . T , self . X * w + b ) priv_function = self . X_priv * w_priv + b_priv loss = cvx . sum ( priv_function ) weight_norm = cvx . norm ( w , 1 ) weight_norm_priv = cvx . norm ( w_priv , 1 ) self . add_constraint ( function >= 1 - cvx . multiply ( self . y . T , priv_function )) self . add_constraint ( priv_function >= 0 ) self . add_constraint ( loss <= init_loss ) self . add_constraint ( weight_norm <= l1_w ) self . add_constraint ( weight_norm_priv <= l1_priv_w ) # Save values for object use later self . w = w self . w_priv = w_priv self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"Module fri.model.lupi_classification"},{"location":"reference/fri/model/lupi_classification/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/lupi_classification/#lupi_classification","text":"class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"LUPI_Classification"},{"location":"reference/fri/model/lupi_classification/#ancestors-in-mro","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_classification/#static-methods","text":"##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ]","title":"Static methods"},{"location":"reference/fri/model/lupi_classification/#instance-variables","text":"``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : ``` python3 lupi_features ``` :","title":"Instance variables"},{"location":"reference/fri/model/lupi_classification/#methods","text":"##### preprocessing ``` python3 def ( self , data , lupi_features = None ) ``` ??? example \" View Source \" def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \" Only binary class data supported \" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class LUPI_Classification ( ProblemType ) : def __init__ ( self , ** kwargs ) : super () . __init__ ( ** kwargs ) self . _lupi_features = None @ property def lupi_features ( self ) : return self . _lupi_features @ classmethod def parameters ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] @ property def get_initmodel_template ( cls ) : return LUPI_Classification_SVM @ property def get_cvxproblem_template ( cls ) : return LUPI_Classification_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) # Store the classes seen during fit classes_ = unique_labels ( y ) if len ( classes_ ) > 2 : raise ValueError ( \" Only binary class data supported \" ) # Negative class is set to - 1 for decision surface y = preprocessing . LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 return X , y","title":"Methods"},{"location":"reference/fri/model/lupi_classification/#lupi_classification_relevance_bound","text":"class ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 ) Helper class that provides a standard way to create an ABC using inheritance.","title":"LUPI_Classification_Relevance_Bound"},{"location":"reference/fri/model/lupi_classification/#ancestors-in-mro_1","text":"fri.model.base_lupi.LUPI_Relevance_CVXProblem fri.model.classification.Classification_Relevance_Bound fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC View Source class LUPI_Classification_Relevance_Bound ( LUPI_Relevance_CVXProblem , Classification_Relevance_Bound ) : def _init_objective_UB_LUPI ( self , sign = None , ** kwargs ) : self . add_constraint ( self . feature_relevance <= sign * self . w_priv [ self . lupi_index ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_objective_LB_LUPI ( self , ** kwargs ) : self . add_constraint ( cvx . abs ( self . w_priv [ self . lupi_index ] ) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ) : # Upper constraints from best initial model l1_w = init_model_constraints [ \" w_l1 \" ] l1_priv_w = init_model_constraints [ \" w_priv_l1 \" ] init_loss = init_model_constraints [ \" loss \" ] # New Variables w = cvx . Variable ( shape = ( self . d ) , name = \" w \" ) w_priv = cvx . Variable ( shape = ( self . d_priv ) , name = \" w_priv \" ) b = cvx . Variable ( name = \" b \" ) b_priv = cvx . Variable ( name = \" b_priv \" ) # New Constraints function = cvx . multiply ( self . y . T , self . X * w + b ) priv_function = self . X_priv * w_priv + b_priv loss = cvx . sum ( priv_function ) weight_norm = cvx . norm ( w , 1 ) weight_norm_priv = cvx . norm ( w_priv , 1 ) self . add_constraint ( function >= 1 - cvx . multiply ( self . y . T , priv_function )) self . add_constraint ( priv_function >= 0 ) self . add_constraint ( loss <= init_loss ) self . add_constraint ( weight_norm <= l1_w ) self . add_constraint ( weight_norm_priv <= l1_priv_w ) # Save values for object use later self . w = w self . w_priv = w_priv self . feature_relevance = cvx . Variable ( nonneg = True , name = \" Feature Relevance \" )","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_classification/#lupi_classification_svm","text":"class ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance.","title":"LUPI_Classification_SVM"},{"location":"reference/fri/model/lupi_classification/#ancestors-in-mro_2","text":"fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_classification/#static-methods_1","text":"##### hyperparameter ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def hyperparameter ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ]","title":"Static methods"},{"location":"reference/fri/model/lupi_classification/#methods_1","text":"##### fit ``` python3 def ( self , X_combined , y , lupi_features = None ) ``` Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . ??? example \" View Source \" def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . \"\"\" if lupi_features is None : raise ValueError ( \" No lupi_features argument given. \" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \" C \" ] scaling_lupi_w = self . hyperparam [ \" scaling_lupi_w \" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ) , name = \" w \" ) w_priv = cvx . Variable ( lupi_features , name = \" w_priv \" ) b = cvx . Variable ( name = \" bias \" ) b_priv = cvx . Variable ( name = \" bias_priv \" ) # Define functions for better readability function = X * w + b priv_function = X_priv * w_priv + b_priv # Combined loss of lupi function and normal slacks , scaled by two constants loss = cvx . sum ( priv_function ) # L1 norm regularization of both functions with 1 scaling constant w_l1 = cvx . norm ( w , 1 ) w_priv_l1 = cvx . norm ( w_priv , 1 ) weight_regularization = 0 . 5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [ cvx . multiply ( y . T , function ) >= 1 - cvx . multiply ( y . T , priv_function ) , priv_function >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value w_priv = w_priv . value b = b . value b_priv = b_priv . value self . model_state = { \" w \" : w , \" w_priv \" : w_priv , \" b \" : b , \" b_priv \" : b_priv , \" lupi_features \" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else } loss = loss . value w_l1 = w_l1 . value w_priv_l1 = w_priv_l1 . value self . constraints = { \" loss \" : loss , \" w_l1 \" : w_l1 , \" w_priv_l1 \" : w_priv_l1 } return self ##### predict ``` python3 def ( self , X ) ``` ??? example \" View Source \" def predict ( self , X ) : X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \" w \" ] b = self . model_state [ \" b \" ] # Simple hyperplane classification rule f = np . dot ( X , w ) + b y = f >= 0 y = y . astype ( int ) # Format binary as signed unit vector y [ y == 0 ] = - 1 return y ##### score ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) # Negative class is set to - 1 for decision surface y = LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \" weighted \" ) if \" verbose \" in kwargs : return classification_report ( y , prediction ) return score View Source class LUPI_Classification_SVM ( InitModel ) : @ classmethod def hyperparameter ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . \"\"\" if lupi_features is None : raise ValueError ( \" No lupi_features argument given. \" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \" C \" ] scaling_lupi_w = self . hyperparam [ \" scaling_lupi_w \" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ) , name = \" w \" ) w_priv = cvx . Variable ( lupi_features , name = \" w_priv \" ) b = cvx . Variable ( name = \" bias \" ) b_priv = cvx . Variable ( name = \" bias_priv \" ) # Define functions for better readability function = X * w + b priv_function = X_priv * w_priv + b_priv # Combined loss of lupi function and normal slacks , scaled by two constants loss = cvx . sum ( priv_function ) # L1 norm regularization of both functions with 1 scaling constant w_l1 = cvx . norm ( w , 1 ) w_priv_l1 = cvx . norm ( w_priv , 1 ) weight_regularization = 0 . 5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [ cvx . multiply ( y . T , function ) >= 1 - cvx . multiply ( y . T , priv_function ) , priv_function >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value w_priv = w_priv . value b = b . value b_priv = b_priv . value self . model_state = { \" w \" : w , \" w_priv \" : w_priv , \" b \" : b , \" b_priv \" : b_priv , \" lupi_features \" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else } loss = loss . value w_l1 = w_l1 . value w_priv_l1 = w_priv_l1 . value self . constraints = { \" loss \" : loss , \" w_l1 \" : w_l1 , \" w_priv_l1 \" : w_priv_l1 } return self def predict ( self , X ) : X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \" w \" ] b = self . model_state [ \" b \" ] # Simple hyperplane classification rule f = np . dot ( X , w ) + b y = f >= 0 y = y . astype ( int ) # Format binary as signed unit vector y [ y == 0 ] = - 1 return y def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) # Negative class is set to - 1 for decision surface y = LabelEncoder () . fit_transform ( y ) y [ y == 0 ] = - 1 # Using weighted f1 score to have a stable score for imbalanced datasets score = fbeta_score ( y , prediction , beta = 1 , average = \" weighted \" ) if \" verbose \" in kwargs : return classification_report ( y , prediction ) return score","title":"Methods"},{"location":"reference/fri/model/lupi_ordinal_regression/","text":"Module fri.model.lupi_ordinal_regression View Source from itertools import product import cvxpy as cvx import numpy as np from sklearn.metrics import make_scorer from sklearn.utils import check_X_y from fri.model.base_lupi import ( LUPI_Relevance_CVXProblem , split_dataset , is_lupi_feature , ) from fri.model.ordinal_regression import ( OrdinalRegression_Relevance_Bound , ordinal_scores , ) from .base_initmodel import LUPI_InitModel from .base_type import ProblemType class LUPI_OrdinalRegression ( ProblemType ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ): return self . _lupi_features @classmethod def parameters ( cls ): return [ \"C\" , \"scaling_lupi_w\" ] @property def get_initmodel_template ( cls ): return LUPI_OrdinalRegression_SVM @property def get_cvxproblem_template ( cls ): return LUPI_OrdinalRegression_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y class LUPI_OrdinalRegression_SVM ( LUPI_InitModel ): @classmethod def hyperparameter ( cls ): return [ \"C\" , \"scaling_lupi_w\" ] def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : raise ValueError ( \"No lupi_features argument given.\" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape self . classes_ = np . unique ( y ) # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \"C\" ] scaling_lupi_w = self . hyperparam [ \"scaling_lupi_w\" ] get_original_bin_name , n_bins = get_bin_mapping ( y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) b_s = cvx . Variable ( shape = ( n_boundaries ), name = \"bias\" ) w_priv = cvx . Variable ( shape = ( self . lupi_features , 2 ), name = \"w_priv\" ) d_priv = cvx . Variable ( shape = ( 2 ), name = \"bias_priv\" ) def priv_function ( bin , sign ): indices = np . where ( y == get_original_bin_name [ bin ]) return X_priv [ indices ] * w_priv [:, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [:, 0 ], 1 ) priv_l1_2 = cvx . norm ( w_priv [:, 1 ], 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) weight_regularization = 0.5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [] loss = 0 for left_bin in range ( 0 , n_bins - 1 ): indices = np . where ( y == get_original_bin_name [ left_bin ]) constraints . append ( X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) constraints . append ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ): indices = np . where ( y == get_original_bin_name [ right_bin ]) constraints . append ( X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) constraints . append ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ): constraints . append ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ]) objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem. solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b_s = b_s . value self . model_state = { \"w\" : w , \"b_s\" : b_s , \"w_priv\" : w_priv . value , \"d_priv\" : d_priv . value , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO: Move this somewhere else \"bin_boundaries\" : n_boundaries , } self . constraints = { \"loss\" : loss . value , \"w_l1\" : w_l1 . value , \"w_priv_l1\" : w_priv_l1 . value , \"priv_l1_1\" : priv_l1_1 . value , \"priv_l1_2\" : priv_l1_2 . value , } return self def predict ( self , X ): X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b_s = self . model_state [ \"b_s\" ] scores = np . dot ( X , w . T )[ np . newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] def score ( self , X , y , error_type = \"mmae\" , return_error = False , ** kwargs ): X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score def make_scorer ( self ): # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \"mze\" ) mae = make_scorer ( ordinal_scores , error_type = \"mae\" ) mmae = make_scorer ( ordinal_scores , error_type = \"mmae\" ) scorer = { \"mze\" : mze , \"mae\" : mae , \"mmae\" : mmae } return scorer , \"mmae\" def get_bin_mapping ( y ): \"\"\" Get ordered unique classes and corresponding mapping from old names Parameters ---------- y: array of discrete values (int, str) Returns ------- \"\"\" classes_ = np . unique ( y ) original_bins = sorted ( classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) return get_old_bin , n_bins class LUPI_OrdinalRegression_Relevance_Bound ( LUPI_Relevance_CVXProblem , OrdinalRegression_Relevance_Bound ): @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems? if not is_priv : yield from super () . generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign in [ 1 , - 1 ]: problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB ( sign = sign ) problem . isLowerBound = True yield problem @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems? if not is_priv : yield from super () . generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ([ 1 , - 1 ], [ 0 , 1 ]): problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ): vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] # We take the max of mins because we need the necessary contribution over all functions min_value = max ( vals ) return min_value def _init_objective_LB_LUPI ( self , sign = None , bin_index = None , ** kwargs ): self . add_constraint ( sign * self . w_priv [ self . lupi_index , :] <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_objective_UB_LUPI ( self , sign = None , pos = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w_priv [ self . lupi_index , pos ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model init_w_l1 = init_model_constraints [ \"w_l1\" ] init_w_priv_l1 = init_model_constraints [ \"w_priv_l1\" ] init_priv_l1_1 = init_model_constraints [ \"priv_l1_1\" ] init_priv_l1_2 = init_model_constraints [ \"priv_l1_2\" ] init_loss = init_model_constraints [ \"loss\" ] get_original_bin_name , n_bins = get_bin_mapping ( self . y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) b_s = cvx . Variable ( shape = ( n_boundaries ), name = \"bias\" ) w_priv = cvx . Variable ( shape = ( self . d_priv , 2 ), name = \"w_priv\" ) d_priv = cvx . Variable ( shape = ( 2 ), name = \"bias_priv\" ) def priv_function ( bin , sign ): indices = np . where ( self . y == get_original_bin_name [ bin ]) return self . X_priv [ indices ] * w_priv [:, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [:, 0 ], 1 ) priv_l1_2 = cvx . norm ( w_priv [:, 1 ], 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) loss = 0 for left_bin in range ( 0 , n_bins - 1 ): indices = np . where ( self . y == get_original_bin_name [ left_bin ]) self . add_constraint ( self . X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) self . add_constraint ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ): indices = np . where ( self . y == get_original_bin_name [ right_bin ]) self . add_constraint ( self . X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) self . add_constraint ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ): self . add_constraint ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ]) self . add_constraint ( w_l1 <= init_w_l1 ) self . add_constraint ( priv_l1_1 <= init_priv_l1_1 ) self . add_constraint ( priv_l1_2 <= init_priv_l1_2 ) self . add_constraint ( loss <= init_loss ) self . w = w self . w_priv = w_priv self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Functions get_bin_mapping def ( y ) Get ordered unique classes and corresponding mapping from old names Parameters y: array of discrete values (int, str) Returns View Source def get_bin_mapping ( y ) : \"\"\" Get ordered unique classes and corresponding mapping from old names Parameters ---------- y : array of discrete values ( int , str ) Returns ------- \"\"\" classes_ = np . unique ( y ) original_bins = sorted ( classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) return get_old_bin , n_bins Classes LUPI_OrdinalRegression class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods ##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] Instance variables ``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : ``` python3 lupi_features ``` : Methods ##### preprocessing ``` python3 def ( self , data , lupi_features = None ) ``` ??? example \" View Source \" def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \" First ordinal class has index > 0. Shifting index... \" ) y = y - np . min ( y ) return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class LUPI_OrdinalRegression ( ProblemType ) : def __init__ ( self , ** kwargs ) : super () . __init__ ( ** kwargs ) self . _lupi_features = None @ property def lupi_features ( self ) : return self . _lupi_features @ classmethod def parameters ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] @ property def get_initmodel_template ( cls ) : return LUPI_OrdinalRegression_SVM @ property def get_cvxproblem_template ( cls ) : return LUPI_OrdinalRegression_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \" First ordinal class has index > 0. Shifting index... \" ) y = y - np . min ( y ) return X , y LUPI_OrdinalRegression_Relevance_Bound class ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_lupi.LUPI_Relevance_CVXProblem fri.model.ordinal_regression.OrdinalRegression_Relevance_Bound fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC Static methods ##### aggregate_min_candidates ``` python3 def ( min_problems_candidates ) ``` ??? example \" View Source \" @ classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] # We take the max of mins because we need the necessary contribution over all functions min_value = max ( vals ) return min_value ##### generate_lower_bound_problem ``` python3 def ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) ``` ??? example \" View Source \" @ classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super () . generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign in [ 1 , - 1 ]: problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB ( sign = sign ) problem . isLowerBound = True yield problem ##### generate_upper_bound_problem ``` python3 def ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) ``` ??? example \" View Source \" @ classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super () . generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ( [ 1 , - 1 ], [ 0 , 1 ] ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem View Source class LUPI_OrdinalRegression_Relevance_Bound ( LUPI_Relevance_CVXProblem , OrdinalRegression_Relevance_Bound ) : @ classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super () . generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign in [ 1 , - 1 ]: problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB ( sign = sign ) problem . isLowerBound = True yield problem @ classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super () . generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ( [ 1 , - 1 ], [ 0 , 1 ] ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem @ classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] # We take the max of mins because we need the necessary contribution over all functions min_value = max ( vals ) return min_value def _init_objective_LB_LUPI ( self , sign = None , bin_index = None , ** kwargs ) : self . add_constraint ( sign * self . w_priv [ self . lupi_index , :] <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_objective_UB_LUPI ( self , sign = None , pos = None , ** kwargs ) : self . add_constraint ( self . feature_relevance <= sign * self . w_priv [ self . lupi_index , pos ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ) : # Upper constraints from initial model init_w_l1 = init_model_constraints [ \" w_l1 \" ] init_w_priv_l1 = init_model_constraints [ \" w_priv_l1 \" ] init_priv_l1_1 = init_model_constraints [ \" priv_l1_1 \" ] init_priv_l1_2 = init_model_constraints [ \" priv_l1_2 \" ] init_loss = init_model_constraints [ \" loss \" ] get_original_bin_name , n_bins = get_bin_mapping ( self . y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( self . d ) , name = \" w \" ) b_s = cvx . Variable ( shape = ( n_boundaries ) , name = \" bias \" ) w_priv = cvx . Variable ( shape = ( self . d_priv , 2 ) , name = \" w_priv \" ) d_priv = cvx . Variable ( shape = ( 2 ) , name = \" bias_priv \" ) def priv_function ( bin , sign ) : indices = np . where ( self . y == get_original_bin_name [ bin ] ) return self . X_priv [ indices ] * w_priv [:, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [:, 0 ], 1 ) priv_l1_2 = cvx . norm ( w_priv [:, 1 ], 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) loss = 0 for left_bin in range ( 0 , n_bins - 1 ) : indices = np . where ( self . y == get_original_bin_name [ left_bin ] ) self . add_constraint ( self . X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) self . add_constraint ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ) : indices = np . where ( self . y == get_original_bin_name [ right_bin ] ) self . add_constraint ( self . X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) self . add_constraint ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ) : self . add_constraint ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ] ) self . add_constraint ( w_l1 <= init_w_l1 ) self . add_constraint ( priv_l1_1 <= init_priv_l1_1 ) self . add_constraint ( priv_l1_2 <= init_priv_l1_2 ) self . add_constraint ( loss <= init_loss ) self . w = w self . w_priv = w_priv self . feature_relevance = cvx . Variable ( nonneg = True , name = \" Feature Relevance \" ) LUPI_OrdinalRegression_SVM class ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_initmodel.LUPI_InitModel fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator Static methods ##### hyperparameter ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def hyperparameter ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] Methods ##### fit ``` python3 def ( self , X_combined , y , lupi_features = None ) ``` Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . ??? example \" View Source \" def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . \"\"\" if lupi_features is None : raise ValueError ( \" No lupi_features argument given. \" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape self . classes_ = np . unique ( y ) # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \" C \" ] scaling_lupi_w = self . hyperparam [ \" scaling_lupi_w \" ] get_original_bin_name , n_bins = get_bin_mapping ( y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ) , name = \" w \" ) b_s = cvx . Variable ( shape = ( n_boundaries ) , name = \" bias \" ) w_priv = cvx . Variable ( shape = ( self . lupi_features , 2 ) , name = \" w_priv \" ) d_priv = cvx . Variable ( shape = ( 2 ) , name = \" bias_priv \" ) def priv_function ( bin , sign ) : indices = np . where ( y == get_original_bin_name [ bin ] ) return X_priv [ indices ] * w_priv [:, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [:, 0 ], 1 ) priv_l1_2 = cvx . norm ( w_priv [:, 1 ], 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) weight_regularization = 0 . 5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [] loss = 0 for left_bin in range ( 0 , n_bins - 1 ) : indices = np . where ( y == get_original_bin_name [ left_bin ] ) constraints . append ( X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) constraints . append ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ) : indices = np . where ( y == get_original_bin_name [ right_bin ] ) constraints . append ( X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) constraints . append ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ) : constraints . append ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ] ) objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b_s = b_s . value self . model_state = { \" w \" : w , \" b_s \" : b_s , \" w_priv \" : w_priv . value , \" d_priv \" : d_priv . value , \" lupi_features \" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else \" bin_boundaries \" : n_boundaries , } self . constraints = { \" loss \" : loss . value , \" w_l1 \" : w_l1 . value , \" w_priv_l1 \" : w_priv_l1 . value , \" priv_l1_1 \" : priv_l1_1 . value , \" priv_l1_2 \" : priv_l1_2 . value , } return self ##### make_scorer ``` python3 def ( self ) ``` ??? example \" View Source \" def make_scorer ( self ) : # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \" mze \" ) mae = make_scorer ( ordinal_scores , error_type = \" mae \" ) mmae = make_scorer ( ordinal_scores , error_type = \" mmae \" ) scorer = { \" mze \" : mze , \" mae \" : mae , \" mmae \" : mmae } return scorer , \" mmae \" ##### predict ``` python3 def ( self , X ) ``` ??? example \" View Source \" def predict ( self , X ) : X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \" w \" ] b_s = self . model_state [ \" b_s \" ] scores = np . dot ( X , w . T ) [ np . newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] ##### score ``` python3 def ( self , X , y , error_type = ' mmae ' , return_error = False , ** kwargs ) ``` ??? example \" View Source \" def score ( self , X , y , error_type = \" mmae \" , return_error = False , ** kwargs ) : X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score View Source class LUPI_OrdinalRegression_SVM ( LUPI_InitModel ) : @ classmethod def hyperparameter ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . \"\"\" if lupi_features is None : raise ValueError ( \" No lupi_features argument given. \" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape self . classes_ = np . unique ( y ) # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \" C \" ] scaling_lupi_w = self . hyperparam [ \" scaling_lupi_w \" ] get_original_bin_name , n_bins = get_bin_mapping ( y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ) , name = \" w \" ) b_s = cvx . Variable ( shape = ( n_boundaries ) , name = \" bias \" ) w_priv = cvx . Variable ( shape = ( self . lupi_features , 2 ) , name = \" w_priv \" ) d_priv = cvx . Variable ( shape = ( 2 ) , name = \" bias_priv \" ) def priv_function ( bin , sign ) : indices = np . where ( y == get_original_bin_name [ bin ] ) return X_priv [ indices ] * w_priv [:, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [:, 0 ], 1 ) priv_l1_2 = cvx . norm ( w_priv [:, 1 ], 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) weight_regularization = 0 . 5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [] loss = 0 for left_bin in range ( 0 , n_bins - 1 ) : indices = np . where ( y == get_original_bin_name [ left_bin ] ) constraints . append ( X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) constraints . append ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ) : indices = np . where ( y == get_original_bin_name [ right_bin ] ) constraints . append ( X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) constraints . append ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ) : constraints . append ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ] ) objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b_s = b_s . value self . model_state = { \" w \" : w , \" b_s \" : b_s , \" w_priv \" : w_priv . value , \" d_priv \" : d_priv . value , \" lupi_features \" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else \" bin_boundaries \" : n_boundaries , } self . constraints = { \" loss \" : loss . value , \" w_l1 \" : w_l1 . value , \" w_priv_l1 \" : w_priv_l1 . value , \" priv_l1_1 \" : priv_l1_1 . value , \" priv_l1_2 \" : priv_l1_2 . value , } return self def predict ( self , X ) : X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \" w \" ] b_s = self . model_state [ \" b_s \" ] scores = np . dot ( X , w . T ) [ np . newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] def score ( self , X , y , error_type = \" mmae \" , return_error = False , ** kwargs ) : X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score def make_scorer ( self ) : # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \" mze \" ) mae = make_scorer ( ordinal_scores , error_type = \" mae \" ) mmae = make_scorer ( ordinal_scores , error_type = \" mmae \" ) scorer = { \" mze \" : mze , \" mae \" : mae , \" mmae \" : mmae } return scorer , \" mmae \"","title":"Lupi Ordinal Regression"},{"location":"reference/fri/model/lupi_ordinal_regression/#module-frimodellupi_ordinal_regression","text":"View Source from itertools import product import cvxpy as cvx import numpy as np from sklearn.metrics import make_scorer from sklearn.utils import check_X_y from fri.model.base_lupi import ( LUPI_Relevance_CVXProblem , split_dataset , is_lupi_feature , ) from fri.model.ordinal_regression import ( OrdinalRegression_Relevance_Bound , ordinal_scores , ) from .base_initmodel import LUPI_InitModel from .base_type import ProblemType class LUPI_OrdinalRegression ( ProblemType ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ): return self . _lupi_features @classmethod def parameters ( cls ): return [ \"C\" , \"scaling_lupi_w\" ] @property def get_initmodel_template ( cls ): return LUPI_OrdinalRegression_SVM @property def get_cvxproblem_template ( cls ): return LUPI_OrdinalRegression_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y class LUPI_OrdinalRegression_SVM ( LUPI_InitModel ): @classmethod def hyperparameter ( cls ): return [ \"C\" , \"scaling_lupi_w\" ] def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : raise ValueError ( \"No lupi_features argument given.\" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape self . classes_ = np . unique ( y ) # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \"C\" ] scaling_lupi_w = self . hyperparam [ \"scaling_lupi_w\" ] get_original_bin_name , n_bins = get_bin_mapping ( y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) b_s = cvx . Variable ( shape = ( n_boundaries ), name = \"bias\" ) w_priv = cvx . Variable ( shape = ( self . lupi_features , 2 ), name = \"w_priv\" ) d_priv = cvx . Variable ( shape = ( 2 ), name = \"bias_priv\" ) def priv_function ( bin , sign ): indices = np . where ( y == get_original_bin_name [ bin ]) return X_priv [ indices ] * w_priv [:, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [:, 0 ], 1 ) priv_l1_2 = cvx . norm ( w_priv [:, 1 ], 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) weight_regularization = 0.5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [] loss = 0 for left_bin in range ( 0 , n_bins - 1 ): indices = np . where ( y == get_original_bin_name [ left_bin ]) constraints . append ( X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) constraints . append ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ): indices = np . where ( y == get_original_bin_name [ right_bin ]) constraints . append ( X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) constraints . append ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ): constraints . append ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ]) objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem. solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b_s = b_s . value self . model_state = { \"w\" : w , \"b_s\" : b_s , \"w_priv\" : w_priv . value , \"d_priv\" : d_priv . value , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO: Move this somewhere else \"bin_boundaries\" : n_boundaries , } self . constraints = { \"loss\" : loss . value , \"w_l1\" : w_l1 . value , \"w_priv_l1\" : w_priv_l1 . value , \"priv_l1_1\" : priv_l1_1 . value , \"priv_l1_2\" : priv_l1_2 . value , } return self def predict ( self , X ): X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b_s = self . model_state [ \"b_s\" ] scores = np . dot ( X , w . T )[ np . newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] def score ( self , X , y , error_type = \"mmae\" , return_error = False , ** kwargs ): X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score def make_scorer ( self ): # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \"mze\" ) mae = make_scorer ( ordinal_scores , error_type = \"mae\" ) mmae = make_scorer ( ordinal_scores , error_type = \"mmae\" ) scorer = { \"mze\" : mze , \"mae\" : mae , \"mmae\" : mmae } return scorer , \"mmae\" def get_bin_mapping ( y ): \"\"\" Get ordered unique classes and corresponding mapping from old names Parameters ---------- y: array of discrete values (int, str) Returns ------- \"\"\" classes_ = np . unique ( y ) original_bins = sorted ( classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) return get_old_bin , n_bins class LUPI_OrdinalRegression_Relevance_Bound ( LUPI_Relevance_CVXProblem , OrdinalRegression_Relevance_Bound ): @classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems? if not is_priv : yield from super () . generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign in [ 1 , - 1 ]: problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB ( sign = sign ) problem . isLowerBound = True yield problem @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems? if not is_priv : yield from super () . generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ([ 1 , - 1 ], [ 0 , 1 ]): problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem @classmethod def aggregate_min_candidates ( cls , min_problems_candidates ): vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] # We take the max of mins because we need the necessary contribution over all functions min_value = max ( vals ) return min_value def _init_objective_LB_LUPI ( self , sign = None , bin_index = None , ** kwargs ): self . add_constraint ( sign * self . w_priv [ self . lupi_index , :] <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_objective_UB_LUPI ( self , sign = None , pos = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w_priv [ self . lupi_index , pos ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model init_w_l1 = init_model_constraints [ \"w_l1\" ] init_w_priv_l1 = init_model_constraints [ \"w_priv_l1\" ] init_priv_l1_1 = init_model_constraints [ \"priv_l1_1\" ] init_priv_l1_2 = init_model_constraints [ \"priv_l1_2\" ] init_loss = init_model_constraints [ \"loss\" ] get_original_bin_name , n_bins = get_bin_mapping ( self . y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) b_s = cvx . Variable ( shape = ( n_boundaries ), name = \"bias\" ) w_priv = cvx . Variable ( shape = ( self . d_priv , 2 ), name = \"w_priv\" ) d_priv = cvx . Variable ( shape = ( 2 ), name = \"bias_priv\" ) def priv_function ( bin , sign ): indices = np . where ( self . y == get_original_bin_name [ bin ]) return self . X_priv [ indices ] * w_priv [:, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [:, 0 ], 1 ) priv_l1_2 = cvx . norm ( w_priv [:, 1 ], 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) loss = 0 for left_bin in range ( 0 , n_bins - 1 ): indices = np . where ( self . y == get_original_bin_name [ left_bin ]) self . add_constraint ( self . X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) self . add_constraint ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ): indices = np . where ( self . y == get_original_bin_name [ right_bin ]) self . add_constraint ( self . X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) self . add_constraint ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ): self . add_constraint ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ]) self . add_constraint ( w_l1 <= init_w_l1 ) self . add_constraint ( priv_l1_1 <= init_priv_l1_1 ) self . add_constraint ( priv_l1_2 <= init_priv_l1_2 ) self . add_constraint ( loss <= init_loss ) self . w = w self . w_priv = w_priv self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"Module fri.model.lupi_ordinal_regression"},{"location":"reference/fri/model/lupi_ordinal_regression/#functions","text":"","title":"Functions"},{"location":"reference/fri/model/lupi_ordinal_regression/#get_bin_mapping","text":"def ( y ) Get ordered unique classes and corresponding mapping from old names Parameters y: array of discrete values (int, str) Returns View Source def get_bin_mapping ( y ) : \"\"\" Get ordered unique classes and corresponding mapping from old names Parameters ---------- y : array of discrete values ( int , str ) Returns ------- \"\"\" classes_ = np . unique ( y ) original_bins = sorted ( classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) return get_old_bin , n_bins","title":"get_bin_mapping"},{"location":"reference/fri/model/lupi_ordinal_regression/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/lupi_ordinal_regression/#lupi_ordinalregression","text":"class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"LUPI_OrdinalRegression"},{"location":"reference/fri/model/lupi_ordinal_regression/#ancestors-in-mro","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_ordinal_regression/#static-methods","text":"##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ]","title":"Static methods"},{"location":"reference/fri/model/lupi_ordinal_regression/#instance-variables","text":"``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : ``` python3 lupi_features ``` :","title":"Instance variables"},{"location":"reference/fri/model/lupi_ordinal_regression/#methods","text":"##### preprocessing ``` python3 def ( self , data , lupi_features = None ) ``` ??? example \" View Source \" def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \" First ordinal class has index > 0. Shifting index... \" ) y = y - np . min ( y ) return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class LUPI_OrdinalRegression ( ProblemType ) : def __init__ ( self , ** kwargs ) : super () . __init__ ( ** kwargs ) self . _lupi_features = None @ property def lupi_features ( self ) : return self . _lupi_features @ classmethod def parameters ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] @ property def get_initmodel_template ( cls ) : return LUPI_OrdinalRegression_SVM @ property def get_cvxproblem_template ( cls ) : return LUPI_OrdinalRegression_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \" First ordinal class has index > 0. Shifting index... \" ) y = y - np . min ( y ) return X , y","title":"Methods"},{"location":"reference/fri/model/lupi_ordinal_regression/#lupi_ordinalregression_relevance_bound","text":"class ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 ) Helper class that provides a standard way to create an ABC using inheritance.","title":"LUPI_OrdinalRegression_Relevance_Bound"},{"location":"reference/fri/model/lupi_ordinal_regression/#ancestors-in-mro_1","text":"fri.model.base_lupi.LUPI_Relevance_CVXProblem fri.model.ordinal_regression.OrdinalRegression_Relevance_Bound fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_ordinal_regression/#static-methods_1","text":"##### aggregate_min_candidates ``` python3 def ( min_problems_candidates ) ``` ??? example \" View Source \" @ classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] # We take the max of mins because we need the necessary contribution over all functions min_value = max ( vals ) return min_value ##### generate_lower_bound_problem ``` python3 def ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) ``` ??? example \" View Source \" @ classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super () . generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign in [ 1 , - 1 ]: problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB ( sign = sign ) problem . isLowerBound = True yield problem ##### generate_upper_bound_problem ``` python3 def ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) ``` ??? example \" View Source \" @ classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super () . generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ( [ 1 , - 1 ], [ 0 , 1 ] ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem View Source class LUPI_OrdinalRegression_Relevance_Bound ( LUPI_Relevance_CVXProblem , OrdinalRegression_Relevance_Bound ) : @ classmethod def generate_lower_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super () . generate_lower_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign in [ 1 , - 1 ]: problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_LB ( sign = sign ) problem . isLowerBound = True yield problem @ classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super () . generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ( [ 1 , - 1 ], [ 0 , 1 ] ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem @ classmethod def aggregate_min_candidates ( cls , min_problems_candidates ) : vals = [ candidate . solved_relevance for candidate in min_problems_candidates ] # We take the max of mins because we need the necessary contribution over all functions min_value = max ( vals ) return min_value def _init_objective_LB_LUPI ( self , sign = None , bin_index = None , ** kwargs ) : self . add_constraint ( sign * self . w_priv [ self . lupi_index , :] <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_objective_UB_LUPI ( self , sign = None , pos = None , ** kwargs ) : self . add_constraint ( self . feature_relevance <= sign * self . w_priv [ self . lupi_index , pos ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ) : # Upper constraints from initial model init_w_l1 = init_model_constraints [ \" w_l1 \" ] init_w_priv_l1 = init_model_constraints [ \" w_priv_l1 \" ] init_priv_l1_1 = init_model_constraints [ \" priv_l1_1 \" ] init_priv_l1_2 = init_model_constraints [ \" priv_l1_2 \" ] init_loss = init_model_constraints [ \" loss \" ] get_original_bin_name , n_bins = get_bin_mapping ( self . y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( self . d ) , name = \" w \" ) b_s = cvx . Variable ( shape = ( n_boundaries ) , name = \" bias \" ) w_priv = cvx . Variable ( shape = ( self . d_priv , 2 ) , name = \" w_priv \" ) d_priv = cvx . Variable ( shape = ( 2 ) , name = \" bias_priv \" ) def priv_function ( bin , sign ) : indices = np . where ( self . y == get_original_bin_name [ bin ] ) return self . X_priv [ indices ] * w_priv [:, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [:, 0 ], 1 ) priv_l1_2 = cvx . norm ( w_priv [:, 1 ], 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) loss = 0 for left_bin in range ( 0 , n_bins - 1 ) : indices = np . where ( self . y == get_original_bin_name [ left_bin ] ) self . add_constraint ( self . X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) self . add_constraint ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ) : indices = np . where ( self . y == get_original_bin_name [ right_bin ] ) self . add_constraint ( self . X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) self . add_constraint ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ) : self . add_constraint ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ] ) self . add_constraint ( w_l1 <= init_w_l1 ) self . add_constraint ( priv_l1_1 <= init_priv_l1_1 ) self . add_constraint ( priv_l1_2 <= init_priv_l1_2 ) self . add_constraint ( loss <= init_loss ) self . w = w self . w_priv = w_priv self . feature_relevance = cvx . Variable ( nonneg = True , name = \" Feature Relevance \" )","title":"Static methods"},{"location":"reference/fri/model/lupi_ordinal_regression/#lupi_ordinalregression_svm","text":"class ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance.","title":"LUPI_OrdinalRegression_SVM"},{"location":"reference/fri/model/lupi_ordinal_regression/#ancestors-in-mro_2","text":"fri.model.base_initmodel.LUPI_InitModel fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_ordinal_regression/#static-methods_2","text":"##### hyperparameter ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def hyperparameter ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ]","title":"Static methods"},{"location":"reference/fri/model/lupi_ordinal_regression/#methods_1","text":"##### fit ``` python3 def ( self , X_combined , y , lupi_features = None ) ``` Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . ??? example \" View Source \" def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . \"\"\" if lupi_features is None : raise ValueError ( \" No lupi_features argument given. \" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape self . classes_ = np . unique ( y ) # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \" C \" ] scaling_lupi_w = self . hyperparam [ \" scaling_lupi_w \" ] get_original_bin_name , n_bins = get_bin_mapping ( y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ) , name = \" w \" ) b_s = cvx . Variable ( shape = ( n_boundaries ) , name = \" bias \" ) w_priv = cvx . Variable ( shape = ( self . lupi_features , 2 ) , name = \" w_priv \" ) d_priv = cvx . Variable ( shape = ( 2 ) , name = \" bias_priv \" ) def priv_function ( bin , sign ) : indices = np . where ( y == get_original_bin_name [ bin ] ) return X_priv [ indices ] * w_priv [:, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [:, 0 ], 1 ) priv_l1_2 = cvx . norm ( w_priv [:, 1 ], 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) weight_regularization = 0 . 5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [] loss = 0 for left_bin in range ( 0 , n_bins - 1 ) : indices = np . where ( y == get_original_bin_name [ left_bin ] ) constraints . append ( X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) constraints . append ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ) : indices = np . where ( y == get_original_bin_name [ right_bin ] ) constraints . append ( X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) constraints . append ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ) : constraints . append ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ] ) objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b_s = b_s . value self . model_state = { \" w \" : w , \" b_s \" : b_s , \" w_priv \" : w_priv . value , \" d_priv \" : d_priv . value , \" lupi_features \" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else \" bin_boundaries \" : n_boundaries , } self . constraints = { \" loss \" : loss . value , \" w_l1 \" : w_l1 . value , \" w_priv_l1 \" : w_priv_l1 . value , \" priv_l1_1 \" : priv_l1_1 . value , \" priv_l1_2 \" : priv_l1_2 . value , } return self ##### make_scorer ``` python3 def ( self ) ``` ??? example \" View Source \" def make_scorer ( self ) : # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \" mze \" ) mae = make_scorer ( ordinal_scores , error_type = \" mae \" ) mmae = make_scorer ( ordinal_scores , error_type = \" mmae \" ) scorer = { \" mze \" : mze , \" mae \" : mae , \" mmae \" : mmae } return scorer , \" mmae \" ##### predict ``` python3 def ( self , X ) ``` ??? example \" View Source \" def predict ( self , X ) : X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \" w \" ] b_s = self . model_state [ \" b_s \" ] scores = np . dot ( X , w . T ) [ np . newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] ##### score ``` python3 def ( self , X , y , error_type = ' mmae ' , return_error = False , ** kwargs ) ``` ??? example \" View Source \" def score ( self , X , y , error_type = \" mmae \" , return_error = False , ** kwargs ) : X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score View Source class LUPI_OrdinalRegression_SVM ( LUPI_InitModel ) : @ classmethod def hyperparameter ( cls ) : return [ \" C \" , \" scaling_lupi_w \" ] def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . \"\"\" if lupi_features is None : raise ValueError ( \" No lupi_features argument given. \" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape self . classes_ = np . unique ( y ) # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \" C \" ] scaling_lupi_w = self . hyperparam [ \" scaling_lupi_w \" ] get_original_bin_name , n_bins = get_bin_mapping ( y ) n_boundaries = n_bins - 1 # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ) , name = \" w \" ) b_s = cvx . Variable ( shape = ( n_boundaries ) , name = \" bias \" ) w_priv = cvx . Variable ( shape = ( self . lupi_features , 2 ) , name = \" w_priv \" ) d_priv = cvx . Variable ( shape = ( 2 ) , name = \" bias_priv \" ) def priv_function ( bin , sign ) : indices = np . where ( y == get_original_bin_name [ bin ] ) return X_priv [ indices ] * w_priv [:, sign ] + d_priv [ sign ] # L1 norm regularization of both functions with 1 scaling constant priv_l1_1 = cvx . norm ( w_priv [:, 0 ], 1 ) priv_l1_2 = cvx . norm ( w_priv [:, 1 ], 1 ) w_priv_l1 = priv_l1_1 + priv_l1_2 w_l1 = cvx . norm ( w , 1 ) weight_regularization = 0 . 5 * ( w_l1 + scaling_lupi_w * w_priv_l1 ) constraints = [] loss = 0 for left_bin in range ( 0 , n_bins - 1 ) : indices = np . where ( y == get_original_bin_name [ left_bin ] ) constraints . append ( X [ indices ] * w - b_s [ left_bin ] <= - 1 + priv_function ( left_bin , 0 ) ) constraints . append ( priv_function ( left_bin , 0 ) >= 0 ) loss += cvx . sum ( priv_function ( left_bin , 0 )) # Add constraints for slack into right neighboring bins for right_bin in range ( 1 , n_bins ) : indices = np . where ( y == get_original_bin_name [ right_bin ] ) constraints . append ( X [ indices ] * w - b_s [ right_bin - 1 ] >= + 1 - priv_function ( right_bin , 1 ) ) constraints . append ( priv_function ( right_bin , 1 ) >= 0 ) loss += cvx . sum ( priv_function ( right_bin , 1 )) for i_boundary in range ( 0 , n_boundaries - 1 ) : constraints . append ( b_s [ i_boundary ] <= b_s [ i_boundary + 1 ] ) objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b_s = b_s . value self . model_state = { \" w \" : w , \" b_s \" : b_s , \" w_priv \" : w_priv . value , \" d_priv \" : d_priv . value , \" lupi_features \" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else \" bin_boundaries \" : n_boundaries , } self . constraints = { \" loss \" : loss . value , \" w_l1 \" : w_l1 . value , \" w_priv_l1 \" : w_priv_l1 . value , \" priv_l1_1 \" : priv_l1_1 . value , \" priv_l1_2 \" : priv_l1_2 . value , } return self def predict ( self , X ) : X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \" w \" ] b_s = self . model_state [ \" b_s \" ] scores = np . dot ( X , w . T ) [ np . newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] def score ( self , X , y , error_type = \" mmae \" , return_error = False , ** kwargs ) : X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score def make_scorer ( self ) : # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \" mze \" ) mae = make_scorer ( ordinal_scores , error_type = \" mae \" ) mmae = make_scorer ( ordinal_scores , error_type = \" mmae \" ) scorer = { \" mze \" : mze , \" mae \" : mae , \" mmae \" : mmae } return scorer , \" mmae \"","title":"Methods"},{"location":"reference/fri/model/lupi_regression/","text":"Module fri.model.lupi_regression View Source from itertools import product import cvxpy as cvx import numpy as np from sklearn.metrics import r2_score from sklearn.metrics.regression import _check_reg_targets from sklearn.utils import check_X_y from fri.model.base_lupi import ( LUPI_Relevance_CVXProblem , split_dataset , is_lupi_feature , ) from fri.model.regression import Regression_Relevance_Bound from .base_initmodel import LUPI_InitModel from .base_type import ProblemType class LUPI_Regression ( ProblemType ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ): return self . _lupi_features @classmethod def parameters ( cls ): return [ \"C\" , \"epsilon\" , \"scaling_lupi_w\" ] @property def get_initmodel_template ( cls ): return LUPI_Regression_SVM @property def get_cvxproblem_template ( cls ): return LUPI_Regression_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y class LUPI_Regression_SVM ( LUPI_InitModel ): @classmethod def hyperparameter ( cls ): return [ \"C\" , \"epsilon\" , \"scaling_lupi_w\" ] def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : raise ValueError ( \"No lupi_features argument given.\" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \"C\" ] epsilon = self . hyperparam [ \"epsilon\" ] scaling_lupi_w = self . hyperparam [ \"scaling_lupi_w\" ] # scaling_lupi_loss = self.hyperparam[\"scaling_lupi_loss\"] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) b = cvx . Variable ( name = \"bias\" ) w_priv_pos = cvx . Variable ( lupi_features , name = \"w_priv_pos\" ) b_priv_pos = cvx . Variable ( name = \"bias_priv_pos\" ) w_priv_neg = cvx . Variable ( lupi_features , name = \"w_priv_neg\" ) b_priv_neg = cvx . Variable ( name = \"bias_priv_neg\" ) # slack = cvx.Variable(shape=(n), name=\"slack\") # Define functions for better readability priv_function_pos = X_priv * w_priv_pos + b_priv_pos priv_function_neg = X_priv * w_priv_neg + b_priv_neg # Combined loss of lupi function and normal slacks, scaled by two constants priv_loss_pos = cvx . sum ( priv_function_pos ) priv_loss_neg = cvx . sum ( priv_function_neg ) priv_loss = priv_loss_pos + priv_loss_neg # slack_loss = cvx.sum(slack) # loss = scaling_lupi_loss * priv_loss loss = priv_loss # L1 norm regularization of both functions with 1 scaling constant weight_regularization = 0.5 * ( cvx . norm ( w , 1 ) + scaling_lupi_w * ( 0.5 * cvx . norm ( w_priv_pos , 1 ) + 0.5 * cvx . norm ( w_priv_neg , 1 )) ) constraints = [ y - X * w - b <= epsilon + priv_function_pos , X * w + b - y <= epsilon + priv_function_neg , priv_function_pos >= 0 , priv_function_neg >= 0 , # priv_loss_pos >= 0, # priv_loss_neg >= 0, # slack_loss >= 0, # slack >= 0, # loss >= 0, ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem. solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) self . model_state = { \"signs_pos\" : priv_function_pos . value > 0 , \"signs_neg\" : priv_function_neg . value > 0 , \"w\" : w . value , \"w_priv_pos\" : w_priv_pos . value , \"w_priv_neg\" : w_priv_neg . value , \"b\" : b . value , \"b_priv_pos\" : b_priv_pos . value , \"b_priv_neg\" : b_priv_neg . value , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO: Move this somewhere else, } w_l1 = np . linalg . norm ( w . value , ord = 1 ) w_priv_pos_l1 = np . linalg . norm ( w_priv_pos . value , ord = 1 ) w_priv_neg_l1 = np . linalg . norm ( w_priv_neg . value , ord = 1 ) # We take the mean to combine all submodels (for priv) into a single normalization factor w_priv_l1 = w_priv_pos_l1 + w_priv_neg_l1 self . constraints = { \"priv_loss\" : priv_loss . value , # \"loss_slack\": slack_loss.value, \"loss\" : loss . value , \"w_l1\" : w_l1 , \"w_priv_l1\" : w_priv_l1 , \"w_priv_pos_l1\" : w_priv_pos_l1 , \"w_priv_neg_l1\" : w_priv_neg_l1 , } return self @property def solver_params ( cls ): return { \"solver\" : \"ECOS\" , \"verbose\" : False } def predict ( self , X ): \"\"\" Method to predict points using svm classification rule. We use both normal and priv. features. This function is mainly used for CV purposes to find the best parameters according to score. Parameters ---------- X : numpy.ndarray \"\"\" X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) _check_reg_targets ( y , prediction , None ) score = r2_score ( y , prediction ) return score class LUPI_Regression_Relevance_Bound ( LUPI_Relevance_CVXProblem , Regression_Relevance_Bound ): @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems? if not is_priv : yield from super () . generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ([ 1 , - 1 ], [ True , False ]): problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem def _init_objective_LB_LUPI ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w_priv_pos [ self . lupi_index ]) <= self . feature_relevance ) self . add_constraint ( cvx . abs ( self . w_priv_neg [ self . lupi_index ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_objective_UB_LUPI ( self , pos = None , sign = None , ** kwargs ): if pos : self . add_constraint ( self . feature_relevance <= sign * self . w_priv_pos [ self . lupi_index ] ) else : self . add_constraint ( self . feature_relevance <= sign * self . w_priv_neg [ self . lupi_index ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from best initial model l1_w = init_model_constraints [ \"w_l1\" ] self . l1_priv_w_pos = init_model_constraints [ \"w_priv_pos_l1\" ] self . l1_priv_w_neg = init_model_constraints [ \"w_priv_neg_l1\" ] init_loss = init_model_constraints [ \"loss\" ] epsilon = parameters [ \"epsilon\" ] # New Variables w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) b = cvx . Variable ( name = \"b\" ) w_priv_pos = cvx . Variable ( self . d_priv , name = \"w_priv_pos\" ) b_priv_pos = cvx . Variable ( name = \"bias_priv_pos\" ) w_priv_neg = cvx . Variable ( self . d_priv , name = \"w_priv_neg\" ) b_priv_neg = cvx . Variable ( name = \"bias_priv_neg\" ) priv_function_pos = self . X_priv * w_priv_pos + b_priv_pos priv_function_neg = self . X_priv * w_priv_neg + b_priv_neg priv_loss = cvx . sum ( priv_function_pos + priv_function_neg ) loss = priv_loss weight_norm = cvx . norm ( w , 1 ) self . weight_norm_priv_pos = cvx . norm ( w_priv_pos , 1 ) self . weight_norm_priv_neg = cvx . norm ( w_priv_neg , 1 ) self . add_constraint ( self . y - self . X * w - b <= epsilon + priv_function_pos ) self . add_constraint ( self . X * w + b - self . y <= epsilon + priv_function_neg ) self . add_constraint ( priv_function_pos >= 0 ) self . add_constraint ( priv_function_neg >= 0 ) self . add_constraint ( loss <= init_loss ) self . add_constraint ( weight_norm <= l1_w ) self . add_constraint ( self . weight_norm_priv_pos <= self . l1_priv_w_pos ) self . add_constraint ( self . weight_norm_priv_neg <= self . l1_priv_w_neg ) # Save values for object use later self . w = w self . w_priv_pos = w_priv_pos self . w_priv_neg = w_priv_neg self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Classes LUPI_Regression class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods ##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" , \" epsilon \" , \" scaling_lupi_w \" ] Instance variables ``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : ``` python3 lupi_features ``` : Methods ##### preprocessing ``` python3 def ( self , data , lupi_features = None ) ``` ??? example \" View Source \" def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class LUPI_Regression ( ProblemType ) : def __init__ ( self , ** kwargs ) : super () . __init__ ( ** kwargs ) self . _lupi_features = None @ property def lupi_features ( self ) : return self . _lupi_features @ classmethod def parameters ( cls ) : return [ \" C \" , \" epsilon \" , \" scaling_lupi_w \" ] @ property def get_initmodel_template ( cls ) : return LUPI_Regression_SVM @ property def get_cvxproblem_template ( cls ) : return LUPI_Regression_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y LUPI_Regression_Relevance_Bound class ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_lupi.LUPI_Relevance_CVXProblem fri.model.regression.Regression_Relevance_Bound fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC Static methods ##### generate_upper_bound_problem ``` python3 def ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) ``` ??? example \" View Source \" @ classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super () . generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ( [ 1 , - 1 ], [ True , False ] ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem View Source class LUPI_Regression_Relevance_Bound ( LUPI_Relevance_CVXProblem , Regression_Relevance_Bound ) : @ classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super () . generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ( [ 1 , - 1 ], [ True , False ] ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem def _init_objective_LB_LUPI ( self , ** kwargs ) : self . add_constraint ( cvx . abs ( self . w_priv_pos [ self . lupi_index ] ) <= self . feature_relevance ) self . add_constraint ( cvx . abs ( self . w_priv_neg [ self . lupi_index ] ) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_objective_UB_LUPI ( self , pos = None , sign = None , ** kwargs ) : if pos : self . add_constraint ( self . feature_relevance <= sign * self . w_priv_pos [ self . lupi_index ] ) else : self . add_constraint ( self . feature_relevance <= sign * self . w_priv_neg [ self . lupi_index ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ) : # Upper constraints from best initial model l1_w = init_model_constraints [ \" w_l1 \" ] self . l1_priv_w_pos = init_model_constraints [ \" w_priv_pos_l1 \" ] self . l1_priv_w_neg = init_model_constraints [ \" w_priv_neg_l1 \" ] init_loss = init_model_constraints [ \" loss \" ] epsilon = parameters [ \" epsilon \" ] # New Variables w = cvx . Variable ( shape = ( self . d ) , name = \" w \" ) b = cvx . Variable ( name = \" b \" ) w_priv_pos = cvx . Variable ( self . d_priv , name = \" w_priv_pos \" ) b_priv_pos = cvx . Variable ( name = \" bias_priv_pos \" ) w_priv_neg = cvx . Variable ( self . d_priv , name = \" w_priv_neg \" ) b_priv_neg = cvx . Variable ( name = \" bias_priv_neg \" ) priv_function_pos = self . X_priv * w_priv_pos + b_priv_pos priv_function_neg = self . X_priv * w_priv_neg + b_priv_neg priv_loss = cvx . sum ( priv_function_pos + priv_function_neg ) loss = priv_loss weight_norm = cvx . norm ( w , 1 ) self . weight_norm_priv_pos = cvx . norm ( w_priv_pos , 1 ) self . weight_norm_priv_neg = cvx . norm ( w_priv_neg , 1 ) self . add_constraint ( self . y - self . X * w - b <= epsilon + priv_function_pos ) self . add_constraint ( self . X * w + b - self . y <= epsilon + priv_function_neg ) self . add_constraint ( priv_function_pos >= 0 ) self . add_constraint ( priv_function_neg >= 0 ) self . add_constraint ( loss <= init_loss ) self . add_constraint ( weight_norm <= l1_w ) self . add_constraint ( self . weight_norm_priv_pos <= self . l1_priv_w_pos ) self . add_constraint ( self . weight_norm_priv_neg <= self . l1_priv_w_neg ) # Save values for object use later self . w = w self . w_priv_pos = w_priv_pos self . w_priv_neg = w_priv_neg self . feature_relevance = cvx . Variable ( nonneg = True , name = \" Feature Relevance \" ) LUPI_Regression_SVM class ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_initmodel.LUPI_InitModel fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator Static methods ##### hyperparameter ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def hyperparameter ( cls ) : return [ \" C \" , \" epsilon \" , \" scaling_lupi_w \" ] Instance variables ``` python3 solver_params ``` : Methods ##### fit ``` python3 def ( self , X_combined , y , lupi_features = None ) ``` Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . ??? example \" View Source \" def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . \"\"\" if lupi_features is None : raise ValueError ( \" No lupi_features argument given. \" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \" C \" ] epsilon = self . hyperparam [ \" epsilon \" ] scaling_lupi_w = self . hyperparam [ \" scaling_lupi_w \" ] # scaling_lupi_loss = self . hyperparam [ \" scaling_lupi_loss \" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ) , name = \" w \" ) b = cvx . Variable ( name = \" bias \" ) w_priv_pos = cvx . Variable ( lupi_features , name = \" w_priv_pos \" ) b_priv_pos = cvx . Variable ( name = \" bias_priv_pos \" ) w_priv_neg = cvx . Variable ( lupi_features , name = \" w_priv_neg \" ) b_priv_neg = cvx . Variable ( name = \" bias_priv_neg \" ) # slack = cvx . Variable ( shape = ( n ) , name = \" slack \" ) # Define functions for better readability priv_function_pos = X_priv * w_priv_pos + b_priv_pos priv_function_neg = X_priv * w_priv_neg + b_priv_neg # Combined loss of lupi function and normal slacks , scaled by two constants priv_loss_pos = cvx . sum ( priv_function_pos ) priv_loss_neg = cvx . sum ( priv_function_neg ) priv_loss = priv_loss_pos + priv_loss_neg # slack_loss = cvx . sum ( slack ) # loss = scaling_lupi_loss * priv_loss loss = priv_loss # L1 norm regularization of both functions with 1 scaling constant weight_regularization = 0 . 5 * ( cvx . norm ( w , 1 ) + scaling_lupi_w * ( 0 . 5 * cvx . norm ( w_priv_pos , 1 ) + 0 . 5 * cvx . norm ( w_priv_neg , 1 )) ) constraints = [ y - X * w - b <= epsilon + priv_function_pos , X * w + b - y <= epsilon + priv_function_neg , priv_function_pos >= 0 , priv_function_neg >= 0 , # priv_loss_pos >= 0 , # priv_loss_neg >= 0 , # slack_loss >= 0 , # slack >= 0 , # loss >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) self . model_state = { \" signs_pos \" : priv_function_pos . value > 0 , \" signs_neg \" : priv_function_neg . value > 0 , \" w \" : w . value , \" w_priv_pos \" : w_priv_pos . value , \" w_priv_neg \" : w_priv_neg . value , \" b \" : b . value , \" b_priv_pos \" : b_priv_pos . value , \" b_priv_neg \" : b_priv_neg . value , \" lupi_features \" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else , } w_l1 = np . linalg . norm ( w . value , ord = 1 ) w_priv_pos_l1 = np . linalg . norm ( w_priv_pos . value , ord = 1 ) w_priv_neg_l1 = np . linalg . norm ( w_priv_neg . value , ord = 1 ) # We take the mean to combine all submodels ( for priv ) into a single normalization factor w_priv_l1 = w_priv_pos_l1 + w_priv_neg_l1 self . constraints = { \" priv_loss \" : priv_loss . value , # \" loss_slack \" : slack_loss . value , \" loss \" : loss . value , \" w_l1 \" : w_l1 , \" w_priv_l1 \" : w_priv_l1 , \" w_priv_pos_l1 \" : w_priv_pos_l1 , \" w_priv_neg_l1 \" : w_priv_neg_l1 , } return self ##### predict ``` python3 def ( self , X ) ``` Method to predict points using svm classification rule . We use both normal and priv . features . This function is mainly used for CV purposes to find the best parameters according to score . Parameters ---------- X : numpy . ndarray ??? example \" View Source \" def predict ( self , X ) : \"\"\" Method to predict points using svm classification rule . We use both normal and priv . features . This function is mainly used for CV purposes to find the best parameters according to score . Parameters ---------- X : numpy . ndarray \"\"\" X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \" w \" ] b = self . model_state [ \" b \" ] y = np . dot ( X , w ) + b return y ##### score ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) _check_reg_targets ( y , prediction , None ) score = r2_score ( y , prediction ) return score View Source class LUPI_Regression_SVM ( LUPI_InitModel ) : @ classmethod def hyperparameter ( cls ) : return [ \" C \" , \" epsilon \" , \" scaling_lupi_w \" ] def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . \"\"\" if lupi_features is None : raise ValueError ( \" No lupi_features argument given. \" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \" C \" ] epsilon = self . hyperparam [ \" epsilon \" ] scaling_lupi_w = self . hyperparam [ \" scaling_lupi_w \" ] # scaling_lupi_loss = self . hyperparam [ \" scaling_lupi_loss \" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ) , name = \" w \" ) b = cvx . Variable ( name = \" bias \" ) w_priv_pos = cvx . Variable ( lupi_features , name = \" w_priv_pos \" ) b_priv_pos = cvx . Variable ( name = \" bias_priv_pos \" ) w_priv_neg = cvx . Variable ( lupi_features , name = \" w_priv_neg \" ) b_priv_neg = cvx . Variable ( name = \" bias_priv_neg \" ) # slack = cvx . Variable ( shape = ( n ) , name = \" slack \" ) # Define functions for better readability priv_function_pos = X_priv * w_priv_pos + b_priv_pos priv_function_neg = X_priv * w_priv_neg + b_priv_neg # Combined loss of lupi function and normal slacks , scaled by two constants priv_loss_pos = cvx . sum ( priv_function_pos ) priv_loss_neg = cvx . sum ( priv_function_neg ) priv_loss = priv_loss_pos + priv_loss_neg # slack_loss = cvx . sum ( slack ) # loss = scaling_lupi_loss * priv_loss loss = priv_loss # L1 norm regularization of both functions with 1 scaling constant weight_regularization = 0 . 5 * ( cvx . norm ( w , 1 ) + scaling_lupi_w * ( 0 . 5 * cvx . norm ( w_priv_pos , 1 ) + 0 . 5 * cvx . norm ( w_priv_neg , 1 )) ) constraints = [ y - X * w - b <= epsilon + priv_function_pos , X * w + b - y <= epsilon + priv_function_neg , priv_function_pos >= 0 , priv_function_neg >= 0 , # priv_loss_pos >= 0 , # priv_loss_neg >= 0 , # slack_loss >= 0 , # slack >= 0 , # loss >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) self . model_state = { \" signs_pos \" : priv_function_pos . value > 0 , \" signs_neg \" : priv_function_neg . value > 0 , \" w \" : w . value , \" w_priv_pos \" : w_priv_pos . value , \" w_priv_neg \" : w_priv_neg . value , \" b \" : b . value , \" b_priv_pos \" : b_priv_pos . value , \" b_priv_neg \" : b_priv_neg . value , \" lupi_features \" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else , } w_l1 = np . linalg . norm ( w . value , ord = 1 ) w_priv_pos_l1 = np . linalg . norm ( w_priv_pos . value , ord = 1 ) w_priv_neg_l1 = np . linalg . norm ( w_priv_neg . value , ord = 1 ) # We take the mean to combine all submodels ( for priv ) into a single normalization factor w_priv_l1 = w_priv_pos_l1 + w_priv_neg_l1 self . constraints = { \" priv_loss \" : priv_loss . value , # \" loss_slack \" : slack_loss . value , \" loss \" : loss . value , \" w_l1 \" : w_l1 , \" w_priv_l1 \" : w_priv_l1 , \" w_priv_pos_l1 \" : w_priv_pos_l1 , \" w_priv_neg_l1 \" : w_priv_neg_l1 , } return self @ property def solver_params ( cls ) : return { \" solver \" : \" ECOS \" , \" verbose \" : False } def predict ( self , X ) : \"\"\" Method to predict points using svm classification rule . We use both normal and priv . features . This function is mainly used for CV purposes to find the best parameters according to score . Parameters ---------- X : numpy . ndarray \"\"\" X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \" w \" ] b = self . model_state [ \" b \" ] y = np . dot ( X , w ) + b return y def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) _check_reg_targets ( y , prediction , None ) score = r2_score ( y , prediction ) return score","title":"Lupi Regression"},{"location":"reference/fri/model/lupi_regression/#module-frimodellupi_regression","text":"View Source from itertools import product import cvxpy as cvx import numpy as np from sklearn.metrics import r2_score from sklearn.metrics.regression import _check_reg_targets from sklearn.utils import check_X_y from fri.model.base_lupi import ( LUPI_Relevance_CVXProblem , split_dataset , is_lupi_feature , ) from fri.model.regression import Regression_Relevance_Bound from .base_initmodel import LUPI_InitModel from .base_type import ProblemType class LUPI_Regression ( ProblemType ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _lupi_features = None @property def lupi_features ( self ): return self . _lupi_features @classmethod def parameters ( cls ): return [ \"C\" , \"epsilon\" , \"scaling_lupi_w\" ] @property def get_initmodel_template ( cls ): return LUPI_Regression_SVM @property def get_cvxproblem_template ( cls ): return LUPI_Regression_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , lupi_features = None ): X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \"Argument 'lupi_features' missing in fit() call.\" ) if not isinstance ( lupi_features , int ): raise ValueError ( \"Argument 'lupi_features' is not type int.\" ) if not 0 < lupi_features < d : raise ValueError ( \"Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature.\" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y class LUPI_Regression_SVM ( LUPI_InitModel ): @classmethod def hyperparameter ( cls ): return [ \"C\" , \"epsilon\" , \"scaling_lupi_w\" ] def fit ( self , X_combined , y , lupi_features = None ): \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information (PI). PI features are expected to be the last features in the dataset. \"\"\" if lupi_features is None : raise ValueError ( \"No lupi_features argument given.\" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \"C\" ] epsilon = self . hyperparam [ \"epsilon\" ] scaling_lupi_w = self . hyperparam [ \"scaling_lupi_w\" ] # scaling_lupi_loss = self.hyperparam[\"scaling_lupi_loss\"] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ), name = \"w\" ) b = cvx . Variable ( name = \"bias\" ) w_priv_pos = cvx . Variable ( lupi_features , name = \"w_priv_pos\" ) b_priv_pos = cvx . Variable ( name = \"bias_priv_pos\" ) w_priv_neg = cvx . Variable ( lupi_features , name = \"w_priv_neg\" ) b_priv_neg = cvx . Variable ( name = \"bias_priv_neg\" ) # slack = cvx.Variable(shape=(n), name=\"slack\") # Define functions for better readability priv_function_pos = X_priv * w_priv_pos + b_priv_pos priv_function_neg = X_priv * w_priv_neg + b_priv_neg # Combined loss of lupi function and normal slacks, scaled by two constants priv_loss_pos = cvx . sum ( priv_function_pos ) priv_loss_neg = cvx . sum ( priv_function_neg ) priv_loss = priv_loss_pos + priv_loss_neg # slack_loss = cvx.sum(slack) # loss = scaling_lupi_loss * priv_loss loss = priv_loss # L1 norm regularization of both functions with 1 scaling constant weight_regularization = 0.5 * ( cvx . norm ( w , 1 ) + scaling_lupi_w * ( 0.5 * cvx . norm ( w_priv_pos , 1 ) + 0.5 * cvx . norm ( w_priv_neg , 1 )) ) constraints = [ y - X * w - b <= epsilon + priv_function_pos , X * w + b - y <= epsilon + priv_function_neg , priv_function_pos >= 0 , priv_function_neg >= 0 , # priv_loss_pos >= 0, # priv_loss_neg >= 0, # slack_loss >= 0, # slack >= 0, # loss >= 0, ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem. solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) self . model_state = { \"signs_pos\" : priv_function_pos . value > 0 , \"signs_neg\" : priv_function_neg . value > 0 , \"w\" : w . value , \"w_priv_pos\" : w_priv_pos . value , \"w_priv_neg\" : w_priv_neg . value , \"b\" : b . value , \"b_priv_pos\" : b_priv_pos . value , \"b_priv_neg\" : b_priv_neg . value , \"lupi_features\" : lupi_features , # Number of lupi features in the dataset TODO: Move this somewhere else, } w_l1 = np . linalg . norm ( w . value , ord = 1 ) w_priv_pos_l1 = np . linalg . norm ( w_priv_pos . value , ord = 1 ) w_priv_neg_l1 = np . linalg . norm ( w_priv_neg . value , ord = 1 ) # We take the mean to combine all submodels (for priv) into a single normalization factor w_priv_l1 = w_priv_pos_l1 + w_priv_neg_l1 self . constraints = { \"priv_loss\" : priv_loss . value , # \"loss_slack\": slack_loss.value, \"loss\" : loss . value , \"w_l1\" : w_l1 , \"w_priv_l1\" : w_priv_l1 , \"w_priv_pos_l1\" : w_priv_pos_l1 , \"w_priv_neg_l1\" : w_priv_neg_l1 , } return self @property def solver_params ( cls ): return { \"solver\" : \"ECOS\" , \"verbose\" : False } def predict ( self , X ): \"\"\" Method to predict points using svm classification rule. We use both normal and priv. features. This function is mainly used for CV purposes to find the best parameters according to score. Parameters ---------- X : numpy.ndarray \"\"\" X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) _check_reg_targets ( y , prediction , None ) score = r2_score ( y , prediction ) return score class LUPI_Regression_Relevance_Bound ( LUPI_Relevance_CVXProblem , Regression_Relevance_Bound ): @classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ): is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems? if not is_priv : yield from super () . generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ([ 1 , - 1 ], [ True , False ]): problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem def _init_objective_LB_LUPI ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w_priv_pos [ self . lupi_index ]) <= self . feature_relevance ) self . add_constraint ( cvx . abs ( self . w_priv_neg [ self . lupi_index ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_objective_UB_LUPI ( self , pos = None , sign = None , ** kwargs ): if pos : self . add_constraint ( self . feature_relevance <= sign * self . w_priv_pos [ self . lupi_index ] ) else : self . add_constraint ( self . feature_relevance <= sign * self . w_priv_neg [ self . lupi_index ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from best initial model l1_w = init_model_constraints [ \"w_l1\" ] self . l1_priv_w_pos = init_model_constraints [ \"w_priv_pos_l1\" ] self . l1_priv_w_neg = init_model_constraints [ \"w_priv_neg_l1\" ] init_loss = init_model_constraints [ \"loss\" ] epsilon = parameters [ \"epsilon\" ] # New Variables w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) b = cvx . Variable ( name = \"b\" ) w_priv_pos = cvx . Variable ( self . d_priv , name = \"w_priv_pos\" ) b_priv_pos = cvx . Variable ( name = \"bias_priv_pos\" ) w_priv_neg = cvx . Variable ( self . d_priv , name = \"w_priv_neg\" ) b_priv_neg = cvx . Variable ( name = \"bias_priv_neg\" ) priv_function_pos = self . X_priv * w_priv_pos + b_priv_pos priv_function_neg = self . X_priv * w_priv_neg + b_priv_neg priv_loss = cvx . sum ( priv_function_pos + priv_function_neg ) loss = priv_loss weight_norm = cvx . norm ( w , 1 ) self . weight_norm_priv_pos = cvx . norm ( w_priv_pos , 1 ) self . weight_norm_priv_neg = cvx . norm ( w_priv_neg , 1 ) self . add_constraint ( self . y - self . X * w - b <= epsilon + priv_function_pos ) self . add_constraint ( self . X * w + b - self . y <= epsilon + priv_function_neg ) self . add_constraint ( priv_function_pos >= 0 ) self . add_constraint ( priv_function_neg >= 0 ) self . add_constraint ( loss <= init_loss ) self . add_constraint ( weight_norm <= l1_w ) self . add_constraint ( self . weight_norm_priv_pos <= self . l1_priv_w_pos ) self . add_constraint ( self . weight_norm_priv_neg <= self . l1_priv_w_neg ) # Save values for object use later self . w = w self . w_priv_pos = w_priv_pos self . w_priv_neg = w_priv_neg self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"Module fri.model.lupi_regression"},{"location":"reference/fri/model/lupi_regression/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/lupi_regression/#lupi_regression","text":"class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"LUPI_Regression"},{"location":"reference/fri/model/lupi_regression/#ancestors-in-mro","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_regression/#static-methods","text":"##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" , \" epsilon \" , \" scaling_lupi_w \" ]","title":"Static methods"},{"location":"reference/fri/model/lupi_regression/#instance-variables","text":"``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : ``` python3 lupi_features ``` :","title":"Instance variables"},{"location":"reference/fri/model/lupi_regression/#methods","text":"##### preprocessing ``` python3 def ( self , data , lupi_features = None ) ``` ??? example \" View Source \" def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class LUPI_Regression ( ProblemType ) : def __init__ ( self , ** kwargs ) : super () . __init__ ( ** kwargs ) self . _lupi_features = None @ property def lupi_features ( self ) : return self . _lupi_features @ classmethod def parameters ( cls ) : return [ \" C \" , \" epsilon \" , \" scaling_lupi_w \" ] @ property def get_initmodel_template ( cls ) : return LUPI_Regression_SVM @ property def get_cvxproblem_template ( cls ) : return LUPI_Regression_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , lupi_features = None ) : X , y = data d = X . shape [ 1 ] if lupi_features is None : raise ValueError ( \" Argument 'lupi_features' missing in fit() call. \" ) if not isinstance ( lupi_features , int ) : raise ValueError ( \" Argument 'lupi_features' is not type int. \" ) if not 0 < lupi_features < d : raise ValueError ( \" Argument 'lupi_features' looks wrong. We need at least 1 priviliged feature (>0) or at least one normal feature. \" ) self . _lupi_features = lupi_features # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y","title":"Methods"},{"location":"reference/fri/model/lupi_regression/#lupi_regression_relevance_bound","text":"class ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 ) Helper class that provides a standard way to create an ABC using inheritance.","title":"LUPI_Regression_Relevance_Bound"},{"location":"reference/fri/model/lupi_regression/#ancestors-in-mro_1","text":"fri.model.base_lupi.LUPI_Relevance_CVXProblem fri.model.regression.Regression_Relevance_Bound fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_regression/#static-methods_1","text":"##### generate_upper_bound_problem ``` python3 def ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 ) ``` ??? example \" View Source \" @ classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super () . generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ( [ 1 , - 1 ], [ True , False ] ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem View Source class LUPI_Regression_Relevance_Bound ( LUPI_Relevance_CVXProblem , Regression_Relevance_Bound ) : @ classmethod def generate_upper_bound_problem ( cls , best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID =- 1 , ) : is_priv = is_lupi_feature ( di , data , best_model_state ) # Is it a lupi feature where we need additional candidate problems ? if not is_priv : yield from super () . generate_upper_bound_problem ( best_hyperparameters , init_constraints , best_model_state , data , di , preset_model , probeID = probeID , ) else : for sign , pos in product ( [ 1 , - 1 ], [ True , False ] ) : problem = cls ( di , data , best_hyperparameters , init_constraints , preset_model = preset_model , best_model_state = best_model_state , probeID = probeID , ) problem . init_objective_UB ( sign = sign , pos = pos ) yield problem def _init_objective_LB_LUPI ( self , ** kwargs ) : self . add_constraint ( cvx . abs ( self . w_priv_pos [ self . lupi_index ] ) <= self . feature_relevance ) self . add_constraint ( cvx . abs ( self . w_priv_neg [ self . lupi_index ] ) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_objective_UB_LUPI ( self , pos = None , sign = None , ** kwargs ) : if pos : self . add_constraint ( self . feature_relevance <= sign * self . w_priv_pos [ self . lupi_index ] ) else : self . add_constraint ( self . feature_relevance <= sign * self . w_priv_neg [ self . lupi_index ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ) : # Upper constraints from best initial model l1_w = init_model_constraints [ \" w_l1 \" ] self . l1_priv_w_pos = init_model_constraints [ \" w_priv_pos_l1 \" ] self . l1_priv_w_neg = init_model_constraints [ \" w_priv_neg_l1 \" ] init_loss = init_model_constraints [ \" loss \" ] epsilon = parameters [ \" epsilon \" ] # New Variables w = cvx . Variable ( shape = ( self . d ) , name = \" w \" ) b = cvx . Variable ( name = \" b \" ) w_priv_pos = cvx . Variable ( self . d_priv , name = \" w_priv_pos \" ) b_priv_pos = cvx . Variable ( name = \" bias_priv_pos \" ) w_priv_neg = cvx . Variable ( self . d_priv , name = \" w_priv_neg \" ) b_priv_neg = cvx . Variable ( name = \" bias_priv_neg \" ) priv_function_pos = self . X_priv * w_priv_pos + b_priv_pos priv_function_neg = self . X_priv * w_priv_neg + b_priv_neg priv_loss = cvx . sum ( priv_function_pos + priv_function_neg ) loss = priv_loss weight_norm = cvx . norm ( w , 1 ) self . weight_norm_priv_pos = cvx . norm ( w_priv_pos , 1 ) self . weight_norm_priv_neg = cvx . norm ( w_priv_neg , 1 ) self . add_constraint ( self . y - self . X * w - b <= epsilon + priv_function_pos ) self . add_constraint ( self . X * w + b - self . y <= epsilon + priv_function_neg ) self . add_constraint ( priv_function_pos >= 0 ) self . add_constraint ( priv_function_neg >= 0 ) self . add_constraint ( loss <= init_loss ) self . add_constraint ( weight_norm <= l1_w ) self . add_constraint ( self . weight_norm_priv_pos <= self . l1_priv_w_pos ) self . add_constraint ( self . weight_norm_priv_neg <= self . l1_priv_w_neg ) # Save values for object use later self . w = w self . w_priv_pos = w_priv_pos self . w_priv_neg = w_priv_neg self . feature_relevance = cvx . Variable ( nonneg = True , name = \" Feature Relevance \" )","title":"Static methods"},{"location":"reference/fri/model/lupi_regression/#lupi_regression_svm","text":"class ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance.","title":"LUPI_Regression_SVM"},{"location":"reference/fri/model/lupi_regression/#ancestors-in-mro_2","text":"fri.model.base_initmodel.LUPI_InitModel fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/lupi_regression/#static-methods_2","text":"##### hyperparameter ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def hyperparameter ( cls ) : return [ \" C \" , \" epsilon \" , \" scaling_lupi_w \" ]","title":"Static methods"},{"location":"reference/fri/model/lupi_regression/#instance-variables_1","text":"``` python3 solver_params ``` :","title":"Instance variables"},{"location":"reference/fri/model/lupi_regression/#methods_1","text":"##### fit ``` python3 def ( self , X_combined , y , lupi_features = None ) ``` Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . ??? example \" View Source \" def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . \"\"\" if lupi_features is None : raise ValueError ( \" No lupi_features argument given. \" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \" C \" ] epsilon = self . hyperparam [ \" epsilon \" ] scaling_lupi_w = self . hyperparam [ \" scaling_lupi_w \" ] # scaling_lupi_loss = self . hyperparam [ \" scaling_lupi_loss \" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ) , name = \" w \" ) b = cvx . Variable ( name = \" bias \" ) w_priv_pos = cvx . Variable ( lupi_features , name = \" w_priv_pos \" ) b_priv_pos = cvx . Variable ( name = \" bias_priv_pos \" ) w_priv_neg = cvx . Variable ( lupi_features , name = \" w_priv_neg \" ) b_priv_neg = cvx . Variable ( name = \" bias_priv_neg \" ) # slack = cvx . Variable ( shape = ( n ) , name = \" slack \" ) # Define functions for better readability priv_function_pos = X_priv * w_priv_pos + b_priv_pos priv_function_neg = X_priv * w_priv_neg + b_priv_neg # Combined loss of lupi function and normal slacks , scaled by two constants priv_loss_pos = cvx . sum ( priv_function_pos ) priv_loss_neg = cvx . sum ( priv_function_neg ) priv_loss = priv_loss_pos + priv_loss_neg # slack_loss = cvx . sum ( slack ) # loss = scaling_lupi_loss * priv_loss loss = priv_loss # L1 norm regularization of both functions with 1 scaling constant weight_regularization = 0 . 5 * ( cvx . norm ( w , 1 ) + scaling_lupi_w * ( 0 . 5 * cvx . norm ( w_priv_pos , 1 ) + 0 . 5 * cvx . norm ( w_priv_neg , 1 )) ) constraints = [ y - X * w - b <= epsilon + priv_function_pos , X * w + b - y <= epsilon + priv_function_neg , priv_function_pos >= 0 , priv_function_neg >= 0 , # priv_loss_pos >= 0 , # priv_loss_neg >= 0 , # slack_loss >= 0 , # slack >= 0 , # loss >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) self . model_state = { \" signs_pos \" : priv_function_pos . value > 0 , \" signs_neg \" : priv_function_neg . value > 0 , \" w \" : w . value , \" w_priv_pos \" : w_priv_pos . value , \" w_priv_neg \" : w_priv_neg . value , \" b \" : b . value , \" b_priv_pos \" : b_priv_pos . value , \" b_priv_neg \" : b_priv_neg . value , \" lupi_features \" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else , } w_l1 = np . linalg . norm ( w . value , ord = 1 ) w_priv_pos_l1 = np . linalg . norm ( w_priv_pos . value , ord = 1 ) w_priv_neg_l1 = np . linalg . norm ( w_priv_neg . value , ord = 1 ) # We take the mean to combine all submodels ( for priv ) into a single normalization factor w_priv_l1 = w_priv_pos_l1 + w_priv_neg_l1 self . constraints = { \" priv_loss \" : priv_loss . value , # \" loss_slack \" : slack_loss . value , \" loss \" : loss . value , \" w_l1 \" : w_l1 , \" w_priv_l1 \" : w_priv_l1 , \" w_priv_pos_l1 \" : w_priv_pos_l1 , \" w_priv_neg_l1 \" : w_priv_neg_l1 , } return self ##### predict ``` python3 def ( self , X ) ``` Method to predict points using svm classification rule . We use both normal and priv . features . This function is mainly used for CV purposes to find the best parameters according to score . Parameters ---------- X : numpy . ndarray ??? example \" View Source \" def predict ( self , X ) : \"\"\" Method to predict points using svm classification rule . We use both normal and priv . features . This function is mainly used for CV purposes to find the best parameters according to score . Parameters ---------- X : numpy . ndarray \"\"\" X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \" w \" ] b = self . model_state [ \" b \" ] y = np . dot ( X , w ) + b return y ##### score ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) _check_reg_targets ( y , prediction , None ) score = r2_score ( y , prediction ) return score View Source class LUPI_Regression_SVM ( LUPI_InitModel ) : @ classmethod def hyperparameter ( cls ) : return [ \" C \" , \" epsilon \" , \" scaling_lupi_w \" ] def fit ( self , X_combined , y , lupi_features = None ) : \"\"\" Parameters ---------- lupi_features : int Number of features in dataset which are considered privileged information ( PI ) . PI features are expected to be the last features in the dataset . \"\"\" if lupi_features is None : raise ValueError ( \" No lupi_features argument given. \" ) self . lupi_features = lupi_features X , X_priv = split_dataset ( X_combined , lupi_features ) ( n , d ) = X . shape # Get parameters from CV model without any feature contstraints C = self . hyperparam [ \" C \" ] epsilon = self . hyperparam [ \" epsilon \" ] scaling_lupi_w = self . hyperparam [ \" scaling_lupi_w \" ] # scaling_lupi_loss = self . hyperparam [ \" scaling_lupi_loss \" ] # Initalize Variables in cvxpy w = cvx . Variable ( shape = ( d ) , name = \" w \" ) b = cvx . Variable ( name = \" bias \" ) w_priv_pos = cvx . Variable ( lupi_features , name = \" w_priv_pos \" ) b_priv_pos = cvx . Variable ( name = \" bias_priv_pos \" ) w_priv_neg = cvx . Variable ( lupi_features , name = \" w_priv_neg \" ) b_priv_neg = cvx . Variable ( name = \" bias_priv_neg \" ) # slack = cvx . Variable ( shape = ( n ) , name = \" slack \" ) # Define functions for better readability priv_function_pos = X_priv * w_priv_pos + b_priv_pos priv_function_neg = X_priv * w_priv_neg + b_priv_neg # Combined loss of lupi function and normal slacks , scaled by two constants priv_loss_pos = cvx . sum ( priv_function_pos ) priv_loss_neg = cvx . sum ( priv_function_neg ) priv_loss = priv_loss_pos + priv_loss_neg # slack_loss = cvx . sum ( slack ) # loss = scaling_lupi_loss * priv_loss loss = priv_loss # L1 norm regularization of both functions with 1 scaling constant weight_regularization = 0 . 5 * ( cvx . norm ( w , 1 ) + scaling_lupi_w * ( 0 . 5 * cvx . norm ( w_priv_pos , 1 ) + 0 . 5 * cvx . norm ( w_priv_neg , 1 )) ) constraints = [ y - X * w - b <= epsilon + priv_function_pos , X * w + b - y <= epsilon + priv_function_neg , priv_function_pos >= 0 , priv_function_neg >= 0 , # priv_loss_pos >= 0 , # priv_loss_neg >= 0 , # slack_loss >= 0 , # slack >= 0 , # loss >= 0 , ] objective = cvx . Minimize ( C * loss + weight_regularization ) # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) self . model_state = { \" signs_pos \" : priv_function_pos . value > 0 , \" signs_neg \" : priv_function_neg . value > 0 , \" w \" : w . value , \" w_priv_pos \" : w_priv_pos . value , \" w_priv_neg \" : w_priv_neg . value , \" b \" : b . value , \" b_priv_pos \" : b_priv_pos . value , \" b_priv_neg \" : b_priv_neg . value , \" lupi_features \" : lupi_features , # Number of lupi features in the dataset TODO : Move this somewhere else , } w_l1 = np . linalg . norm ( w . value , ord = 1 ) w_priv_pos_l1 = np . linalg . norm ( w_priv_pos . value , ord = 1 ) w_priv_neg_l1 = np . linalg . norm ( w_priv_neg . value , ord = 1 ) # We take the mean to combine all submodels ( for priv ) into a single normalization factor w_priv_l1 = w_priv_pos_l1 + w_priv_neg_l1 self . constraints = { \" priv_loss \" : priv_loss . value , # \" loss_slack \" : slack_loss . value , \" loss \" : loss . value , \" w_l1 \" : w_l1 , \" w_priv_l1 \" : w_priv_l1 , \" w_priv_pos_l1 \" : w_priv_pos_l1 , \" w_priv_neg_l1 \" : w_priv_neg_l1 , } return self @ property def solver_params ( cls ) : return { \" solver \" : \" ECOS \" , \" verbose \" : False } def predict ( self , X ) : \"\"\" Method to predict points using svm classification rule . We use both normal and priv . features . This function is mainly used for CV purposes to find the best parameters according to score . Parameters ---------- X : numpy . ndarray \"\"\" X , X_priv = split_dataset ( X , self . lupi_features ) w = self . model_state [ \" w \" ] b = self . model_state [ \" b \" ] y = np . dot ( X , w ) + b return y def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) _check_reg_targets ( y , prediction , None ) score = r2_score ( y , prediction ) return score","title":"Methods"},{"location":"reference/fri/model/ordinal_regression/","text":"Module fri.model.ordinal_regression View Source import cvxpy as cvx import numpy as np from sklearn.metrics import make_scorer from sklearn.utils import check_X_y from .base_cvxproblem import Relevance_CVXProblem from .base_initmodel import InitModel from .base_type import ProblemType class OrdinalRegression ( ProblemType ): @classmethod def parameters ( cls ): return [ \"C\" ] @property def get_initmodel_template ( cls ): return OrdinalRegression_SVM @property def get_cvxproblem_template ( cls ): return OrdinalRegression_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y class OrdinalRegression_SVM ( InitModel ): @classmethod def hyperparameter ( cls ): return [ \"C\" ] def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . hyperparam [ \"C\" ] self . classes_ = np . unique ( y ) original_bins = sorted ( self . classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) w = cvx . Variable ( shape = ( d ), name = \"w\" ) # For ordinal regression we use two slack variables, we observe the slack in both directions slack_left = cvx . Variable ( shape = ( n ), name = \"slack_left\" ) slack_right = cvx . Variable ( shape = ( n ), name = \"slack_right\" ) # We have an offset for every bin boundary b_s = cvx . Variable ( shape = ( n_bins - 1 ), name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack_left + slack_right )) constraints = [ slack_left >= 0 , slack_right >= 0 ] # Add constraints for slack into left neighboring bins for i in range ( n_bins - 1 ): indices = np . where ( y == get_old_bin [ i ]) constraints . append ( X [ indices ] * w - slack_left [ indices ] <= b_s [ i ] - 1 ) # Add constraints for slack into right neighboring bins for i in range ( 1 , n_bins ): indices = np . where ( y == get_old_bin [ i ]) constraints . append ( X [ indices ] * w + slack_right [ indices ] >= b_s [ i - 1 ] + 1 ) # Add explicit constraint, that all bins are ascending for i in range ( n_bins - 2 ): constraints . append ( b_s [ i ] <= b_s [ i + 1 ]) # Solve problem. solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b_s = b_s . value slack_left = np . asarray ( slack_left . value ) . flatten () slack_right = np . asarray ( slack_right . value ) . flatten () self . model_state = { \"w\" : w , \"b_s\" : b_s , \"slack\" : ( slack_left , slack_right )} loss = np . sum ( slack_left + slack_right ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ): w = self . model_state [ \"w\" ] b_s = self . model_state [ \"b_s\" ] scores = np . dot ( X , w . T )[ np . newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] def score ( self , X , y , error_type = \"mmae\" , return_error = False , ** kwargs ): X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score def make_scorer ( self ): # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \"mze\" ) mae = make_scorer ( ordinal_scores , error_type = \"mae\" ) mmae = make_scorer ( ordinal_scores , error_type = \"mmae\" ) scorer = { \"mze\" : mze , \"mae\" : mae , \"mmae\" : mmae } return scorer , \"mmae\" def ordinal_scores ( y , prediction , error_type , return_error = False ): \"\"\"Score function for ordinal problems. Parameters ---------- y : target class vector Truth vector prediction : prediction class vector Predicted classes error_type : str Error type \"mze\",\"mae\",\"mmae\" return_error : bool, optional Return error (lower is better) or score (inverted, higher is better) Returns ------- float Error or score depending on 'return_error' Raises ------ ValueError When using wrong error_type \"\"\" n = len ( y ) classes = np . unique ( y ) n_bins = len ( classes ) max_dist = n_bins - 1 # If only one class available, we dont need to average if max_dist == 0 : error_type = \"mze\" def mze ( prediction , y ): return np . sum ( prediction != y ) def mae ( prediction , y ): return np . sum ( np . abs ( prediction - y )) # Score based on mean zero-one error if error_type == \"mze\" : error = mze ( prediction , y ) / n score = 1 - error # Score based on mean absolute error elif error_type == \"mae\" : error = mae ( prediction , y ) / n score = ( max_dist - error ) / max_dist # Score based on macro-averaged mean absolute error elif error_type == \"mmae\" : sum = 0 for i in range ( n_bins ): samples = y == i n_samples = np . sum ( samples ) if n_samples > 0 : bin_error = mae ( prediction [ samples ], y [ samples ]) / n_samples sum += bin_error error = sum / n_bins score = ( max_dist - error ) / max_dist else : raise ValueError ( \"error_type {} not available!'\" . format ( error_type )) if return_error : return error else : return score class OrdinalRegression_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): n_bins = len ( np . unique ( self . y )) # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] # New Variables self . w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) # For ordinal regression we use two slack variables, we observe the slack in both directions self . slack_left = cvx . Variable ( shape = ( self . n ), name = \"slack_left\" , nonneg = True ) self . slack_right = cvx . Variable ( shape = ( self . n ), name = \"slack_right\" , nonneg = True ) # We have an offset for every bin boundary self . b_s = cvx . Variable ( shape = ( n_bins - 1 ), name = \"bias\" ) # New Constraints self . loss = cvx . sum ( self . slack_left + self . slack_right ) self . weight_norm = cvx . norm ( self . w , 1 ) for i in range ( n_bins - 1 ): indices = np . where ( self . y == i ) self . add_constraint ( self . X [ indices ] * self . w - self . slack_left [ indices ] <= self . b_s [ i ] - 1 ) for i in range ( 1 , n_bins ): indices = np . where ( self . y == i ) self . add_constraint ( self . X [ indices ] * self . w + self . slack_right [ indices ] >= self . b_s [ i - 1 ] + 1 ) for i in range ( n_bins - 2 ): self . add_constraint ( self . b_s [ i ] <= self . b_s [ i + 1 ]) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Functions ordinal_scores def ( y , prediction , error_type , return_error = False ) Score function for ordinal problems. Parameters y : target class vector Truth vector prediction : prediction class vector Predicted classes error_type : str Error type \"mze\",\"mae\",\"mmae\" return_error : bool, optional Return error (lower is better) or score (inverted, higher is better) Returns float Error or score depending on 'return_error' Raises ValueError When using wrong error_type View Source def ordinal_scores ( y , prediction , error_type , return_error = False ) : \"\"\" Score function for ordinal problems. Parameters ---------- y : target class vector Truth vector prediction : prediction class vector Predicted classes error_type : str Error type \" mze \" , \" mae \" , \" mmae \" return_error : bool , optional Return error ( lower is better ) or score ( inverted , higher is better ) Returns ------- float Error or score depending on ' return_error ' Raises ------ ValueError When using wrong error_type \"\"\" n = len ( y ) classes = np . unique ( y ) n_bins = len ( classes ) max_dist = n_bins - 1 # If only one class available , we dont need to average if max_dist == 0 : error_type = \" mze \" def mze ( prediction , y ) : return np . sum ( prediction != y ) def mae ( prediction , y ) : return np . sum ( np . abs ( prediction - y )) # Score based on mean zero - one error if error_type == \" mze \" : error = mze ( prediction , y ) / n score = 1 - error # Score based on mean absolute error elif error_type == \" mae \" : error = mae ( prediction , y ) / n score = ( max_dist - error ) / max_dist # Score based on macro - averaged mean absolute error elif error_type == \" mmae \" : sum = 0 for i in range ( n_bins ) : samples = y == i n_samples = np . sum ( samples ) if n_samples > 0 : bin_error = mae ( prediction [ samples ], y [ samples ] ) / n_samples sum += bin_error error = sum / n_bins score = ( max_dist - error ) / max_dist else : raise ValueError ( \" error_type {} not available!' \" . format ( error_type )) if return_error : return error else : return score Classes OrdinalRegression class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods ##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" ] Instance variables ``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : Methods ##### preprocessing ``` python3 def ( self , data , ** kwargs ) ``` ??? example \" View Source \" def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \" First ordinal class has index > 0. Shifting index... \" ) y = y - np . min ( y ) return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class OrdinalRegression ( ProblemType ) : @ classmethod def parameters ( cls ) : return [ \" C \" ] @ property def get_initmodel_template ( cls ) : return OrdinalRegression_SVM @ property def get_cvxproblem_template ( cls ) : return OrdinalRegression_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \" First ordinal class has index > 0. Shifting index... \" ) y = y - np . min ( y ) return X , y OrdinalRegression_Relevance_Bound class ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC Descendants fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression_Relevance_Bound Methods ##### init_objective_LB ``` python3 def ( self , ** kwargs ) ``` ??? example \"View Source\" def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) ##### init_objective_UB ``` python3 def ( self , sign = None , ** kwargs ) ``` ??? example \"View Source\" def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) View Source class OrdinalRegression_Relevance_Bound ( Relevance_CVXProblem ) : def init_objective_UB ( self , sign = None , ** kwargs ) : self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ) : self . add_constraint ( cvx . abs ( self . w [ self . current_feature ] ) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ) : n_bins = len ( np . unique ( self . y )) # Upper constraints from initial model l1_w = init_model_constraints [ \" w_l1 \" ] init_loss = init_model_constraints [ \" loss \" ] C = parameters [ \" C \" ] # New Variables self . w = cvx . Variable ( shape = ( self . d ) , name = \" w \" ) # For ordinal regression we use two slack variables , we observe the slack in both directions self . slack_left = cvx . Variable ( shape = ( self . n ) , name = \" slack_left \" , nonneg = True ) self . slack_right = cvx . Variable ( shape = ( self . n ) , name = \" slack_right \" , nonneg = True ) # We have an offset for every bin boundary self . b_s = cvx . Variable ( shape = ( n_bins - 1 ) , name = \" bias \" ) # New Constraints self . loss = cvx . sum ( self . slack_left + self . slack_right ) self . weight_norm = cvx . norm ( self . w , 1 ) for i in range ( n_bins - 1 ) : indices = np . where ( self . y == i ) self . add_constraint ( self . X [ indices ] * self . w - self . slack_left [ indices ] <= self . b_s [ i ] - 1 ) for i in range ( 1 , n_bins ) : indices = np . where ( self . y == i ) self . add_constraint ( self . X [ indices ] * self . w + self . slack_right [ indices ] >= self . b_s [ i - 1 ] + 1 ) for i in range ( n_bins - 2 ) : self . add_constraint ( self . b_s [ i ] <= self . b_s [ i + 1 ] ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \" Feature Relevance \" ) OrdinalRegression_SVM class ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator Static methods ##### hyperparameter ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def hyperparameter ( cls ) : return [ \" C \" ] Methods ##### fit ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" def fit ( self , X , y , ** kwargs ) : ( n , d ) = X . shape C = self . hyperparam [ \" C \" ] self . classes_ = np . unique ( y ) original_bins = sorted ( self . classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) w = cvx . Variable ( shape = ( d ) , name = \" w \" ) # For ordinal regression we use two slack variables , we observe the slack in both directions slack_left = cvx . Variable ( shape = ( n ) , name = \" slack_left \" ) slack_right = cvx . Variable ( shape = ( n ) , name = \" slack_right \" ) # We have an offset for every bin boundary b_s = cvx . Variable ( shape = ( n_bins - 1 ) , name = \" bias \" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack_left + slack_right )) constraints = [ slack_left >= 0 , slack_right >= 0 ] # Add constraints for slack into left neighboring bins for i in range ( n_bins - 1 ) : indices = np . where ( y == get_old_bin [ i ] ) constraints . append ( X [ indices ] * w - slack_left [ indices ] <= b_s [ i ] - 1 ) # Add constraints for slack into right neighboring bins for i in range ( 1 , n_bins ) : indices = np . where ( y == get_old_bin [ i ] ) constraints . append ( X [ indices ] * w + slack_right [ indices ] >= b_s [ i - 1 ] + 1 ) # Add explicit constraint , that all bins are ascending for i in range ( n_bins - 2 ) : constraints . append ( b_s [ i ] <= b_s [ i + 1 ] ) # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b_s = b_s . value slack_left = np . asarray ( slack_left . value ) . flatten () slack_right = np . asarray ( slack_right . value ) . flatten () self . model_state = { \" w \" : w , \" b_s \" : b_s , \" slack \" : ( slack_left , slack_right ) } loss = np . sum ( slack_left + slack_right ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \" loss \" : loss , \" w_l1 \" : w_l1 } return self ##### make_scorer ``` python3 def ( self ) ``` ??? example \" View Source \" def make_scorer ( self ) : # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \" mze \" ) mae = make_scorer ( ordinal_scores , error_type = \" mae \" ) mmae = make_scorer ( ordinal_scores , error_type = \" mmae \" ) scorer = { \" mze \" : mze , \" mae \" : mae , \" mmae \" : mmae } return scorer , \" mmae \" ##### predict ``` python3 def ( self , X ) ``` ??? example \" View Source \" def predict ( self , X ) : w = self . model_state [ \" w \" ] b_s = self . model_state [ \" b_s \" ] scores = np . dot ( X , w . T ) [ np . newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] ##### score ``` python3 def ( self , X , y , error_type = ' mmae ' , return_error = False , ** kwargs ) ``` ??? example \" View Source \" def score ( self , X , y , error_type = \" mmae \" , return_error = False , ** kwargs ) : X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score View Source class OrdinalRegression_SVM ( InitModel ) : @ classmethod def hyperparameter ( cls ) : return [ \" C \" ] def fit ( self , X , y , ** kwargs ) : ( n , d ) = X . shape C = self . hyperparam [ \" C \" ] self . classes_ = np . unique ( y ) original_bins = sorted ( self . classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) w = cvx . Variable ( shape = ( d ) , name = \" w \" ) # For ordinal regression we use two slack variables , we observe the slack in both directions slack_left = cvx . Variable ( shape = ( n ) , name = \" slack_left \" ) slack_right = cvx . Variable ( shape = ( n ) , name = \" slack_right \" ) # We have an offset for every bin boundary b_s = cvx . Variable ( shape = ( n_bins - 1 ) , name = \" bias \" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack_left + slack_right )) constraints = [ slack_left >= 0 , slack_right >= 0 ] # Add constraints for slack into left neighboring bins for i in range ( n_bins - 1 ) : indices = np . where ( y == get_old_bin [ i ] ) constraints . append ( X [ indices ] * w - slack_left [ indices ] <= b_s [ i ] - 1 ) # Add constraints for slack into right neighboring bins for i in range ( 1 , n_bins ) : indices = np . where ( y == get_old_bin [ i ] ) constraints . append ( X [ indices ] * w + slack_right [ indices ] >= b_s [ i - 1 ] + 1 ) # Add explicit constraint , that all bins are ascending for i in range ( n_bins - 2 ) : constraints . append ( b_s [ i ] <= b_s [ i + 1 ] ) # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b_s = b_s . value slack_left = np . asarray ( slack_left . value ) . flatten () slack_right = np . asarray ( slack_right . value ) . flatten () self . model_state = { \" w \" : w , \" b_s \" : b_s , \" slack \" : ( slack_left , slack_right ) } loss = np . sum ( slack_left + slack_right ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \" loss \" : loss , \" w_l1 \" : w_l1 } return self def predict ( self , X ) : w = self . model_state [ \" w \" ] b_s = self . model_state [ \" b_s \" ] scores = np . dot ( X , w . T ) [ np . newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] def score ( self , X , y , error_type = \" mmae \" , return_error = False , ** kwargs ) : X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score def make_scorer ( self ) : # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \" mze \" ) mae = make_scorer ( ordinal_scores , error_type = \" mae \" ) mmae = make_scorer ( ordinal_scores , error_type = \" mmae \" ) scorer = { \" mze \" : mze , \" mae \" : mae , \" mmae \" : mmae } return scorer , \" mmae \"","title":"Ordinal Regression"},{"location":"reference/fri/model/ordinal_regression/#module-frimodelordinal_regression","text":"View Source import cvxpy as cvx import numpy as np from sklearn.metrics import make_scorer from sklearn.utils import check_X_y from .base_cvxproblem import Relevance_CVXProblem from .base_initmodel import InitModel from .base_type import ProblemType class OrdinalRegression ( ProblemType ): @classmethod def parameters ( cls ): return [ \"C\" ] @property def get_initmodel_template ( cls ): return OrdinalRegression_SVM @property def get_cvxproblem_template ( cls ): return OrdinalRegression_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \"First ordinal class has index > 0. Shifting index...\" ) y = y - np . min ( y ) return X , y class OrdinalRegression_SVM ( InitModel ): @classmethod def hyperparameter ( cls ): return [ \"C\" ] def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . hyperparam [ \"C\" ] self . classes_ = np . unique ( y ) original_bins = sorted ( self . classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) w = cvx . Variable ( shape = ( d ), name = \"w\" ) # For ordinal regression we use two slack variables, we observe the slack in both directions slack_left = cvx . Variable ( shape = ( n ), name = \"slack_left\" ) slack_right = cvx . Variable ( shape = ( n ), name = \"slack_right\" ) # We have an offset for every bin boundary b_s = cvx . Variable ( shape = ( n_bins - 1 ), name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack_left + slack_right )) constraints = [ slack_left >= 0 , slack_right >= 0 ] # Add constraints for slack into left neighboring bins for i in range ( n_bins - 1 ): indices = np . where ( y == get_old_bin [ i ]) constraints . append ( X [ indices ] * w - slack_left [ indices ] <= b_s [ i ] - 1 ) # Add constraints for slack into right neighboring bins for i in range ( 1 , n_bins ): indices = np . where ( y == get_old_bin [ i ]) constraints . append ( X [ indices ] * w + slack_right [ indices ] >= b_s [ i - 1 ] + 1 ) # Add explicit constraint, that all bins are ascending for i in range ( n_bins - 2 ): constraints . append ( b_s [ i ] <= b_s [ i + 1 ]) # Solve problem. solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b_s = b_s . value slack_left = np . asarray ( slack_left . value ) . flatten () slack_right = np . asarray ( slack_right . value ) . flatten () self . model_state = { \"w\" : w , \"b_s\" : b_s , \"slack\" : ( slack_left , slack_right )} loss = np . sum ( slack_left + slack_right ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ): w = self . model_state [ \"w\" ] b_s = self . model_state [ \"b_s\" ] scores = np . dot ( X , w . T )[ np . newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] def score ( self , X , y , error_type = \"mmae\" , return_error = False , ** kwargs ): X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score def make_scorer ( self ): # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \"mze\" ) mae = make_scorer ( ordinal_scores , error_type = \"mae\" ) mmae = make_scorer ( ordinal_scores , error_type = \"mmae\" ) scorer = { \"mze\" : mze , \"mae\" : mae , \"mmae\" : mmae } return scorer , \"mmae\" def ordinal_scores ( y , prediction , error_type , return_error = False ): \"\"\"Score function for ordinal problems. Parameters ---------- y : target class vector Truth vector prediction : prediction class vector Predicted classes error_type : str Error type \"mze\",\"mae\",\"mmae\" return_error : bool, optional Return error (lower is better) or score (inverted, higher is better) Returns ------- float Error or score depending on 'return_error' Raises ------ ValueError When using wrong error_type \"\"\" n = len ( y ) classes = np . unique ( y ) n_bins = len ( classes ) max_dist = n_bins - 1 # If only one class available, we dont need to average if max_dist == 0 : error_type = \"mze\" def mze ( prediction , y ): return np . sum ( prediction != y ) def mae ( prediction , y ): return np . sum ( np . abs ( prediction - y )) # Score based on mean zero-one error if error_type == \"mze\" : error = mze ( prediction , y ) / n score = 1 - error # Score based on mean absolute error elif error_type == \"mae\" : error = mae ( prediction , y ) / n score = ( max_dist - error ) / max_dist # Score based on macro-averaged mean absolute error elif error_type == \"mmae\" : sum = 0 for i in range ( n_bins ): samples = y == i n_samples = np . sum ( samples ) if n_samples > 0 : bin_error = mae ( prediction [ samples ], y [ samples ]) / n_samples sum += bin_error error = sum / n_bins score = ( max_dist - error ) / max_dist else : raise ValueError ( \"error_type {} not available!'\" . format ( error_type )) if return_error : return error else : return score class OrdinalRegression_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): n_bins = len ( np . unique ( self . y )) # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] # New Variables self . w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) # For ordinal regression we use two slack variables, we observe the slack in both directions self . slack_left = cvx . Variable ( shape = ( self . n ), name = \"slack_left\" , nonneg = True ) self . slack_right = cvx . Variable ( shape = ( self . n ), name = \"slack_right\" , nonneg = True ) # We have an offset for every bin boundary self . b_s = cvx . Variable ( shape = ( n_bins - 1 ), name = \"bias\" ) # New Constraints self . loss = cvx . sum ( self . slack_left + self . slack_right ) self . weight_norm = cvx . norm ( self . w , 1 ) for i in range ( n_bins - 1 ): indices = np . where ( self . y == i ) self . add_constraint ( self . X [ indices ] * self . w - self . slack_left [ indices ] <= self . b_s [ i ] - 1 ) for i in range ( 1 , n_bins ): indices = np . where ( self . y == i ) self . add_constraint ( self . X [ indices ] * self . w + self . slack_right [ indices ] >= self . b_s [ i - 1 ] + 1 ) for i in range ( n_bins - 2 ): self . add_constraint ( self . b_s [ i ] <= self . b_s [ i + 1 ]) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"Module fri.model.ordinal_regression"},{"location":"reference/fri/model/ordinal_regression/#functions","text":"","title":"Functions"},{"location":"reference/fri/model/ordinal_regression/#ordinal_scores","text":"def ( y , prediction , error_type , return_error = False ) Score function for ordinal problems.","title":"ordinal_scores"},{"location":"reference/fri/model/ordinal_regression/#parameters","text":"y : target class vector Truth vector prediction : prediction class vector Predicted classes error_type : str Error type \"mze\",\"mae\",\"mmae\" return_error : bool, optional Return error (lower is better) or score (inverted, higher is better)","title":"Parameters"},{"location":"reference/fri/model/ordinal_regression/#returns","text":"float Error or score depending on 'return_error'","title":"Returns"},{"location":"reference/fri/model/ordinal_regression/#raises","text":"ValueError When using wrong error_type View Source def ordinal_scores ( y , prediction , error_type , return_error = False ) : \"\"\" Score function for ordinal problems. Parameters ---------- y : target class vector Truth vector prediction : prediction class vector Predicted classes error_type : str Error type \" mze \" , \" mae \" , \" mmae \" return_error : bool , optional Return error ( lower is better ) or score ( inverted , higher is better ) Returns ------- float Error or score depending on ' return_error ' Raises ------ ValueError When using wrong error_type \"\"\" n = len ( y ) classes = np . unique ( y ) n_bins = len ( classes ) max_dist = n_bins - 1 # If only one class available , we dont need to average if max_dist == 0 : error_type = \" mze \" def mze ( prediction , y ) : return np . sum ( prediction != y ) def mae ( prediction , y ) : return np . sum ( np . abs ( prediction - y )) # Score based on mean zero - one error if error_type == \" mze \" : error = mze ( prediction , y ) / n score = 1 - error # Score based on mean absolute error elif error_type == \" mae \" : error = mae ( prediction , y ) / n score = ( max_dist - error ) / max_dist # Score based on macro - averaged mean absolute error elif error_type == \" mmae \" : sum = 0 for i in range ( n_bins ) : samples = y == i n_samples = np . sum ( samples ) if n_samples > 0 : bin_error = mae ( prediction [ samples ], y [ samples ] ) / n_samples sum += bin_error error = sum / n_bins score = ( max_dist - error ) / max_dist else : raise ValueError ( \" error_type {} not available!' \" . format ( error_type )) if return_error : return error else : return score","title":"Raises"},{"location":"reference/fri/model/ordinal_regression/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/ordinal_regression/#ordinalregression","text":"class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"OrdinalRegression"},{"location":"reference/fri/model/ordinal_regression/#ancestors-in-mro","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/ordinal_regression/#static-methods","text":"##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" ]","title":"Static methods"},{"location":"reference/fri/model/ordinal_regression/#instance-variables","text":"``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` :","title":"Instance variables"},{"location":"reference/fri/model/ordinal_regression/#methods","text":"##### preprocessing ``` python3 def ( self , data , ** kwargs ) ``` ??? example \" View Source \" def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \" First ordinal class has index > 0. Shifting index... \" ) y = y - np . min ( y ) return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class OrdinalRegression ( ProblemType ) : @ classmethod def parameters ( cls ) : return [ \" C \" ] @ property def get_initmodel_template ( cls ) : return OrdinalRegression_SVM @ property def get_cvxproblem_template ( cls ) : return OrdinalRegression_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) if np . min ( y ) > 0 : print ( \" First ordinal class has index > 0. Shifting index... \" ) y = y - np . min ( y ) return X , y","title":"Methods"},{"location":"reference/fri/model/ordinal_regression/#ordinalregression_relevance_bound","text":"class ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"OrdinalRegression_Relevance_Bound"},{"location":"reference/fri/model/ordinal_regression/#ancestors-in-mro_1","text":"fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/ordinal_regression/#descendants","text":"fri.model.lupi_ordinal_regression.LUPI_OrdinalRegression_Relevance_Bound","title":"Descendants"},{"location":"reference/fri/model/ordinal_regression/#methods_1","text":"##### init_objective_LB ``` python3 def ( self , ** kwargs ) ``` ??? example \"View Source\" def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) ##### init_objective_UB ``` python3 def ( self , sign = None , ** kwargs ) ``` ??? example \"View Source\" def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) View Source class OrdinalRegression_Relevance_Bound ( Relevance_CVXProblem ) : def init_objective_UB ( self , sign = None , ** kwargs ) : self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ) : self . add_constraint ( cvx . abs ( self . w [ self . current_feature ] ) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ) : n_bins = len ( np . unique ( self . y )) # Upper constraints from initial model l1_w = init_model_constraints [ \" w_l1 \" ] init_loss = init_model_constraints [ \" loss \" ] C = parameters [ \" C \" ] # New Variables self . w = cvx . Variable ( shape = ( self . d ) , name = \" w \" ) # For ordinal regression we use two slack variables , we observe the slack in both directions self . slack_left = cvx . Variable ( shape = ( self . n ) , name = \" slack_left \" , nonneg = True ) self . slack_right = cvx . Variable ( shape = ( self . n ) , name = \" slack_right \" , nonneg = True ) # We have an offset for every bin boundary self . b_s = cvx . Variable ( shape = ( n_bins - 1 ) , name = \" bias \" ) # New Constraints self . loss = cvx . sum ( self . slack_left + self . slack_right ) self . weight_norm = cvx . norm ( self . w , 1 ) for i in range ( n_bins - 1 ) : indices = np . where ( self . y == i ) self . add_constraint ( self . X [ indices ] * self . w - self . slack_left [ indices ] <= self . b_s [ i ] - 1 ) for i in range ( 1 , n_bins ) : indices = np . where ( self . y == i ) self . add_constraint ( self . X [ indices ] * self . w + self . slack_right [ indices ] >= self . b_s [ i - 1 ] + 1 ) for i in range ( n_bins - 2 ) : self . add_constraint ( self . b_s [ i ] <= self . b_s [ i + 1 ] ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \" Feature Relevance \" )","title":"Methods"},{"location":"reference/fri/model/ordinal_regression/#ordinalregression_svm","text":"class ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance.","title":"OrdinalRegression_SVM"},{"location":"reference/fri/model/ordinal_regression/#ancestors-in-mro_2","text":"fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/ordinal_regression/#static-methods_1","text":"##### hyperparameter ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def hyperparameter ( cls ) : return [ \" C \" ]","title":"Static methods"},{"location":"reference/fri/model/ordinal_regression/#methods_2","text":"##### fit ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" def fit ( self , X , y , ** kwargs ) : ( n , d ) = X . shape C = self . hyperparam [ \" C \" ] self . classes_ = np . unique ( y ) original_bins = sorted ( self . classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) w = cvx . Variable ( shape = ( d ) , name = \" w \" ) # For ordinal regression we use two slack variables , we observe the slack in both directions slack_left = cvx . Variable ( shape = ( n ) , name = \" slack_left \" ) slack_right = cvx . Variable ( shape = ( n ) , name = \" slack_right \" ) # We have an offset for every bin boundary b_s = cvx . Variable ( shape = ( n_bins - 1 ) , name = \" bias \" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack_left + slack_right )) constraints = [ slack_left >= 0 , slack_right >= 0 ] # Add constraints for slack into left neighboring bins for i in range ( n_bins - 1 ) : indices = np . where ( y == get_old_bin [ i ] ) constraints . append ( X [ indices ] * w - slack_left [ indices ] <= b_s [ i ] - 1 ) # Add constraints for slack into right neighboring bins for i in range ( 1 , n_bins ) : indices = np . where ( y == get_old_bin [ i ] ) constraints . append ( X [ indices ] * w + slack_right [ indices ] >= b_s [ i - 1 ] + 1 ) # Add explicit constraint , that all bins are ascending for i in range ( n_bins - 2 ) : constraints . append ( b_s [ i ] <= b_s [ i + 1 ] ) # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b_s = b_s . value slack_left = np . asarray ( slack_left . value ) . flatten () slack_right = np . asarray ( slack_right . value ) . flatten () self . model_state = { \" w \" : w , \" b_s \" : b_s , \" slack \" : ( slack_left , slack_right ) } loss = np . sum ( slack_left + slack_right ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \" loss \" : loss , \" w_l1 \" : w_l1 } return self ##### make_scorer ``` python3 def ( self ) ``` ??? example \" View Source \" def make_scorer ( self ) : # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \" mze \" ) mae = make_scorer ( ordinal_scores , error_type = \" mae \" ) mmae = make_scorer ( ordinal_scores , error_type = \" mmae \" ) scorer = { \" mze \" : mze , \" mae \" : mae , \" mmae \" : mmae } return scorer , \" mmae \" ##### predict ``` python3 def ( self , X ) ``` ??? example \" View Source \" def predict ( self , X ) : w = self . model_state [ \" w \" ] b_s = self . model_state [ \" b_s \" ] scores = np . dot ( X , w . T ) [ np . newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] ##### score ``` python3 def ( self , X , y , error_type = ' mmae ' , return_error = False , ** kwargs ) ``` ??? example \" View Source \" def score ( self , X , y , error_type = \" mmae \" , return_error = False , ** kwargs ) : X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score View Source class OrdinalRegression_SVM ( InitModel ) : @ classmethod def hyperparameter ( cls ) : return [ \" C \" ] def fit ( self , X , y , ** kwargs ) : ( n , d ) = X . shape C = self . hyperparam [ \" C \" ] self . classes_ = np . unique ( y ) original_bins = sorted ( self . classes_ ) n_bins = len ( original_bins ) bins = np . arange ( n_bins ) get_old_bin = dict ( zip ( bins , original_bins )) w = cvx . Variable ( shape = ( d ) , name = \" w \" ) # For ordinal regression we use two slack variables , we observe the slack in both directions slack_left = cvx . Variable ( shape = ( n ) , name = \" slack_left \" ) slack_right = cvx . Variable ( shape = ( n ) , name = \" slack_right \" ) # We have an offset for every bin boundary b_s = cvx . Variable ( shape = ( n_bins - 1 ) , name = \" bias \" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack_left + slack_right )) constraints = [ slack_left >= 0 , slack_right >= 0 ] # Add constraints for slack into left neighboring bins for i in range ( n_bins - 1 ) : indices = np . where ( y == get_old_bin [ i ] ) constraints . append ( X [ indices ] * w - slack_left [ indices ] <= b_s [ i ] - 1 ) # Add constraints for slack into right neighboring bins for i in range ( 1 , n_bins ) : indices = np . where ( y == get_old_bin [ i ] ) constraints . append ( X [ indices ] * w + slack_right [ indices ] >= b_s [ i - 1 ] + 1 ) # Add explicit constraint , that all bins are ascending for i in range ( n_bins - 2 ) : constraints . append ( b_s [ i ] <= b_s [ i + 1 ] ) # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b_s = b_s . value slack_left = np . asarray ( slack_left . value ) . flatten () slack_right = np . asarray ( slack_right . value ) . flatten () self . model_state = { \" w \" : w , \" b_s \" : b_s , \" slack \" : ( slack_left , slack_right ) } loss = np . sum ( slack_left + slack_right ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \" loss \" : loss , \" w_l1 \" : w_l1 } return self def predict ( self , X ) : w = self . model_state [ \" w \" ] b_s = self . model_state [ \" b_s \" ] scores = np . dot ( X , w . T ) [ np . newaxis ] bin_thresholds = np . append ( b_s , np . inf ) # If thresholds are smaller than score the value belongs to the bigger bin # after subtracting we check for positive elements indices = np . sum ( scores . T - bin_thresholds >= 0 , - 1 ) return self . classes_ [ indices ] def score ( self , X , y , error_type = \" mmae \" , return_error = False , ** kwargs ) : X , y = check_X_y ( X , y ) prediction = self . predict ( X ) score = ordinal_scores ( y , prediction , error_type , return_error = return_error ) return score def make_scorer ( self ) : # Use multiple scores for ordinal regression mze = make_scorer ( ordinal_scores , error_type = \" mze \" ) mae = make_scorer ( ordinal_scores , error_type = \" mae \" ) mmae = make_scorer ( ordinal_scores , error_type = \" mmae \" ) scorer = { \" mze \" : mze , \" mae \" : mae , \" mmae \" : mmae } return scorer , \" mmae \"","title":"Methods"},{"location":"reference/fri/model/regression/","text":"Module fri.model.regression View Source import cvxpy as cvx import numpy as np from sklearn.utils import check_X_y from .base_cvxproblem import Relevance_CVXProblem from .base_initmodel import InitModel from .base_type import ProblemType class Regression ( ProblemType ): @classmethod def parameters ( cls ): return [ \"C\" , \"epsilon\" ] @property def get_initmodel_template ( cls ): return Regression_SVR @property def get_cvxproblem_template ( cls ): return Regression_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y class Regression_SVR ( InitModel ): @classmethod def hyperparameter ( cls ): return [ \"C\" , \"epsilon\" ] def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . hyperparam [ \"C\" ] epsilon = self . hyperparam [ \"epsilon\" ] w = cvx . Variable ( shape = ( d ), name = \"w\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) b = cvx . Variable ( name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . abs ( y - ( X * w + b )) <= epsilon + slack , slack >= 0 ] # Solve problem. solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b = b . value slack = np . asarray ( slack . value ) . flatten () self . model_state = { \"w\" : w , \"b\" : b , \"slack\" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ): w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) from sklearn.metrics import r2_score from sklearn.metrics.regression import _check_reg_targets _check_reg_targets ( y , prediction , None ) # Using weighted f1 score to have a stable score for imbalanced datasets score = r2_score ( y , prediction ) return score class Regression_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] epsilon = parameters [ \"epsilon\" ] # New Variables self . w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) self . b = cvx . Variable ( name = \"b\" ) self . slack = cvx . Variable ( shape = ( self . n ), nonneg = True , name = \"slack\" ) # New Constraints distance_from_plane = cvx . abs ( self . y - ( self . X * self . w + self . b )) self . loss = cvx . sum ( self . slack ) self . weight_norm = cvx . norm ( self . w , 1 ) self . add_constraint ( distance_from_plane <= epsilon + self . slack ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Classes Regression class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_type.ProblemType abc.ABC Static methods ##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" , \" epsilon \" ] Instance variables ``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` : Methods ##### preprocessing ``` python3 def ( self , data , ** kwargs ) ``` ??? example \" View Source \" def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class Regression ( ProblemType ) : @ classmethod def parameters ( cls ) : return [ \" C \" , \" epsilon \" ] @ property def get_initmodel_template ( cls ) : return Regression_SVR @ property def get_cvxproblem_template ( cls ) : return Regression_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y Regression_Relevance_Bound class ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC Descendants fri.model.lupi_regression.LUPI_Regression_Relevance_Bound Methods ##### init_objective_LB ``` python3 def ( self , ** kwargs ) ``` ??? example \"View Source\" def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) ##### init_objective_UB ``` python3 def ( self , sign = None , ** kwargs ) ``` ??? example \"View Source\" def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) View Source class Regression_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] epsilon = parameters [ \"epsilon\" ] # New Variables self . w = cvx . Variable ( shape =( self . d ), name = \"w\" ) self . b = cvx . Variable ( name = \"b\" ) self . slack = cvx . Variable ( shape =( self . n ), nonneg = True , name = \"slack\" ) # New Constraints distance_from_plane = cvx . abs ( self . y - ( self . X * self . w + self . b )) self . loss = cvx . sum ( self . slack ) self . weight_norm = cvx . norm ( self . w , 1 ) self . add_constraint ( distance_from_plane <= epsilon + self . slack ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" ) Regression_SVR class ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator Static methods ##### hyperparameter ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def hyperparameter ( cls ) : return [ \" C \" , \" epsilon \" ] Methods ##### fit ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" def fit ( self , X , y , ** kwargs ) : ( n , d ) = X . shape C = self . hyperparam [ \" C \" ] epsilon = self . hyperparam [ \" epsilon \" ] w = cvx . Variable ( shape = ( d ) , name = \" w \" ) slack = cvx . Variable ( shape = ( n ) , name = \" slack \" ) b = cvx . Variable ( name = \" bias \" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . abs ( y - ( X * w + b )) <= epsilon + slack , slack >= 0 ] # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b = b . value slack = np . asarray ( slack . value ) . flatten () self . model_state = { \" w \" : w , \" b \" : b , \" slack \" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \" loss \" : loss , \" w_l1 \" : w_l1 } return self ##### predict ``` python3 def ( self , X ) ``` ??? example \" View Source \" def predict ( self , X ) : w = self . model_state [ \" w \" ] b = self . model_state [ \" b \" ] y = np . dot ( X , w ) + b return y ##### score ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) from sklearn . metrics import r2_score from sklearn . metrics . regression import _check_reg_targets _check_reg_targets ( y , prediction , None ) # Using weighted f1 score to have a stable score for imbalanced datasets score = r2_score ( y , prediction ) return score View Source class Regression_SVR ( InitModel ) : @ classmethod def hyperparameter ( cls ) : return [ \" C \" , \" epsilon \" ] def fit ( self , X , y , ** kwargs ) : ( n , d ) = X . shape C = self . hyperparam [ \" C \" ] epsilon = self . hyperparam [ \" epsilon \" ] w = cvx . Variable ( shape = ( d ) , name = \" w \" ) slack = cvx . Variable ( shape = ( n ) , name = \" slack \" ) b = cvx . Variable ( name = \" bias \" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . abs ( y - ( X * w + b )) <= epsilon + slack , slack >= 0 ] # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b = b . value slack = np . asarray ( slack . value ) . flatten () self . model_state = { \" w \" : w , \" b \" : b , \" slack \" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \" loss \" : loss , \" w_l1 \" : w_l1 } return self def predict ( self , X ) : w = self . model_state [ \" w \" ] b = self . model_state [ \" b \" ] y = np . dot ( X , w ) + b return y def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) from sklearn . metrics import r2_score from sklearn . metrics . regression import _check_reg_targets _check_reg_targets ( y , prediction , None ) # Using weighted f1 score to have a stable score for imbalanced datasets score = r2_score ( y , prediction ) return score","title":"Regression"},{"location":"reference/fri/model/regression/#module-frimodelregression","text":"View Source import cvxpy as cvx import numpy as np from sklearn.utils import check_X_y from .base_cvxproblem import Relevance_CVXProblem from .base_initmodel import InitModel from .base_type import ProblemType class Regression ( ProblemType ): @classmethod def parameters ( cls ): return [ \"C\" , \"epsilon\" ] @property def get_initmodel_template ( cls ): return Regression_SVR @property def get_cvxproblem_template ( cls ): return Regression_Relevance_Bound def relax_factors ( cls ): return [ \"loss_slack\" , \"w_l1_slack\" ] def preprocessing ( self , data , ** kwargs ): X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y class Regression_SVR ( InitModel ): @classmethod def hyperparameter ( cls ): return [ \"C\" , \"epsilon\" ] def fit ( self , X , y , ** kwargs ): ( n , d ) = X . shape C = self . hyperparam [ \"C\" ] epsilon = self . hyperparam [ \"epsilon\" ] w = cvx . Variable ( shape = ( d ), name = \"w\" ) slack = cvx . Variable ( shape = ( n ), name = \"slack\" ) b = cvx . Variable ( name = \"bias\" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . abs ( y - ( X * w + b )) <= epsilon + slack , slack >= 0 ] # Solve problem. solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b = b . value slack = np . asarray ( slack . value ) . flatten () self . model_state = { \"w\" : w , \"b\" : b , \"slack\" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \"loss\" : loss , \"w_l1\" : w_l1 } return self def predict ( self , X ): w = self . model_state [ \"w\" ] b = self . model_state [ \"b\" ] y = np . dot ( X , w ) + b return y def score ( self , X , y , ** kwargs ): prediction = self . predict ( X ) from sklearn.metrics import r2_score from sklearn.metrics.regression import _check_reg_targets _check_reg_targets ( y , prediction , None ) # Using weighted f1 score to have a stable score for imbalanced datasets score = r2_score ( y , prediction ) return score class Regression_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] epsilon = parameters [ \"epsilon\" ] # New Variables self . w = cvx . Variable ( shape = ( self . d ), name = \"w\" ) self . b = cvx . Variable ( name = \"b\" ) self . slack = cvx . Variable ( shape = ( self . n ), nonneg = True , name = \"slack\" ) # New Constraints distance_from_plane = cvx . abs ( self . y - ( self . X * self . w + self . b )) self . loss = cvx . sum ( self . slack ) self . weight_norm = cvx . norm ( self . w , 1 ) self . add_constraint ( distance_from_plane <= epsilon + self . slack ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"Module fri.model.regression"},{"location":"reference/fri/model/regression/#classes","text":"","title":"Classes"},{"location":"reference/fri/model/regression/#regression","text":"class ( ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"Regression"},{"location":"reference/fri/model/regression/#ancestors-in-mro","text":"fri.model.base_type.ProblemType abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/regression/#static-methods","text":"##### parameters ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def parameters ( cls ) : return [ \" C \" , \" epsilon \" ]","title":"Static methods"},{"location":"reference/fri/model/regression/#instance-variables","text":"``` python3 get_cvxproblem_template ``` : ``` python3 get_initmodel_template ``` :","title":"Instance variables"},{"location":"reference/fri/model/regression/#methods","text":"##### preprocessing ``` python3 def ( self , data , ** kwargs ) ``` ??? example \" View Source \" def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y ##### relax_factors ``` python3 def ( cls ) ``` ??? example \" View Source \" def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] View Source class Regression ( ProblemType ) : @ classmethod def parameters ( cls ) : return [ \" C \" , \" epsilon \" ] @ property def get_initmodel_template ( cls ) : return Regression_SVR @ property def get_cvxproblem_template ( cls ) : return Regression_Relevance_Bound def relax_factors ( cls ) : return [ \" loss_slack \" , \" w_l1_slack \" ] def preprocessing ( self , data , ** kwargs ) : X , y = data # Check that X and y have correct shape X , y = check_X_y ( X , y ) return X , y","title":"Methods"},{"location":"reference/fri/model/regression/#regression_relevance_bound","text":"class ( current_feature : int , data : tuple , hyperparameters , best_model_constraints , preset_model = None , best_model_state = None , probeID =- 1 , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"Regression_Relevance_Bound"},{"location":"reference/fri/model/regression/#ancestors-in-mro_1","text":"fri.model.base_cvxproblem.Relevance_CVXProblem abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/regression/#descendants","text":"fri.model.lupi_regression.LUPI_Regression_Relevance_Bound","title":"Descendants"},{"location":"reference/fri/model/regression/#methods_1","text":"##### init_objective_LB ``` python3 def ( self , ** kwargs ) ``` ??? example \"View Source\" def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) ##### init_objective_UB ``` python3 def ( self , sign = None , ** kwargs ) ``` ??? example \"View Source\" def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) View Source class Regression_Relevance_Bound ( Relevance_CVXProblem ): def init_objective_UB ( self , sign = None , ** kwargs ): self . add_constraint ( self . feature_relevance <= sign * self . w [ self . current_feature ] ) self . _objective = cvx . Maximize ( self . feature_relevance ) def init_objective_LB ( self , ** kwargs ): self . add_constraint ( cvx . abs ( self . w [ self . current_feature ]) <= self . feature_relevance ) self . _objective = cvx . Minimize ( self . feature_relevance ) def _init_constraints ( self , parameters , init_model_constraints ): # Upper constraints from initial model l1_w = init_model_constraints [ \"w_l1\" ] init_loss = init_model_constraints [ \"loss\" ] C = parameters [ \"C\" ] epsilon = parameters [ \"epsilon\" ] # New Variables self . w = cvx . Variable ( shape =( self . d ), name = \"w\" ) self . b = cvx . Variable ( name = \"b\" ) self . slack = cvx . Variable ( shape =( self . n ), nonneg = True , name = \"slack\" ) # New Constraints distance_from_plane = cvx . abs ( self . y - ( self . X * self . w + self . b )) self . loss = cvx . sum ( self . slack ) self . weight_norm = cvx . norm ( self . w , 1 ) self . add_constraint ( distance_from_plane <= epsilon + self . slack ) self . add_constraint ( self . weight_norm <= l1_w ) self . add_constraint ( C * self . loss <= C * init_loss ) self . feature_relevance = cvx . Variable ( nonneg = True , name = \"Feature Relevance\" )","title":"Methods"},{"location":"reference/fri/model/regression/#regression_svr","text":"class ( ** parameters ) Helper class that provides a standard way to create an ABC using inheritance.","title":"Regression_SVR"},{"location":"reference/fri/model/regression/#ancestors-in-mro_2","text":"fri.model.base_initmodel.InitModel abc.ABC sklearn.base.BaseEstimator","title":"Ancestors (in MRO)"},{"location":"reference/fri/model/regression/#static-methods_1","text":"##### hyperparameter ``` python3 def ( ) ``` ??? example \" View Source \" @ classmethod def hyperparameter ( cls ) : return [ \" C \" , \" epsilon \" ]","title":"Static methods"},{"location":"reference/fri/model/regression/#methods_2","text":"##### fit ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" def fit ( self , X , y , ** kwargs ) : ( n , d ) = X . shape C = self . hyperparam [ \" C \" ] epsilon = self . hyperparam [ \" epsilon \" ] w = cvx . Variable ( shape = ( d ) , name = \" w \" ) slack = cvx . Variable ( shape = ( n ) , name = \" slack \" ) b = cvx . Variable ( name = \" bias \" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . abs ( y - ( X * w + b )) <= epsilon + slack , slack >= 0 ] # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b = b . value slack = np . asarray ( slack . value ) . flatten () self . model_state = { \" w \" : w , \" b \" : b , \" slack \" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \" loss \" : loss , \" w_l1 \" : w_l1 } return self ##### predict ``` python3 def ( self , X ) ``` ??? example \" View Source \" def predict ( self , X ) : w = self . model_state [ \" w \" ] b = self . model_state [ \" b \" ] y = np . dot ( X , w ) + b return y ##### score ``` python3 def ( self , X , y , ** kwargs ) ``` ??? example \" View Source \" def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) from sklearn . metrics import r2_score from sklearn . metrics . regression import _check_reg_targets _check_reg_targets ( y , prediction , None ) # Using weighted f1 score to have a stable score for imbalanced datasets score = r2_score ( y , prediction ) return score View Source class Regression_SVR ( InitModel ) : @ classmethod def hyperparameter ( cls ) : return [ \" C \" , \" epsilon \" ] def fit ( self , X , y , ** kwargs ) : ( n , d ) = X . shape C = self . hyperparam [ \" C \" ] epsilon = self . hyperparam [ \" epsilon \" ] w = cvx . Variable ( shape = ( d ) , name = \" w \" ) slack = cvx . Variable ( shape = ( n ) , name = \" slack \" ) b = cvx . Variable ( name = \" bias \" ) objective = cvx . Minimize ( cvx . norm ( w , 1 ) + C * cvx . sum ( slack )) constraints = [ cvx . abs ( y - ( X * w + b )) <= epsilon + slack , slack >= 0 ] # Solve problem . solver_params = self . solver_params problem = cvx . Problem ( objective , constraints ) problem . solve ( ** solver_params ) w = w . value b = b . value slack = np . asarray ( slack . value ) . flatten () self . model_state = { \" w \" : w , \" b \" : b , \" slack \" : slack } loss = np . sum ( slack ) w_l1 = np . linalg . norm ( w , ord = 1 ) self . constraints = { \" loss \" : loss , \" w_l1 \" : w_l1 } return self def predict ( self , X ) : w = self . model_state [ \" w \" ] b = self . model_state [ \" b \" ] y = np . dot ( X , w ) + b return y def score ( self , X , y , ** kwargs ) : prediction = self . predict ( X ) from sklearn . metrics import r2_score from sklearn . metrics . regression import _check_reg_targets _check_reg_targets ( y , prediction , None ) # Using weighted f1 score to have a stable score for imbalanced datasets score = r2_score ( y , prediction ) return score","title":"Methods"},{"location":"reference/fri/toydata/","text":"Module fri.toydata View Source import numpy as np from fri import ProblemName from .gen_data import genRegressionData , genClassificationData , genOrdinalRegressionData from .gen_lupi import genLupiData __all__ = [ \"genRegressionData\" , \"genClassificationData\" , \"genOrdinalRegressionData\" , \"genLupiData\" , ] def quick_generate ( problem : object , ** kwargs ) -> [ np . ndarray , np . ndarray ]: \"\"\" Method to wrap individual data generation functions. Allows passing `problem` as a string such as \"classification\" or `ProblemName` object of the corresponding type. For possible kwargs see `genClassificationData' or `genLupiData`. Parameters ---------- problem : str or `ProblemName` Type of data to generate (e.g. \"classification\" or `ProblemName.CLASSIFICATION` kwargs : **dict arguments to pass to the generation functions Returns ------- Tuple[numpy.ndarray, numpy.ndarray] \"\"\" if problem is \"regression\" or problem is ProblemName . REGRESSION : gen = genRegressionData elif problem is \"classification\" or problem is ProblemName . CLASSIFICATION : gen = genClassificationData elif problem is \"ordreg\" or problem is ProblemName . ORDINALREGRESSION : gen = genOrdinalRegressionData elif problem is \"lupi_regression\" or problem is ProblemName . LUPI_REGRESSION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_REGRESSION elif problem is \"lupi_classification\" or problem is ProblemName . LUPI_CLASSIFICATION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_CLASSIFICATION elif problem is \"lupi_ordreg\" or problem is ProblemName . LUPI_ORDREGRESSION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_ORDREGRESSION else : raise ValueError ( \"Unknown problem type. Try 'regression', 'classification' or 'ordreg' and/or add 'lupi_' prefix\" ) return gen ( ** kwargs ) Sub-modules fri.toydata.gen_data fri.toydata.gen_lupi Functions genClassificationData def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None ) Generate synthetic classification data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation. Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes. Raises ValueError Description ValueError Wrong parameters for specified amonut of features/samples. Examples X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None X.shape (200, 2) y.shape (200,) View Source def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 1 , flip_y : float = 0 , random_state : object = None , partition = None , ) : \"\"\" Generate synthetic classification data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float Added gaussian noise to data . Parameter scales Std of normal distribution . flip_y : float , optional Ratio of samples randomly switched to wrong class . random_state : object , optional Randomstate object used for generation . Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output classes . Raises ------ ValueError Description ValueError Wrong parameters for specified amonut of features / samples . Examples --------- >>> X , y = genClassificationData ( n_samples = 200 ) Generating dataset with d = 2 , n = 200 , strongly = 1 , weakly = 0 , partition of weakly = None >>> X . shape ( 200 , 2 ) >>> y . shape ( 200 , ) \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) X = np . zeros (( n_samples , n_features )) # Find partitions which def \u00ed ne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 X_informative , Y = generate_binary_classification_problem ( n_samples , n_strel + part_size , random_state ) X = _fillVariableSpace ( X_informative , random_state , n_samples = n_samples , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) # Add target noise if flip_y > 0 : n_flip = int ( flip_y * n_samples ) Y [ random_state . choice ( n_samples , n_flip ) ] *= - 1 # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ) , scale = noise ) return X , Y genLupiData def ( problemName : fri . ProblemName , n_samples : int = 100 , random_state : object = None , noise : float = 0.1 , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel_groups : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 ) Generate Lupi Data for Classification, Regression and Ordinal Regression Problems Parameters problemName : ProblemName Defines the type of y-values of the problem. Example ProblemName.CLASSIFICATION . n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. noise : float, optional Noise of the created samples around ground truth. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel_groups : int, optional Number of 2 feature groups which are part of redundant subsets (weakly relevant) n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated. Returns X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] View Source def genLupiData ( problemName : ProblemName , n_samples : int = 100 , random_state : object = None , noise : float = 0 . 1 , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel_groups : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0 . 0 , ) : \"\"\" Generate Lupi Data for Classification , Regression and Ordinal Regression Problems Parameters ---------- problemName : ProblemName Defines the type of y - values of the problem . Example ` ProblemName . CLASSIFICATION `. n_samples : int , optional Number of samples random_state : object , optional Randomstate object used for generation . noise : float , optional Noise of the created samples around ground truth . n_ordinal_bins : int , optional Number of bins in which the regressional target variable is split to form the ordinal classes , Only has an effect if problemType == ' ordinalRegression ' n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_weakrel_groups : int , optional Number of 2 feature groups which are part of redundant subsets ( weakly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . n_irrel : int , optional Number of features which are irrelevant to the underlying model label_noise : float , optional Percentage of labels which get permutated . Returns ------- X : array of shape [ n_samples , ( n_strel + n_weakrel + n_repeated + n_irrel ) ] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [ n_samples ] The generated target values In case of problemType == ' classification ' : values are in [ 0 , 1 ] In case of problemType == ' regression ' : values are continious In case of problemType == ' ordinalRegression ' : values are in [ 0 , n_ordinal_bins ] \"\"\" random_state = check_random_state ( random_state ) n_informative = n_strel + n_weakrel_groups w = random_state . normal ( size = n_informative ) X_informative = random_state . normal ( size = ( n_samples , n_informative )) X_priv_strel = X_informative [:, : n_strel ] X_priv_weakrel = np . zeros ( [ n_samples , n_weakrel_groups * 2 ] ) idx = 0 for i in range ( n_weakrel_groups ) : X_priv_weakrel [:, idx : idx + 2 ] = np . tile ( X_informative [:, n_strel + i ], ( 2 , 1 ) ) . T + random_state . normal ( loc = 0 , scale = np . std ( X_informative [:, n_strel + i ] ) , size = 2 ) idx += 2 X_priv_repeated = _genRepeatedFeatures ( n_repeated , np . hstack ( [ X_priv_strel , X_priv_weakrel ] ) , random_state ) X_priv = np . hstack ( [ X_priv_strel , X_priv_weakrel , X_priv_repeated ] ) e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ) , scale = noise * np . std ( X_priv ) ) X = X_priv + e scores = np . dot ( X_informative , w ) if ( problemName == \" classification \" or problemName == ProblemName . LUPI_CLASSIFICATION ) : y = scores > 0 elif problemName == \" regression \" or problemName == ProblemName . LUPI_REGRESSION : y = scores elif ( problemName == \" ordinalRegression \" or problemName == ProblemName . LUPI_ORDREGRESSION ) : step = 1 / ( n_ordinal_bins ) quantiles = [ i * step for i in range ( 1 , n_ordinal_bins ) ] bs = np . quantile ( scores , quantiles ) bs = np . append ( bs , np . inf ) scores = scores [:, np . newaxis ] y = np . sum ( scores - bs >= 0 , - 1 ) if n_irrel > 0 : X = np . hstack ( [ X , random_state . normal ( size = ( n_samples , n_irrel )) ] ) X_priv = np . hstack ( [ X_priv , random_state . normal ( size = ( n_samples , n_irrel )) ] ) if label_noise > 0 : sample = random_state . choice ( len ( y ) , int ( len ( y ) * label_noise )) y [ sample ] = random_state . permutation ( y [ sample ] ) return ( X , X_priv , y . squeeze ()) genOrdinalRegressionData def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 ) Generate ordinal regression data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ValueError Wrong parameters for specified amonut of features/samples. View Source def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 0 , random_state : object = None , partition = None , n_target_bins : int = 3 , ) : \"\"\" Generate ordinal regression data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float , optional Noise of the created samples around ground truth . random_state : object , optional Randomstate object used for generation . n_target_bins : int , optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output values ( target ) . Raises ------ ValueError Wrong parameters for specified amonut of features / samples . \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) if not n_target_bins > 1 : raise ValueError ( \" At least 2 target bins needed \" ) # Use normal regression data as starting point X_regression , Y_regression = genRegressionData ( n_samples = int ( n_samples ) , n_features = int ( n_features ) , n_redundant = int ( n_redundant ) , n_strel = int ( n_strel ) , n_repeated = int ( n_repeated ) , noise = 0 , random_state = random_state , partition = partition , ) bin_size = int ( np . floor ( n_samples / n_target_bins )) rest = int ( n_samples - ( bin_size * n_target_bins )) # Sort the target values and rearange the data accordingly sort_indices = np . argsort ( Y_regression ) X = X_regression [ sort_indices ] Y = Y_regression [ sort_indices ] # Assign ordinal classes as target values for i in range ( n_target_bins ) : Y [ bin_size * i : bin_size * ( i + 1 ) ] = i # Put non divisable rest into last bin if rest > 0 : Y [ - rest :] = n_target_bins - 1 X , Y = shuffle ( X , Y , random_state = random_state ) # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ) , scale = noise ) return X , Y genRegressionData def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None ) -> object Generate synthetic regression data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ValueError Wrong parameters for specified amonut of features/samples. View Source def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 0 , random_state : object = None , partition = None , ) -> object : \"\"\" Generate synthetic regression data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float , optional Noise of the created samples around ground truth . random_state : object , optional Randomstate object used for generation . Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output values ( target ) . Raises ------ ValueError Wrong parameters for specified amonut of features / samples . \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) # Find partitions which def \u00ed ne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 n_informative = n_strel + part_size X = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [: n_informative , :] = 0 . 3 bias = 0 y = np . dot ( X , ground_truth ) + bias # Add noise if noise > 0 . 0 : y += random_state . normal ( scale = noise , size = y . shape ) X = _fillVariableSpace ( X , random_state , n_samples = n_samples , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , noise = noise , partition = partition , ) y = np . squeeze ( y ) return X , y","title":"Index"},{"location":"reference/fri/toydata/#module-fritoydata","text":"View Source import numpy as np from fri import ProblemName from .gen_data import genRegressionData , genClassificationData , genOrdinalRegressionData from .gen_lupi import genLupiData __all__ = [ \"genRegressionData\" , \"genClassificationData\" , \"genOrdinalRegressionData\" , \"genLupiData\" , ] def quick_generate ( problem : object , ** kwargs ) -> [ np . ndarray , np . ndarray ]: \"\"\" Method to wrap individual data generation functions. Allows passing `problem` as a string such as \"classification\" or `ProblemName` object of the corresponding type. For possible kwargs see `genClassificationData' or `genLupiData`. Parameters ---------- problem : str or `ProblemName` Type of data to generate (e.g. \"classification\" or `ProblemName.CLASSIFICATION` kwargs : **dict arguments to pass to the generation functions Returns ------- Tuple[numpy.ndarray, numpy.ndarray] \"\"\" if problem is \"regression\" or problem is ProblemName . REGRESSION : gen = genRegressionData elif problem is \"classification\" or problem is ProblemName . CLASSIFICATION : gen = genClassificationData elif problem is \"ordreg\" or problem is ProblemName . ORDINALREGRESSION : gen = genOrdinalRegressionData elif problem is \"lupi_regression\" or problem is ProblemName . LUPI_REGRESSION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_REGRESSION elif problem is \"lupi_classification\" or problem is ProblemName . LUPI_CLASSIFICATION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_CLASSIFICATION elif problem is \"lupi_ordreg\" or problem is ProblemName . LUPI_ORDREGRESSION : gen = genLupiData kwargs [ \"problemName\" ] = ProblemName . LUPI_ORDREGRESSION else : raise ValueError ( \"Unknown problem type. Try 'regression', 'classification' or 'ordreg' and/or add 'lupi_' prefix\" ) return gen ( ** kwargs )","title":"Module fri.toydata"},{"location":"reference/fri/toydata/#sub-modules","text":"fri.toydata.gen_data fri.toydata.gen_lupi","title":"Sub-modules"},{"location":"reference/fri/toydata/#functions","text":"","title":"Functions"},{"location":"reference/fri/toydata/#genclassificationdata","text":"def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None ) Generate synthetic classification data","title":"genClassificationData"},{"location":"reference/fri/toydata/#parameters","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation.","title":"Parameters"},{"location":"reference/fri/toydata/#returns","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes.","title":"Returns"},{"location":"reference/fri/toydata/#raises","text":"ValueError Description ValueError Wrong parameters for specified amonut of features/samples.","title":"Raises"},{"location":"reference/fri/toydata/#examples","text":"X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None X.shape (200, 2) y.shape (200,) View Source def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 1 , flip_y : float = 0 , random_state : object = None , partition = None , ) : \"\"\" Generate synthetic classification data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float Added gaussian noise to data . Parameter scales Std of normal distribution . flip_y : float , optional Ratio of samples randomly switched to wrong class . random_state : object , optional Randomstate object used for generation . Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output classes . Raises ------ ValueError Description ValueError Wrong parameters for specified amonut of features / samples . Examples --------- >>> X , y = genClassificationData ( n_samples = 200 ) Generating dataset with d = 2 , n = 200 , strongly = 1 , weakly = 0 , partition of weakly = None >>> X . shape ( 200 , 2 ) >>> y . shape ( 200 , ) \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) X = np . zeros (( n_samples , n_features )) # Find partitions which def \u00ed ne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 X_informative , Y = generate_binary_classification_problem ( n_samples , n_strel + part_size , random_state ) X = _fillVariableSpace ( X_informative , random_state , n_samples = n_samples , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) # Add target noise if flip_y > 0 : n_flip = int ( flip_y * n_samples ) Y [ random_state . choice ( n_samples , n_flip ) ] *= - 1 # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ) , scale = noise ) return X , Y","title":"Examples"},{"location":"reference/fri/toydata/#genlupidata","text":"def ( problemName : fri . ProblemName , n_samples : int = 100 , random_state : object = None , noise : float = 0.1 , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel_groups : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 ) Generate Lupi Data for Classification, Regression and Ordinal Regression Problems","title":"genLupiData"},{"location":"reference/fri/toydata/#parameters_1","text":"problemName : ProblemName Defines the type of y-values of the problem. Example ProblemName.CLASSIFICATION . n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. noise : float, optional Noise of the created samples around ground truth. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel_groups : int, optional Number of 2 feature groups which are part of redundant subsets (weakly relevant) n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated.","title":"Parameters"},{"location":"reference/fri/toydata/#returns_1","text":"X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] View Source def genLupiData ( problemName : ProblemName , n_samples : int = 100 , random_state : object = None , noise : float = 0 . 1 , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel_groups : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0 . 0 , ) : \"\"\" Generate Lupi Data for Classification , Regression and Ordinal Regression Problems Parameters ---------- problemName : ProblemName Defines the type of y - values of the problem . Example ` ProblemName . CLASSIFICATION `. n_samples : int , optional Number of samples random_state : object , optional Randomstate object used for generation . noise : float , optional Noise of the created samples around ground truth . n_ordinal_bins : int , optional Number of bins in which the regressional target variable is split to form the ordinal classes , Only has an effect if problemType == ' ordinalRegression ' n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_weakrel_groups : int , optional Number of 2 feature groups which are part of redundant subsets ( weakly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . n_irrel : int , optional Number of features which are irrelevant to the underlying model label_noise : float , optional Percentage of labels which get permutated . Returns ------- X : array of shape [ n_samples , ( n_strel + n_weakrel + n_repeated + n_irrel ) ] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [ n_samples ] The generated target values In case of problemType == ' classification ' : values are in [ 0 , 1 ] In case of problemType == ' regression ' : values are continious In case of problemType == ' ordinalRegression ' : values are in [ 0 , n_ordinal_bins ] \"\"\" random_state = check_random_state ( random_state ) n_informative = n_strel + n_weakrel_groups w = random_state . normal ( size = n_informative ) X_informative = random_state . normal ( size = ( n_samples , n_informative )) X_priv_strel = X_informative [:, : n_strel ] X_priv_weakrel = np . zeros ( [ n_samples , n_weakrel_groups * 2 ] ) idx = 0 for i in range ( n_weakrel_groups ) : X_priv_weakrel [:, idx : idx + 2 ] = np . tile ( X_informative [:, n_strel + i ], ( 2 , 1 ) ) . T + random_state . normal ( loc = 0 , scale = np . std ( X_informative [:, n_strel + i ] ) , size = 2 ) idx += 2 X_priv_repeated = _genRepeatedFeatures ( n_repeated , np . hstack ( [ X_priv_strel , X_priv_weakrel ] ) , random_state ) X_priv = np . hstack ( [ X_priv_strel , X_priv_weakrel , X_priv_repeated ] ) e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ) , scale = noise * np . std ( X_priv ) ) X = X_priv + e scores = np . dot ( X_informative , w ) if ( problemName == \" classification \" or problemName == ProblemName . LUPI_CLASSIFICATION ) : y = scores > 0 elif problemName == \" regression \" or problemName == ProblemName . LUPI_REGRESSION : y = scores elif ( problemName == \" ordinalRegression \" or problemName == ProblemName . LUPI_ORDREGRESSION ) : step = 1 / ( n_ordinal_bins ) quantiles = [ i * step for i in range ( 1 , n_ordinal_bins ) ] bs = np . quantile ( scores , quantiles ) bs = np . append ( bs , np . inf ) scores = scores [:, np . newaxis ] y = np . sum ( scores - bs >= 0 , - 1 ) if n_irrel > 0 : X = np . hstack ( [ X , random_state . normal ( size = ( n_samples , n_irrel )) ] ) X_priv = np . hstack ( [ X_priv , random_state . normal ( size = ( n_samples , n_irrel )) ] ) if label_noise > 0 : sample = random_state . choice ( len ( y ) , int ( len ( y ) * label_noise )) y [ sample ] = random_state . permutation ( y [ sample ] ) return ( X , X_priv , y . squeeze ())","title":"Returns"},{"location":"reference/fri/toydata/#genordinalregressiondata","text":"def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 ) Generate ordinal regression data","title":"genOrdinalRegressionData"},{"location":"reference/fri/toydata/#parameters_2","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes","title":"Parameters"},{"location":"reference/fri/toydata/#returns_2","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target).","title":"Returns"},{"location":"reference/fri/toydata/#raises_1","text":"ValueError Wrong parameters for specified amonut of features/samples. View Source def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 0 , random_state : object = None , partition = None , n_target_bins : int = 3 , ) : \"\"\" Generate ordinal regression data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float , optional Noise of the created samples around ground truth . random_state : object , optional Randomstate object used for generation . n_target_bins : int , optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output values ( target ) . Raises ------ ValueError Wrong parameters for specified amonut of features / samples . \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) if not n_target_bins > 1 : raise ValueError ( \" At least 2 target bins needed \" ) # Use normal regression data as starting point X_regression , Y_regression = genRegressionData ( n_samples = int ( n_samples ) , n_features = int ( n_features ) , n_redundant = int ( n_redundant ) , n_strel = int ( n_strel ) , n_repeated = int ( n_repeated ) , noise = 0 , random_state = random_state , partition = partition , ) bin_size = int ( np . floor ( n_samples / n_target_bins )) rest = int ( n_samples - ( bin_size * n_target_bins )) # Sort the target values and rearange the data accordingly sort_indices = np . argsort ( Y_regression ) X = X_regression [ sort_indices ] Y = Y_regression [ sort_indices ] # Assign ordinal classes as target values for i in range ( n_target_bins ) : Y [ bin_size * i : bin_size * ( i + 1 ) ] = i # Put non divisable rest into last bin if rest > 0 : Y [ - rest :] = n_target_bins - 1 X , Y = shuffle ( X , Y , random_state = random_state ) # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ) , scale = noise ) return X , Y","title":"Raises"},{"location":"reference/fri/toydata/#genregressiondata","text":"def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None ) -> object Generate synthetic regression data","title":"genRegressionData"},{"location":"reference/fri/toydata/#parameters_3","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation.","title":"Parameters"},{"location":"reference/fri/toydata/#returns_3","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target).","title":"Returns"},{"location":"reference/fri/toydata/#raises_2","text":"ValueError Wrong parameters for specified amonut of features/samples. View Source def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 0 , random_state : object = None , partition = None , ) -> object : \"\"\" Generate synthetic regression data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float , optional Noise of the created samples around ground truth . random_state : object , optional Randomstate object used for generation . Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output values ( target ) . Raises ------ ValueError Wrong parameters for specified amonut of features / samples . \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) # Find partitions which def \u00ed ne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 n_informative = n_strel + part_size X = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [: n_informative , :] = 0 . 3 bias = 0 y = np . dot ( X , ground_truth ) + bias # Add noise if noise > 0 . 0 : y += random_state . normal ( scale = noise , size = y . shape ) X = _fillVariableSpace ( X , random_state , n_samples = n_samples , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , noise = noise , partition = partition , ) y = np . squeeze ( y ) return X , y","title":"Raises"},{"location":"reference/fri/toydata/gen_data/","text":"Module fri.toydata.gen_data View Source import numpy as np from numpy.random import RandomState from sklearn.utils import check_random_state from sklearn.utils import shuffle def _combFeat ( n , size , strRelFeat , randomstate ): # Split each strongly relevant feature into linear combination of it weakFeats = np . tile ( strRelFeat , ( size , 1 )) . T weakFeats = randomstate . normal ( loc = 0 , scale = 1 , size = size ) + weakFeats return weakFeats def _dummyFeat ( n , randomstate ): return randomstate . randn ( n ) def _repeatFeat ( feats , i , randomstate ): i_pick = randomstate . choice ( i ) return feats [:, i_pick ] def _checkParam ( n_samples : int = 100 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , n_features = 1 , flip_y : float = 0 , noise : float = 1 , partition = None , ** kwargs ): if not 1 < n_samples : raise ValueError ( \"We need at least 2 samples.\" ) if not 0 < n_features : raise ValueError ( \"We need at least one feature.\" ) if not 0 <= flip_y < 1 : raise ValueError ( \"Flip percentage has to be between 0 and 1.\" ) if not n_redundant + n_repeated + n_strel <= n_features : raise ValueError ( \"Inconsistent number of features\" ) if n_strel + n_redundant < 1 : raise ValueError ( \"No informative features.\" ) if n_strel == 0 and n_redundant < 2 : raise ValueError ( \"We need more than 1 redundant feature.\" ) if partition is not None : if sum ( partition ) != n_redundant : raise ValueError ( \"Sum of partition values should yield number of redundant features.\" ) if 0 in partition or 1 in partition : raise ValueError ( \"Subset defined in Partition needs at least 2 features. 0 and 1 is not allowed.\" ) def _fillVariableSpace ( X_informative , random_state : RandomState , n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 1 , partition = None , ** kwargs ): if partition is not None : assert n_redundant == np . sum ( partition ) X = np . zeros (( int ( n_samples ), int ( n_features ))) X [:, : n_strel ] = X_informative [:, : n_strel ] holdout = X_informative [:, n_strel :] i = n_strel pi = 0 for x in range ( len ( holdout . T )): size = partition [ pi ] X [:, i : i + size ] = _combFeat ( n_samples , size , holdout [:, x ], random_state ) i += size pi += 1 for x in range ( n_repeated ): X [:, i ] = _repeatFeat ( X [:, : i ], i , random_state ) i += 1 for x in range ( n_features - i ): X [:, i ] = _dummyFeat ( n_samples , random_state ) i += 1 return X def generate_binary_classification_problem ( n_samples : int , features : int , random_state : RandomState = None , data_range = 1 ): \"\"\" Generate data uniformly distributed in a square and perfectly separated by the hyperplane given by normal_vector and b. Keyword arguments: n_samples -- number of samples required (default 100) n_features -- number of features required normal_vector -- the normal vector of the separating hyperplane data_range -- data is distributed between -data_range and data_range (default 10) \"\"\" random_state = check_random_state ( random_state ) data = random_state . normal ( size = ( n_samples , features ), scale = data_range ) labels = np . sum ( data , 1 ) > 0 labels = labels . astype ( int ) labels [ labels == 0 ] = - 1 return data , labels def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None , ): \"\"\"Generate synthetic classification data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes. Raises ------ ValueError Description ValueError Wrong parameters for specified amonut of features/samples. Examples --------- >>> X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None >>> X.shape (200, 2) >>> y.shape (200,) \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) X = np . zeros (( n_samples , n_features )) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 X_informative , Y = generate_binary_classification_problem ( n_samples , n_strel + part_size , random_state ) X = _fillVariableSpace ( X_informative , random_state , n_samples = n_samples , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) # Add target noise if flip_y > 0 : n_flip = int ( flip_y * n_samples ) Y [ random_state . choice ( n_samples , n_flip )] *= - 1 # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise ) return X , Y def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , ) -> object : \"\"\"Generate synthetic regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 n_informative = n_strel + part_size X = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [: n_informative , :] = 0.3 bias = 0 y = np . dot ( X , ground_truth ) + bias # Add noise if noise > 0.0 : y += random_state . normal ( scale = noise , size = y . shape ) X = _fillVariableSpace ( X , random_state , n_samples = n_samples , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , noise = noise , partition = partition , ) y = np . squeeze ( y ) return X , y def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 , ): \"\"\" Generate ordinal regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) if not n_target_bins > 1 : raise ValueError ( \"At least 2 target bins needed\" ) # Use normal regression data as starting point X_regression , Y_regression = genRegressionData ( n_samples = int ( n_samples ), n_features = int ( n_features ), n_redundant = int ( n_redundant ), n_strel = int ( n_strel ), n_repeated = int ( n_repeated ), noise = 0 , random_state = random_state , partition = partition , ) bin_size = int ( np . floor ( n_samples / n_target_bins )) rest = int ( n_samples - ( bin_size * n_target_bins )) # Sort the target values and rearange the data accordingly sort_indices = np . argsort ( Y_regression ) X = X_regression [ sort_indices ] Y = Y_regression [ sort_indices ] # Assign ordinal classes as target values for i in range ( n_target_bins ): Y [ bin_size * i : bin_size * ( i + 1 )] = i # Put non divisable rest into last bin if rest > 0 : Y [ - rest :] = n_target_bins - 1 X , Y = shuffle ( X , Y , random_state = random_state ) # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise ) return X , Y Functions genClassificationData def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None ) Generate synthetic classification data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation. Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes. Raises ValueError Description ValueError Wrong parameters for specified amonut of features/samples. Examples X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None X.shape (200, 2) y.shape (200,) View Source def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 1 , flip_y : float = 0 , random_state : object = None , partition = None , ) : \"\"\" Generate synthetic classification data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float Added gaussian noise to data . Parameter scales Std of normal distribution . flip_y : float , optional Ratio of samples randomly switched to wrong class . random_state : object , optional Randomstate object used for generation . Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output classes . Raises ------ ValueError Description ValueError Wrong parameters for specified amonut of features / samples . Examples --------- >>> X , y = genClassificationData ( n_samples = 200 ) Generating dataset with d = 2 , n = 200 , strongly = 1 , weakly = 0 , partition of weakly = None >>> X . shape ( 200 , 2 ) >>> y . shape ( 200 , ) \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) X = np . zeros (( n_samples , n_features )) # Find partitions which def \u00ed ne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 X_informative , Y = generate_binary_classification_problem ( n_samples , n_strel + part_size , random_state ) X = _fillVariableSpace ( X_informative , random_state , n_samples = n_samples , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) # Add target noise if flip_y > 0 : n_flip = int ( flip_y * n_samples ) Y [ random_state . choice ( n_samples , n_flip ) ] *= - 1 # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ) , scale = noise ) return X , Y genOrdinalRegressionData def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 ) Generate ordinal regression data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ValueError Wrong parameters for specified amonut of features/samples. View Source def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 0 , random_state : object = None , partition = None , n_target_bins : int = 3 , ) : \"\"\" Generate ordinal regression data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float , optional Noise of the created samples around ground truth . random_state : object , optional Randomstate object used for generation . n_target_bins : int , optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output values ( target ) . Raises ------ ValueError Wrong parameters for specified amonut of features / samples . \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) if not n_target_bins > 1 : raise ValueError ( \" At least 2 target bins needed \" ) # Use normal regression data as starting point X_regression , Y_regression = genRegressionData ( n_samples = int ( n_samples ) , n_features = int ( n_features ) , n_redundant = int ( n_redundant ) , n_strel = int ( n_strel ) , n_repeated = int ( n_repeated ) , noise = 0 , random_state = random_state , partition = partition , ) bin_size = int ( np . floor ( n_samples / n_target_bins )) rest = int ( n_samples - ( bin_size * n_target_bins )) # Sort the target values and rearange the data accordingly sort_indices = np . argsort ( Y_regression ) X = X_regression [ sort_indices ] Y = Y_regression [ sort_indices ] # Assign ordinal classes as target values for i in range ( n_target_bins ) : Y [ bin_size * i : bin_size * ( i + 1 ) ] = i # Put non divisable rest into last bin if rest > 0 : Y [ - rest :] = n_target_bins - 1 X , Y = shuffle ( X , Y , random_state = random_state ) # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ) , scale = noise ) return X , Y genRegressionData def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None ) -> object Generate synthetic regression data Parameters n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. Returns X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ValueError Wrong parameters for specified amonut of features/samples. View Source def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 0 , random_state : object = None , partition = None , ) -> object : \"\"\" Generate synthetic regression data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float , optional Noise of the created samples around ground truth . random_state : object , optional Randomstate object used for generation . Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output values ( target ) . Raises ------ ValueError Wrong parameters for specified amonut of features / samples . \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) # Find partitions which def \u00ed ne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 n_informative = n_strel + part_size X = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [: n_informative , :] = 0 . 3 bias = 0 y = np . dot ( X , ground_truth ) + bias # Add noise if noise > 0 . 0 : y += random_state . normal ( scale = noise , size = y . shape ) X = _fillVariableSpace ( X , random_state , n_samples = n_samples , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , noise = noise , partition = partition , ) y = np . squeeze ( y ) return X , y generate_binary_classification_problem def ( n_samples : int , features : int , random_state : numpy . random . mtrand . RandomState = None , data_range = 1 ) Generate data uniformly distributed in a square and perfectly separated by the hyperplane given by normal_vector and b. Keyword arguments: n_samples -- number of samples required (default 100) n_features -- number of features required normal_vector -- the normal vector of the separating hyperplane data_range -- data is distributed between -data_range and data_range (default 10) View Source def generate_binary_classification_problem ( n_samples : int , features : int , random_state : RandomState = None , data_range = 1 ) : \"\"\" Generate data uniformly distributed in a square and perfectly separated by the hyperplane given by normal_vector and b. Keyword arguments : n_samples -- number of samples required ( default 100 ) n_features -- number of features required normal_vector -- the normal vector of the separating hyperplane data_range -- data is distributed between - data_range and data_range ( default 10 ) \"\"\" random_state = check_random_state ( random_state ) data = random_state . normal ( size = ( n_samples , features ) , scale = data_range ) labels = np . sum ( data , 1 ) > 0 labels = labels . astype ( int ) labels [ labels == 0 ] = - 1 return data , labels","title":"Gen Data"},{"location":"reference/fri/toydata/gen_data/#module-fritoydatagen_data","text":"View Source import numpy as np from numpy.random import RandomState from sklearn.utils import check_random_state from sklearn.utils import shuffle def _combFeat ( n , size , strRelFeat , randomstate ): # Split each strongly relevant feature into linear combination of it weakFeats = np . tile ( strRelFeat , ( size , 1 )) . T weakFeats = randomstate . normal ( loc = 0 , scale = 1 , size = size ) + weakFeats return weakFeats def _dummyFeat ( n , randomstate ): return randomstate . randn ( n ) def _repeatFeat ( feats , i , randomstate ): i_pick = randomstate . choice ( i ) return feats [:, i_pick ] def _checkParam ( n_samples : int = 100 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , n_features = 1 , flip_y : float = 0 , noise : float = 1 , partition = None , ** kwargs ): if not 1 < n_samples : raise ValueError ( \"We need at least 2 samples.\" ) if not 0 < n_features : raise ValueError ( \"We need at least one feature.\" ) if not 0 <= flip_y < 1 : raise ValueError ( \"Flip percentage has to be between 0 and 1.\" ) if not n_redundant + n_repeated + n_strel <= n_features : raise ValueError ( \"Inconsistent number of features\" ) if n_strel + n_redundant < 1 : raise ValueError ( \"No informative features.\" ) if n_strel == 0 and n_redundant < 2 : raise ValueError ( \"We need more than 1 redundant feature.\" ) if partition is not None : if sum ( partition ) != n_redundant : raise ValueError ( \"Sum of partition values should yield number of redundant features.\" ) if 0 in partition or 1 in partition : raise ValueError ( \"Subset defined in Partition needs at least 2 features. 0 and 1 is not allowed.\" ) def _fillVariableSpace ( X_informative , random_state : RandomState , n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 1 , partition = None , ** kwargs ): if partition is not None : assert n_redundant == np . sum ( partition ) X = np . zeros (( int ( n_samples ), int ( n_features ))) X [:, : n_strel ] = X_informative [:, : n_strel ] holdout = X_informative [:, n_strel :] i = n_strel pi = 0 for x in range ( len ( holdout . T )): size = partition [ pi ] X [:, i : i + size ] = _combFeat ( n_samples , size , holdout [:, x ], random_state ) i += size pi += 1 for x in range ( n_repeated ): X [:, i ] = _repeatFeat ( X [:, : i ], i , random_state ) i += 1 for x in range ( n_features - i ): X [:, i ] = _dummyFeat ( n_samples , random_state ) i += 1 return X def generate_binary_classification_problem ( n_samples : int , features : int , random_state : RandomState = None , data_range = 1 ): \"\"\" Generate data uniformly distributed in a square and perfectly separated by the hyperplane given by normal_vector and b. Keyword arguments: n_samples -- number of samples required (default 100) n_features -- number of features required normal_vector -- the normal vector of the separating hyperplane data_range -- data is distributed between -data_range and data_range (default 10) \"\"\" random_state = check_random_state ( random_state ) data = random_state . normal ( size = ( n_samples , features ), scale = data_range ) labels = np . sum ( data , 1 ) > 0 labels = labels . astype ( int ) labels [ labels == 0 ] = - 1 return data , labels def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None , ): \"\"\"Generate synthetic classification data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes. Raises ------ ValueError Description ValueError Wrong parameters for specified amonut of features/samples. Examples --------- >>> X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None >>> X.shape (200, 2) >>> y.shape (200,) \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) X = np . zeros (( n_samples , n_features )) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 X_informative , Y = generate_binary_classification_problem ( n_samples , n_strel + part_size , random_state ) X = _fillVariableSpace ( X_informative , random_state , n_samples = n_samples , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) # Add target noise if flip_y > 0 : n_flip = int ( flip_y * n_samples ) Y [ random_state . choice ( n_samples , n_flip )] *= - 1 # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise ) return X , Y def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , ) -> object : \"\"\"Generate synthetic regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) # Find partitions which def\u00edne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 n_informative = n_strel + part_size X = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [: n_informative , :] = 0.3 bias = 0 y = np . dot ( X , ground_truth ) + bias # Add noise if noise > 0.0 : y += random_state . normal ( scale = noise , size = y . shape ) X = _fillVariableSpace ( X , random_state , n_samples = n_samples , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , noise = noise , partition = partition , ) y = np . squeeze ( y ) return X , y def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 , ): \"\"\" Generate ordinal regression data Parameters ---------- n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns ------- X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) if not n_target_bins > 1 : raise ValueError ( \"At least 2 target bins needed\" ) # Use normal regression data as starting point X_regression , Y_regression = genRegressionData ( n_samples = int ( n_samples ), n_features = int ( n_features ), n_redundant = int ( n_redundant ), n_strel = int ( n_strel ), n_repeated = int ( n_repeated ), noise = 0 , random_state = random_state , partition = partition , ) bin_size = int ( np . floor ( n_samples / n_target_bins )) rest = int ( n_samples - ( bin_size * n_target_bins )) # Sort the target values and rearange the data accordingly sort_indices = np . argsort ( Y_regression ) X = X_regression [ sort_indices ] Y = Y_regression [ sort_indices ] # Assign ordinal classes as target values for i in range ( n_target_bins ): Y [ bin_size * i : bin_size * ( i + 1 )] = i # Put non divisable rest into last bin if rest > 0 : Y [ - rest :] = n_target_bins - 1 X , Y = shuffle ( X , Y , random_state = random_state ) # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ), scale = noise ) return X , Y","title":"Module fri.toydata.gen_data"},{"location":"reference/fri/toydata/gen_data/#functions","text":"","title":"Functions"},{"location":"reference/fri/toydata/gen_data/#genclassificationdata","text":"def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.1 , flip_y : float = 0 , random_state : object = None , partition = None ) Generate synthetic classification data","title":"genClassificationData"},{"location":"reference/fri/toydata/gen_data/#parameters","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float Added gaussian noise to data. Parameter scales Std of normal distribution. flip_y : float, optional Ratio of samples randomly switched to wrong class. random_state : object, optional Randomstate object used for generation.","title":"Parameters"},{"location":"reference/fri/toydata/gen_data/#returns","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output classes.","title":"Returns"},{"location":"reference/fri/toydata/gen_data/#raises","text":"ValueError Description ValueError Wrong parameters for specified amonut of features/samples.","title":"Raises"},{"location":"reference/fri/toydata/gen_data/#examples","text":"X,y = genClassificationData(n_samples=200) Generating dataset with d=2,n=200,strongly=1,weakly=0, partition of weakly=None X.shape (200, 2) y.shape (200,) View Source def genClassificationData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 1 , flip_y : float = 0 , random_state : object = None , partition = None , ) : \"\"\" Generate synthetic classification data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float Added gaussian noise to data . Parameter scales Std of normal distribution . flip_y : float , optional Ratio of samples randomly switched to wrong class . random_state : object , optional Randomstate object used for generation . Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output classes . Raises ------ ValueError Description ValueError Wrong parameters for specified amonut of features / samples . Examples --------- >>> X , y = genClassificationData ( n_samples = 200 ) Generating dataset with d = 2 , n = 200 , strongly = 1 , weakly = 0 , partition of weakly = None >>> X . shape ( 200 , 2 ) >>> y . shape ( 200 , ) \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) X = np . zeros (( n_samples , n_features )) # Find partitions which def \u00ed ne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 X_informative , Y = generate_binary_classification_problem ( n_samples , n_strel + part_size , random_state ) X = _fillVariableSpace ( X_informative , random_state , n_samples = n_samples , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , partition = partition , ) # Add target noise if flip_y > 0 : n_flip = int ( flip_y * n_samples ) Y [ random_state . choice ( n_samples , n_flip ) ] *= - 1 # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ) , scale = noise ) return X , Y","title":"Examples"},{"location":"reference/fri/toydata/gen_data/#genordinalregressiondata","text":"def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None , n_target_bins : int = 3 ) Generate ordinal regression data","title":"genOrdinalRegressionData"},{"location":"reference/fri/toydata/gen_data/#parameters_1","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation. n_target_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes","title":"Parameters"},{"location":"reference/fri/toydata/gen_data/#returns_1","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target).","title":"Returns"},{"location":"reference/fri/toydata/gen_data/#raises_1","text":"ValueError Wrong parameters for specified amonut of features/samples. View Source def genOrdinalRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 0 , random_state : object = None , partition = None , n_target_bins : int = 3 , ) : \"\"\" Generate ordinal regression data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float , optional Noise of the created samples around ground truth . random_state : object , optional Randomstate object used for generation . n_target_bins : int , optional Number of bins in which the regressional target variable is split to form the ordinal classes Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output values ( target ) . Raises ------ ValueError Wrong parameters for specified amonut of features / samples . \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) if not n_target_bins > 1 : raise ValueError ( \" At least 2 target bins needed \" ) # Use normal regression data as starting point X_regression , Y_regression = genRegressionData ( n_samples = int ( n_samples ) , n_features = int ( n_features ) , n_redundant = int ( n_redundant ) , n_strel = int ( n_strel ) , n_repeated = int ( n_repeated ) , noise = 0 , random_state = random_state , partition = partition , ) bin_size = int ( np . floor ( n_samples / n_target_bins )) rest = int ( n_samples - ( bin_size * n_target_bins )) # Sort the target values and rearange the data accordingly sort_indices = np . argsort ( Y_regression ) X = X_regression [ sort_indices ] Y = Y_regression [ sort_indices ] # Assign ordinal classes as target values for i in range ( n_target_bins ) : Y [ bin_size * i : bin_size * ( i + 1 ) ] = i # Put non divisable rest into last bin if rest > 0 : Y [ - rest :] = n_target_bins - 1 X , Y = shuffle ( X , Y , random_state = random_state ) # Add gaussian noise to data X = X + random_state . normal ( size = ( n_samples , n_features ) , scale = noise ) return X , Y","title":"Raises"},{"location":"reference/fri/toydata/gen_data/#genregressiondata","text":"def ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0.0 , random_state : object = None , partition = None ) -> object Generate synthetic regression data","title":"genRegressionData"},{"location":"reference/fri/toydata/gen_data/#parameters_2","text":"n_samples : int, optional Number of samples n_features : int, optional Number of features n_redundant : int, optional Number of features which are part of redundant subsets (weakly relevant) n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_repeated : int, optional Number of features which are clones of existing ones. noise : float, optional Noise of the created samples around ground truth. random_state : object, optional Randomstate object used for generation.","title":"Parameters"},{"location":"reference/fri/toydata/gen_data/#returns_2","text":"X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The output values (target).","title":"Returns"},{"location":"reference/fri/toydata/gen_data/#raises_2","text":"ValueError Wrong parameters for specified amonut of features/samples. View Source def genRegressionData ( n_samples : int = 100 , n_features : int = 2 , n_redundant : int = 0 , n_strel : int = 1 , n_repeated : int = 0 , noise : float = 0 . 0 , random_state : object = None , partition = None , ) -> object : \"\"\" Generate synthetic regression data Parameters ---------- n_samples : int , optional Number of samples n_features : int , optional Number of features n_redundant : int , optional Number of features which are part of redundant subsets ( weakly relevant ) n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . noise : float , optional Noise of the created samples around ground truth . random_state : object , optional Randomstate object used for generation . Returns ------- X : array of shape [ n_samples , n_features ] The generated samples . y : array of shape [ n_samples ] The output values ( target ) . Raises ------ ValueError Wrong parameters for specified amonut of features / samples . \"\"\" _checkParam ( ** locals ()) random_state = check_random_state ( random_state ) # Find partitions which def \u00ed ne the weakly relevant subsets if partition is None and n_redundant > 0 : partition = [ n_redundant ] part_size = 1 elif partition is not None : part_size = len ( partition ) else : part_size = 0 n_informative = n_strel + part_size X = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [: n_informative , :] = 0 . 3 bias = 0 y = np . dot ( X , ground_truth ) + bias # Add noise if noise > 0 . 0 : y += random_state . normal ( scale = noise , size = y . shape ) X = _fillVariableSpace ( X , random_state , n_samples = n_samples , n_features = n_features , n_redundant = n_redundant , n_strel = n_strel , n_repeated = n_repeated , noise = noise , partition = partition , ) y = np . squeeze ( y ) return X , y","title":"Raises"},{"location":"reference/fri/toydata/gen_data/#generate_binary_classification_problem","text":"def ( n_samples : int , features : int , random_state : numpy . random . mtrand . RandomState = None , data_range = 1 ) Generate data uniformly distributed in a square and perfectly separated by the hyperplane given by normal_vector and b. Keyword arguments: n_samples -- number of samples required (default 100) n_features -- number of features required normal_vector -- the normal vector of the separating hyperplane data_range -- data is distributed between -data_range and data_range (default 10) View Source def generate_binary_classification_problem ( n_samples : int , features : int , random_state : RandomState = None , data_range = 1 ) : \"\"\" Generate data uniformly distributed in a square and perfectly separated by the hyperplane given by normal_vector and b. Keyword arguments : n_samples -- number of samples required ( default 100 ) n_features -- number of features required normal_vector -- the normal vector of the separating hyperplane data_range -- data is distributed between - data_range and data_range ( default 10 ) \"\"\" random_state = check_random_state ( random_state ) data = random_state . normal ( size = ( n_samples , features ) , scale = data_range ) labels = np . sum ( data , 1 ) > 0 labels = labels . astype ( int ) labels [ labels == 0 ] = - 1 return data , labels","title":"generate_binary_classification_problem"},{"location":"reference/fri/toydata/gen_lupi/","text":"Module fri.toydata.gen_lupi View Source import numpy as np from sklearn.utils import check_random_state from fri import ProblemName def _checkLupiParam ( problemName , lupiType , n_strel , n_weakrel , n_priv_weakrel , partition , partition_priv ): \"\"\" Checks if the parameters supplied to the genLupiData() function are okay. Parameters ---------- problemName : Str Must be one of ['classification', 'regression', 'ordinalRegression'] lupiType : Str Must be one of ['cleanLabels', 'cleanFeatures'] n_strel : int Stands for the number of strongly relevant features to generate in genLupiData() Must be greater than 0 n_weakrel : int Must be equal to the length of the partition list n_priv_weakrel : int Must be equal to the length of the partition_priv list partition : list of int The length of the list must be equal to n_weakrel partition_priv : list of int The length of the list must be equal to n_priv_weakrel \"\"\" if type ( problemName ) is not ProblemName : raise ValueError ( \"Not of Type ProblemName\" ) if lupiType not in [ \"cleanLabels\" , \"cleanFeatures\" ]: raise ValueError ( \"The lupiType parameter must be a string out of ['cleanLabels', 'cleanFeatures'].\" ) if n_strel < 1 : raise ValueError ( \"At least one strongly relevant feature is necessary (Parmeter 'n_strel' must be greater than 0).\" ) if partition is not None : if sum ( partition ) != n_weakrel : raise ValueError ( \"The sum over the entries in the partition list must be equal to the parameter 'n_weakrel'.\" ) if 0 in partition or 1 in partition : raise ValueError ( \"The entries in the partition list must be greater or equal to 2.\" ) if partition_priv is not None : if sum ( partition_priv ) != n_priv_weakrel : raise ValueError ( \"The sum over the entries in the partition_priv list must be equal to the parameter 'n_priv_weakrel'.\" ) if 0 in partition_priv or 1 in partition_priv : raise ValueError ( \"The entries in the partition_priv list must be greater or equal to 2.\" ) if lupiType == \"cleanLabels\" and n_priv_weakrel > 0 : raise ValueError ( \"The 'cleanLabels' data has only one strongly relevant feature by nature, this can be repeated ('n_priv_repeated'),\" \"or useless information can be added ('n_priv_irrel') but it can not be weakend => n_priv_weakrel hast to be 0.\" ) def _genWeakFeatures ( n_weakrel , X , random_state , partition ): \"\"\" Generate n_weakrel features out of the strRelFeature Parameters ---------- n_weakrel : int Number of weakly relevant feature to be generated X : array of shape [n_samples, n_features] Contains the data out of which the weakly relevant features are created random_state : Random State object Used to generate the samples partition : list of int Used to define how many weak features are calculated from the same strong feature The sum of the entries in the partition list must be equal to n_weakrel Returns ---------- X_weakrel : array of shape [n_samples, n_weakrel] Contains the data of the generated weak relevant features \"\"\" X_weakrel = np . zeros ([ X . shape [ 0 ], n_weakrel ]) if partition is None : for i in range ( n_weakrel ): X_weakrel [:, i ] = X [ :, random_state . choice ( X . shape [ 1 ]) ] + random_state . normal ( loc = 0 , scale = 1 , size = 1 ) else : idx = 0 for j in range ( len ( partition )): X_weakrel [:, idx : idx + partition [ j ]] = np . tile ( X [:, random_state . choice ( X . shape [ 1 ])], ( partition [ j ], 1 ) ) . T + random_state . normal ( loc = 0 , scale = 1 , size = partition [ j ]) idx += partition [ j ] return X_weakrel def _genRepeatedFeatures ( n_repeated , X , random_state ): \"\"\" Generate repeated features by picking a random existing feature out of X Parameters ---------- n_repeated : int Number of repeated features to create X : array of shape [n_samples, n_features] Contains the data of which the repeated features are picked random_state : Random State object Used to randomly pick a feature out of X \"\"\" X_repeated = np . zeros ([ X . shape [ 0 ], n_repeated ]) for i in range ( n_repeated ): X_repeated [:, i ] = X [:, random_state . choice ( X . shape [ 1 ])] return X_repeated # def _genCleanLabelsLupiData(problemType, n_samples, n_informative, noise, random_state, n_ordinal_bins): # # \"\"\" # Generate strongly relevant problem data (X_informative) alongside one strongly relevant privileged feature (X_priv_strel), # the privileged feature consists of the clean (real) y-labels for the problem data. The actually returned # y-values (y) are noisy and differ in form based on the problemType. # # Parameters # ---------- # problemType : Str # Must be one of ['classification', 'regression', 'ordinalRegression'], defines the y-values of the problem # n_samples : int # Number of samples to be created # n_informative : int # Number of strongly relevant features to be created # noise : float # Noise of the created samples around ground truth # random_state : Random State object # Used to randomly pick a feature out of X # n_ordinal_bins : int # Number of bins in which the regressional target variable is split to form the ordinal classes, # Only has an effect if problemType == 'ordinalRegression' # \"\"\" # # w = random_state.normal(size=n_informative) # X_informative = random_state.normal(size=(n_samples, n_informative)) # e = random_state.normal(size=n_samples, scale=noise) # X_priv_strel = np.dot(X_informative[:, :n_informative], w) # scores = (X_priv_strel + e)[:, np.newaxis] # # if problemType == 'classification': # y = (scores > 0).astype(int) # elif problemType == 'regression': # y = scores # elif problemType == 'ordinalRegression': # bs = np.append(np.sort(random_state.normal(size=n_ordinal_bins - 1)), np.inf) # y = np.sum(scores - bs >= 0, -1) # # return (X_informative, X_priv_strel[:, np.newaxis], y) # # # def _genCleanFeaturesLupiData(problemType, n_samples, n_strel, noise, random_state, n_ordinal_bins): # # \"\"\" # Generate strongly relevant problem data (X_strel) alongside the same number of strongly relevant privileged # features (X_priv_strel). The privileged features are the actual clean versions of the data, while the data (X_strel) # that is corresponding to the target variable y has noise in it. # # Parameters # ---------- # problemType : Str # Must be one of ['classification', 'regression', 'ordinalRegression'], defines the y-values of the problem # n_samples : int # Number of samples to be created # n_strel : int # Number of strongly relevant features to be created # noise : float # Noise of the created samples around ground truth # random_state : Random State object # Used to randomly pick a feature out of X # n_ordinal_bins : int # Number of bins in which the regressional target variable is split to form the ordinal classes, # Only has an effect if problemType == 'ordinalRegression' # \"\"\" # # w = random_state.normal(size=n_strel) # X_priv_strel = random_state.normal(size=(n_samples, n_strel)) # e = np.random.normal(size=(n_samples, n_strel), scale=noise) # X_strel = X_priv_strel + e # scores = np.dot(X_priv_strel, w)[:, np.newaxis] # # if problemType == 'classification': # y = (scores > 0).astype(int) # elif problemType == 'regression': # y = scores # elif problemType == 'ordinalRegression': # bs = np.append(np.sort(random_state.normal(size=n_ordinal_bins - 1)), np.inf) # y = np.sum(scores - bs >= 0, -1) # # return (X_strel, X_priv_strel, y) # def genLupiData(problemName: ProblemName, lupiType: str = \"cleanFeatures\", n_samples: int = 100, random_state: object = None, noise: float = 0.1, # n_ordinal_bins: int = 3, n_strel: int = 1, n_weakrel: int = 0, n_repeated: int = 0, n_irrel: int = 0, # n_priv_weakrel: int = 0, n_priv_repeated: int = 0, n_priv_irrel: int = 0, partition=None, # partition_priv=None): # # \"\"\" # Generate Lupi Data for Classification, Regression and Ordinal Regression Problems # # Parameters # ---------- # problemName : ProblemName # Defines the type of y-values of the problem. Example `ProblemName.CLASSIFICATION`. # lupiType : Str # Must be one of ['cleanLabels', 'cleanFeatures'], defines the strongly relevant features of the privileged data # n_samples : int, optional # Number of samples # random_state : object, optional # Randomstate object used for generation. # noise : float, optional # Noise of the created samples around ground truth. # n_ordinal_bins : int, optional # Number of bins in which the regressional target variable is split to form the ordinal classes, # Only has an effect if problemType == 'ordinalRegression' # n_strel : int, optional # Number of features which are mandatory for the underlying model (strongly relevant) # n_weakrel : int, optional # Number of features which are part of redundant subsets (weakly relevant) # n_repeated : int, optional # Number of features which are clones of existing ones. # n_irrel : int, optional # Number of features which are irrelevant to the underlying model # n_priv_weakrel : int, optional # Number of features in the privileged data that are weakly relevant # n_priv_repeated : int, optional # Number of privileged features which are clones of existing privileged features # n_priv_irrel: int, optional # Number of privileged features which are irrelevant # partition : list of int # Entries of the list define weak subsets. So an entry of the list says how many weak features are calculated # from the same strong feature. # The sum over the list entries must be equal to n_weakrel # partition_priv : list of int # Entries of the list define privileged weak subsets. So an entry of the list says how many privileged weak # features are calculated from the same privileged strong feature. # The sum over the list entries must be equal to n_priv_weakrel # # # Returns # ------- # X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] # The generated samples # X_priv : # The generated privileged samples # In case of lupiType == 'cleanLabels' : array of shape [n_samples, (n_priv_weakrel + n_priv_repeated + n_priv_irrel + 1)] # In case of lupiType == 'cleanFeatures' : array of shape [n_samples, (n_priv_weakrel + n_priv_repeated + n_priv_irrel + n_strel)] # y : array of shape [n_samples] # The generated target values # In case of problemType == 'classification' : values are in [0,1] # In case of problemType == 'regression' : values are continious # In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] # # # \"\"\" # # _checkLupiParam(problemName=problemName, lupiType=lupiType, n_strel=n_strel, n_weakrel=n_weakrel, # n_priv_weakrel=n_priv_weakrel, partition=partition, partition_priv=partition_priv) # random_state = check_random_state(random_state) # # # if lupiType == 'cleanLabels': # # n_informative = n_strel + n_weakrel # # # X_strel : array of shape [n_samples, n_strel], contains the strongly relevant data features # # X_priv_strel : array of shape [n_samples], contains the strongly relevant privileged data feature # # y : array of shape [n_samples], contains the target values to the problem # X_informative, X_priv_strel, y = _genCleanLabelsLupiData(problemType=problemName, n_samples=n_samples, n_informative=n_informative, # noise=noise, random_state=random_state, n_ordinal_bins=n_ordinal_bins) # # X_priv_repeated = _genRepeatedFeatures(n_priv_repeated, X_priv_strel, random_state) # X_priv_irrel = random_state.normal(size=(n_samples, n_priv_irrel)) # X_priv = np.hstack([X_priv_strel, X_priv_repeated, X_priv_irrel]) # # elif lupiType == 'cleanFeatures': # # # X_strel : array of shape [n_samples, n_strel], contains the strongly relevant data features # # X_priv_strel = array of shape [n_samples, n_strel], contains the strongly relevant privileged features # # y : array of shape [n_samples], contains the target values to the problem # X_strel, X_priv_strel, y = _genCleanFeaturesLupiData(problemType=problemName, n_samples=n_samples, n_strel=n_strel, # noise=noise, random_state=random_state, n_ordinal_bins=n_ordinal_bins) # # X_priv_weakrel = _genWeakFeatures(n_priv_weakrel, X_priv_strel, random_state, partition_priv) # X_priv_repeated = _genRepeatedFeatures(n_priv_repeated, X_priv_strel, random_state) # X_priv_irrel = random_state.normal(size=(n_samples, n_priv_irrel)) # X_priv = np.hstack([X_priv_strel, X_priv_weakrel, X_priv_repeated, X_priv_irrel]) # # # X_strel = X_informative[:, :n_strel] # X_weakrel = _genWeakFeatures(n_weakrel, X_informative[:, n_strel:], random_state, partition) # X_repeated = _genRepeatedFeatures(n_repeated, X_strel, random_state) # X_irrel = random_state.normal(size=(n_samples, n_irrel)) # X = np.hstack([X_strel, X_weakrel, X_repeated, X_irrel]) # # return X, X_priv, y def genLupiData ( problemName : ProblemName , n_samples : int = 100 , random_state : object = None , noise : float = 0.1 , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel_groups : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 , ): \"\"\" Generate Lupi Data for Classification, Regression and Ordinal Regression Problems Parameters ---------- problemName : ProblemName Defines the type of y-values of the problem. Example `ProblemName.CLASSIFICATION`. n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. noise : float, optional Noise of the created samples around ground truth. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel_groups : int, optional Number of 2 feature groups which are part of redundant subsets (weakly relevant) n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated. Returns ------- X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] \"\"\" random_state = check_random_state ( random_state ) n_informative = n_strel + n_weakrel_groups w = random_state . normal ( size = n_informative ) X_informative = random_state . normal ( size = ( n_samples , n_informative )) X_priv_strel = X_informative [:, : n_strel ] X_priv_weakrel = np . zeros ([ n_samples , n_weakrel_groups * 2 ]) idx = 0 for i in range ( n_weakrel_groups ): X_priv_weakrel [:, idx : idx + 2 ] = np . tile ( X_informative [:, n_strel + i ], ( 2 , 1 ) ) . T + random_state . normal ( loc = 0 , scale = np . std ( X_informative [:, n_strel + i ]), size = 2 ) idx += 2 X_priv_repeated = _genRepeatedFeatures ( n_repeated , np . hstack ([ X_priv_strel , X_priv_weakrel ]), random_state ) X_priv = np . hstack ([ X_priv_strel , X_priv_weakrel , X_priv_repeated ]) e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ]), scale = noise * np . std ( X_priv ) ) X = X_priv + e scores = np . dot ( X_informative , w ) if ( problemName == \"classification\" or problemName == ProblemName . LUPI_CLASSIFICATION ): y = scores > 0 elif problemName == \"regression\" or problemName == ProblemName . LUPI_REGRESSION : y = scores elif ( problemName == \"ordinalRegression\" or problemName == ProblemName . LUPI_ORDREGRESSION ): step = 1 / ( n_ordinal_bins ) quantiles = [ i * step for i in range ( 1 , n_ordinal_bins )] bs = np . quantile ( scores , quantiles ) bs = np . append ( bs , np . inf ) scores = scores [:, np . newaxis ] y = np . sum ( scores - bs >= 0 , - 1 ) if n_irrel > 0 : X = np . hstack ([ X , random_state . normal ( size = ( n_samples , n_irrel ))]) X_priv = np . hstack ([ X_priv , random_state . normal ( size = ( n_samples , n_irrel ))]) if label_noise > 0 : sample = random_state . choice ( len ( y ), int ( len ( y ) * label_noise )) y [ sample ] = random_state . permutation ( y [ sample ]) return ( X , X_priv , y . squeeze ()) ####################################################################################################################### # # # New Regression # # # ####################################################################################################################### def _checkParam2 ( n_samples , n_strel , n_weakrel , flip_y : float = 0 , partition = None ): if not 1 < n_samples : raise ValueError ( \"We need at least 2 samples.\" ) if not 0 <= flip_y < 1 : raise ValueError ( \"Flip percentage has to be between 0 and 1.\" ) if n_strel + n_weakrel < 1 : raise ValueError ( \"No informative features.\" ) if n_strel == 0 and n_weakrel < 2 : raise ValueError ( \"If we have no strong features, we need more than 1 weak feature.\" ) if partition is not None : if sum ( partition ) != n_weakrel : raise ValueError ( \"Sum of partition values should yield number of redundant features.\" ) if 0 in partition or 1 in partition : raise ValueError ( \"Subset defined in Partition needs at least 2 features. 0 and 1 is not allowed.\" ) def genRegressionData2 ( n_samples : int = 100 , random_state : object = None , noise : float = 0.0 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , partition = None , ) -> object : \"\"\"Generate synthetic regression data Parameters ---------- n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation noise : float, optional Noise of the created samples around ground truth n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features that are irrelevant partition: list of int Entries of the list define how many weak relevant features are based on the same strongly relevant feature The sum over all list entries must be equal to n_weakrel Returns ------- X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam2 ( n_samples = n_samples , n_strel = n_strel , n_weakrel = n_weakrel , partition = partition ) random_state = check_random_state ( random_state ) if partition is None : n_informative = n_strel + n_weakrel else : n_informative = n_strel + len ( partition ) X_informative = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [: n_informative , :] = 0.3 bias = 0 y = np . dot ( X_informative , ground_truth ) + bias if noise > 0.0 : y += random_state . normal ( scale = noise , size = y . shape ) X_strel = X_informative [:, : n_strel ] X_weakrel = _genWeakFeatures ( n_weakrel , X_informative [:, n_strel :], random_state , partition ) X_repeated = _genRepeatedFeatures ( n_repeated , np . hstack ([ X_strel , X_weakrel ]), random_state ) X_irrel = random_state . normal ( size = ( n_samples , n_irrel )) X = np . hstack ([ X_strel , X_weakrel , X_repeated , X_irrel ]) y = np . squeeze ( y ) return X , y Functions genLupiData def ( problemName : fri . ProblemName , n_samples : int = 100 , random_state : object = None , noise : float = 0.1 , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel_groups : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 ) Generate Lupi Data for Classification, Regression and Ordinal Regression Problems Parameters problemName : ProblemName Defines the type of y-values of the problem. Example ProblemName.CLASSIFICATION . n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. noise : float, optional Noise of the created samples around ground truth. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel_groups : int, optional Number of 2 feature groups which are part of redundant subsets (weakly relevant) n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated. Returns X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] View Source def genLupiData ( problemName : ProblemName , n_samples : int = 100 , random_state : object = None , noise : float = 0 . 1 , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel_groups : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0 . 0 , ) : \"\"\" Generate Lupi Data for Classification , Regression and Ordinal Regression Problems Parameters ---------- problemName : ProblemName Defines the type of y - values of the problem . Example ` ProblemName . CLASSIFICATION `. n_samples : int , optional Number of samples random_state : object , optional Randomstate object used for generation . noise : float , optional Noise of the created samples around ground truth . n_ordinal_bins : int , optional Number of bins in which the regressional target variable is split to form the ordinal classes , Only has an effect if problemType == ' ordinalRegression ' n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_weakrel_groups : int , optional Number of 2 feature groups which are part of redundant subsets ( weakly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . n_irrel : int , optional Number of features which are irrelevant to the underlying model label_noise : float , optional Percentage of labels which get permutated . Returns ------- X : array of shape [ n_samples , ( n_strel + n_weakrel + n_repeated + n_irrel ) ] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [ n_samples ] The generated target values In case of problemType == ' classification ' : values are in [ 0 , 1 ] In case of problemType == ' regression ' : values are continious In case of problemType == ' ordinalRegression ' : values are in [ 0 , n_ordinal_bins ] \"\"\" random_state = check_random_state ( random_state ) n_informative = n_strel + n_weakrel_groups w = random_state . normal ( size = n_informative ) X_informative = random_state . normal ( size = ( n_samples , n_informative )) X_priv_strel = X_informative [:, : n_strel ] X_priv_weakrel = np . zeros ( [ n_samples , n_weakrel_groups * 2 ] ) idx = 0 for i in range ( n_weakrel_groups ) : X_priv_weakrel [:, idx : idx + 2 ] = np . tile ( X_informative [:, n_strel + i ], ( 2 , 1 ) ) . T + random_state . normal ( loc = 0 , scale = np . std ( X_informative [:, n_strel + i ] ) , size = 2 ) idx += 2 X_priv_repeated = _genRepeatedFeatures ( n_repeated , np . hstack ( [ X_priv_strel , X_priv_weakrel ] ) , random_state ) X_priv = np . hstack ( [ X_priv_strel , X_priv_weakrel , X_priv_repeated ] ) e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ) , scale = noise * np . std ( X_priv ) ) X = X_priv + e scores = np . dot ( X_informative , w ) if ( problemName == \" classification \" or problemName == ProblemName . LUPI_CLASSIFICATION ) : y = scores > 0 elif problemName == \" regression \" or problemName == ProblemName . LUPI_REGRESSION : y = scores elif ( problemName == \" ordinalRegression \" or problemName == ProblemName . LUPI_ORDREGRESSION ) : step = 1 / ( n_ordinal_bins ) quantiles = [ i * step for i in range ( 1 , n_ordinal_bins ) ] bs = np . quantile ( scores , quantiles ) bs = np . append ( bs , np . inf ) scores = scores [:, np . newaxis ] y = np . sum ( scores - bs >= 0 , - 1 ) if n_irrel > 0 : X = np . hstack ( [ X , random_state . normal ( size = ( n_samples , n_irrel )) ] ) X_priv = np . hstack ( [ X_priv , random_state . normal ( size = ( n_samples , n_irrel )) ] ) if label_noise > 0 : sample = random_state . choice ( len ( y ) , int ( len ( y ) * label_noise )) y [ sample ] = random_state . permutation ( y [ sample ] ) return ( X , X_priv , y . squeeze ()) genRegressionData2 def ( n_samples : int = 100 , random_state : object = None , noise : float = 0.0 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , partition = None ) -> object Generate synthetic regression data Parameters n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation noise : float, optional Noise of the created samples around ground truth n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features that are irrelevant partition: list of int Entries of the list define how many weak relevant features are based on the same strongly relevant feature The sum over all list entries must be equal to n_weakrel Returns X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel] The generated samples. y : array of shape [n_samples] The output values (target). Raises ValueError Wrong parameters for specified amonut of features/samples. View Source def genRegressionData2 ( n_samples : int = 100 , random_state : object = None , noise : float = 0 . 0 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , partition = None , ) -> object : \"\"\" Generate synthetic regression data Parameters ---------- n_samples : int , optional Number of samples random_state : object , optional Randomstate object used for generation noise : float , optional Noise of the created samples around ground truth n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_weakrel : int , optional Number of weakly relevant features n_repeated : int , optional Number of features which are clones of existing ones . n_irrel : int , optional Number of features that are irrelevant partition : list of int Entries of the list define how many weak relevant features are based on the same strongly relevant feature The sum over all list entries must be equal to n_weakrel Returns ------- X : array of shape [ n_samples , ( n_strel + n_weakrel + n_repeated + n_irrel ] The generated samples . y : array of shape [ n_samples ] The output values ( target ) . Raises ------ ValueError Wrong parameters for specified amonut of features / samples . \"\"\" _checkParam2 ( n_samples = n_samples , n_strel = n_strel , n_weakrel = n_weakrel , partition = partition ) random_state = check_random_state ( random_state ) if partition is None : n_informative = n_strel + n_weakrel else : n_informative = n_strel + len ( partition ) X_informative = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [: n_informative , :] = 0 . 3 bias = 0 y = np . dot ( X_informative , ground_truth ) + bias if noise > 0 . 0 : y += random_state . normal ( scale = noise , size = y . shape ) X_strel = X_informative [:, : n_strel ] X_weakrel = _genWeakFeatures ( n_weakrel , X_informative [:, n_strel :], random_state , partition ) X_repeated = _genRepeatedFeatures ( n_repeated , np . hstack ( [ X_strel , X_weakrel ] ) , random_state ) X_irrel = random_state . normal ( size = ( n_samples , n_irrel )) X = np . hstack ( [ X_strel , X_weakrel , X_repeated , X_irrel ] ) y = np . squeeze ( y ) return X , y","title":"Gen Lupi"},{"location":"reference/fri/toydata/gen_lupi/#module-fritoydatagen_lupi","text":"View Source import numpy as np from sklearn.utils import check_random_state from fri import ProblemName def _checkLupiParam ( problemName , lupiType , n_strel , n_weakrel , n_priv_weakrel , partition , partition_priv ): \"\"\" Checks if the parameters supplied to the genLupiData() function are okay. Parameters ---------- problemName : Str Must be one of ['classification', 'regression', 'ordinalRegression'] lupiType : Str Must be one of ['cleanLabels', 'cleanFeatures'] n_strel : int Stands for the number of strongly relevant features to generate in genLupiData() Must be greater than 0 n_weakrel : int Must be equal to the length of the partition list n_priv_weakrel : int Must be equal to the length of the partition_priv list partition : list of int The length of the list must be equal to n_weakrel partition_priv : list of int The length of the list must be equal to n_priv_weakrel \"\"\" if type ( problemName ) is not ProblemName : raise ValueError ( \"Not of Type ProblemName\" ) if lupiType not in [ \"cleanLabels\" , \"cleanFeatures\" ]: raise ValueError ( \"The lupiType parameter must be a string out of ['cleanLabels', 'cleanFeatures'].\" ) if n_strel < 1 : raise ValueError ( \"At least one strongly relevant feature is necessary (Parmeter 'n_strel' must be greater than 0).\" ) if partition is not None : if sum ( partition ) != n_weakrel : raise ValueError ( \"The sum over the entries in the partition list must be equal to the parameter 'n_weakrel'.\" ) if 0 in partition or 1 in partition : raise ValueError ( \"The entries in the partition list must be greater or equal to 2.\" ) if partition_priv is not None : if sum ( partition_priv ) != n_priv_weakrel : raise ValueError ( \"The sum over the entries in the partition_priv list must be equal to the parameter 'n_priv_weakrel'.\" ) if 0 in partition_priv or 1 in partition_priv : raise ValueError ( \"The entries in the partition_priv list must be greater or equal to 2.\" ) if lupiType == \"cleanLabels\" and n_priv_weakrel > 0 : raise ValueError ( \"The 'cleanLabels' data has only one strongly relevant feature by nature, this can be repeated ('n_priv_repeated'),\" \"or useless information can be added ('n_priv_irrel') but it can not be weakend => n_priv_weakrel hast to be 0.\" ) def _genWeakFeatures ( n_weakrel , X , random_state , partition ): \"\"\" Generate n_weakrel features out of the strRelFeature Parameters ---------- n_weakrel : int Number of weakly relevant feature to be generated X : array of shape [n_samples, n_features] Contains the data out of which the weakly relevant features are created random_state : Random State object Used to generate the samples partition : list of int Used to define how many weak features are calculated from the same strong feature The sum of the entries in the partition list must be equal to n_weakrel Returns ---------- X_weakrel : array of shape [n_samples, n_weakrel] Contains the data of the generated weak relevant features \"\"\" X_weakrel = np . zeros ([ X . shape [ 0 ], n_weakrel ]) if partition is None : for i in range ( n_weakrel ): X_weakrel [:, i ] = X [ :, random_state . choice ( X . shape [ 1 ]) ] + random_state . normal ( loc = 0 , scale = 1 , size = 1 ) else : idx = 0 for j in range ( len ( partition )): X_weakrel [:, idx : idx + partition [ j ]] = np . tile ( X [:, random_state . choice ( X . shape [ 1 ])], ( partition [ j ], 1 ) ) . T + random_state . normal ( loc = 0 , scale = 1 , size = partition [ j ]) idx += partition [ j ] return X_weakrel def _genRepeatedFeatures ( n_repeated , X , random_state ): \"\"\" Generate repeated features by picking a random existing feature out of X Parameters ---------- n_repeated : int Number of repeated features to create X : array of shape [n_samples, n_features] Contains the data of which the repeated features are picked random_state : Random State object Used to randomly pick a feature out of X \"\"\" X_repeated = np . zeros ([ X . shape [ 0 ], n_repeated ]) for i in range ( n_repeated ): X_repeated [:, i ] = X [:, random_state . choice ( X . shape [ 1 ])] return X_repeated # def _genCleanLabelsLupiData(problemType, n_samples, n_informative, noise, random_state, n_ordinal_bins): # # \"\"\" # Generate strongly relevant problem data (X_informative) alongside one strongly relevant privileged feature (X_priv_strel), # the privileged feature consists of the clean (real) y-labels for the problem data. The actually returned # y-values (y) are noisy and differ in form based on the problemType. # # Parameters # ---------- # problemType : Str # Must be one of ['classification', 'regression', 'ordinalRegression'], defines the y-values of the problem # n_samples : int # Number of samples to be created # n_informative : int # Number of strongly relevant features to be created # noise : float # Noise of the created samples around ground truth # random_state : Random State object # Used to randomly pick a feature out of X # n_ordinal_bins : int # Number of bins in which the regressional target variable is split to form the ordinal classes, # Only has an effect if problemType == 'ordinalRegression' # \"\"\" # # w = random_state.normal(size=n_informative) # X_informative = random_state.normal(size=(n_samples, n_informative)) # e = random_state.normal(size=n_samples, scale=noise) # X_priv_strel = np.dot(X_informative[:, :n_informative], w) # scores = (X_priv_strel + e)[:, np.newaxis] # # if problemType == 'classification': # y = (scores > 0).astype(int) # elif problemType == 'regression': # y = scores # elif problemType == 'ordinalRegression': # bs = np.append(np.sort(random_state.normal(size=n_ordinal_bins - 1)), np.inf) # y = np.sum(scores - bs >= 0, -1) # # return (X_informative, X_priv_strel[:, np.newaxis], y) # # # def _genCleanFeaturesLupiData(problemType, n_samples, n_strel, noise, random_state, n_ordinal_bins): # # \"\"\" # Generate strongly relevant problem data (X_strel) alongside the same number of strongly relevant privileged # features (X_priv_strel). The privileged features are the actual clean versions of the data, while the data (X_strel) # that is corresponding to the target variable y has noise in it. # # Parameters # ---------- # problemType : Str # Must be one of ['classification', 'regression', 'ordinalRegression'], defines the y-values of the problem # n_samples : int # Number of samples to be created # n_strel : int # Number of strongly relevant features to be created # noise : float # Noise of the created samples around ground truth # random_state : Random State object # Used to randomly pick a feature out of X # n_ordinal_bins : int # Number of bins in which the regressional target variable is split to form the ordinal classes, # Only has an effect if problemType == 'ordinalRegression' # \"\"\" # # w = random_state.normal(size=n_strel) # X_priv_strel = random_state.normal(size=(n_samples, n_strel)) # e = np.random.normal(size=(n_samples, n_strel), scale=noise) # X_strel = X_priv_strel + e # scores = np.dot(X_priv_strel, w)[:, np.newaxis] # # if problemType == 'classification': # y = (scores > 0).astype(int) # elif problemType == 'regression': # y = scores # elif problemType == 'ordinalRegression': # bs = np.append(np.sort(random_state.normal(size=n_ordinal_bins - 1)), np.inf) # y = np.sum(scores - bs >= 0, -1) # # return (X_strel, X_priv_strel, y) # def genLupiData(problemName: ProblemName, lupiType: str = \"cleanFeatures\", n_samples: int = 100, random_state: object = None, noise: float = 0.1, # n_ordinal_bins: int = 3, n_strel: int = 1, n_weakrel: int = 0, n_repeated: int = 0, n_irrel: int = 0, # n_priv_weakrel: int = 0, n_priv_repeated: int = 0, n_priv_irrel: int = 0, partition=None, # partition_priv=None): # # \"\"\" # Generate Lupi Data for Classification, Regression and Ordinal Regression Problems # # Parameters # ---------- # problemName : ProblemName # Defines the type of y-values of the problem. Example `ProblemName.CLASSIFICATION`. # lupiType : Str # Must be one of ['cleanLabels', 'cleanFeatures'], defines the strongly relevant features of the privileged data # n_samples : int, optional # Number of samples # random_state : object, optional # Randomstate object used for generation. # noise : float, optional # Noise of the created samples around ground truth. # n_ordinal_bins : int, optional # Number of bins in which the regressional target variable is split to form the ordinal classes, # Only has an effect if problemType == 'ordinalRegression' # n_strel : int, optional # Number of features which are mandatory for the underlying model (strongly relevant) # n_weakrel : int, optional # Number of features which are part of redundant subsets (weakly relevant) # n_repeated : int, optional # Number of features which are clones of existing ones. # n_irrel : int, optional # Number of features which are irrelevant to the underlying model # n_priv_weakrel : int, optional # Number of features in the privileged data that are weakly relevant # n_priv_repeated : int, optional # Number of privileged features which are clones of existing privileged features # n_priv_irrel: int, optional # Number of privileged features which are irrelevant # partition : list of int # Entries of the list define weak subsets. So an entry of the list says how many weak features are calculated # from the same strong feature. # The sum over the list entries must be equal to n_weakrel # partition_priv : list of int # Entries of the list define privileged weak subsets. So an entry of the list says how many privileged weak # features are calculated from the same privileged strong feature. # The sum over the list entries must be equal to n_priv_weakrel # # # Returns # ------- # X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] # The generated samples # X_priv : # The generated privileged samples # In case of lupiType == 'cleanLabels' : array of shape [n_samples, (n_priv_weakrel + n_priv_repeated + n_priv_irrel + 1)] # In case of lupiType == 'cleanFeatures' : array of shape [n_samples, (n_priv_weakrel + n_priv_repeated + n_priv_irrel + n_strel)] # y : array of shape [n_samples] # The generated target values # In case of problemType == 'classification' : values are in [0,1] # In case of problemType == 'regression' : values are continious # In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] # # # \"\"\" # # _checkLupiParam(problemName=problemName, lupiType=lupiType, n_strel=n_strel, n_weakrel=n_weakrel, # n_priv_weakrel=n_priv_weakrel, partition=partition, partition_priv=partition_priv) # random_state = check_random_state(random_state) # # # if lupiType == 'cleanLabels': # # n_informative = n_strel + n_weakrel # # # X_strel : array of shape [n_samples, n_strel], contains the strongly relevant data features # # X_priv_strel : array of shape [n_samples], contains the strongly relevant privileged data feature # # y : array of shape [n_samples], contains the target values to the problem # X_informative, X_priv_strel, y = _genCleanLabelsLupiData(problemType=problemName, n_samples=n_samples, n_informative=n_informative, # noise=noise, random_state=random_state, n_ordinal_bins=n_ordinal_bins) # # X_priv_repeated = _genRepeatedFeatures(n_priv_repeated, X_priv_strel, random_state) # X_priv_irrel = random_state.normal(size=(n_samples, n_priv_irrel)) # X_priv = np.hstack([X_priv_strel, X_priv_repeated, X_priv_irrel]) # # elif lupiType == 'cleanFeatures': # # # X_strel : array of shape [n_samples, n_strel], contains the strongly relevant data features # # X_priv_strel = array of shape [n_samples, n_strel], contains the strongly relevant privileged features # # y : array of shape [n_samples], contains the target values to the problem # X_strel, X_priv_strel, y = _genCleanFeaturesLupiData(problemType=problemName, n_samples=n_samples, n_strel=n_strel, # noise=noise, random_state=random_state, n_ordinal_bins=n_ordinal_bins) # # X_priv_weakrel = _genWeakFeatures(n_priv_weakrel, X_priv_strel, random_state, partition_priv) # X_priv_repeated = _genRepeatedFeatures(n_priv_repeated, X_priv_strel, random_state) # X_priv_irrel = random_state.normal(size=(n_samples, n_priv_irrel)) # X_priv = np.hstack([X_priv_strel, X_priv_weakrel, X_priv_repeated, X_priv_irrel]) # # # X_strel = X_informative[:, :n_strel] # X_weakrel = _genWeakFeatures(n_weakrel, X_informative[:, n_strel:], random_state, partition) # X_repeated = _genRepeatedFeatures(n_repeated, X_strel, random_state) # X_irrel = random_state.normal(size=(n_samples, n_irrel)) # X = np.hstack([X_strel, X_weakrel, X_repeated, X_irrel]) # # return X, X_priv, y def genLupiData ( problemName : ProblemName , n_samples : int = 100 , random_state : object = None , noise : float = 0.1 , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel_groups : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 , ): \"\"\" Generate Lupi Data for Classification, Regression and Ordinal Regression Problems Parameters ---------- problemName : ProblemName Defines the type of y-values of the problem. Example `ProblemName.CLASSIFICATION`. n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. noise : float, optional Noise of the created samples around ground truth. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel_groups : int, optional Number of 2 feature groups which are part of redundant subsets (weakly relevant) n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated. Returns ------- X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] \"\"\" random_state = check_random_state ( random_state ) n_informative = n_strel + n_weakrel_groups w = random_state . normal ( size = n_informative ) X_informative = random_state . normal ( size = ( n_samples , n_informative )) X_priv_strel = X_informative [:, : n_strel ] X_priv_weakrel = np . zeros ([ n_samples , n_weakrel_groups * 2 ]) idx = 0 for i in range ( n_weakrel_groups ): X_priv_weakrel [:, idx : idx + 2 ] = np . tile ( X_informative [:, n_strel + i ], ( 2 , 1 ) ) . T + random_state . normal ( loc = 0 , scale = np . std ( X_informative [:, n_strel + i ]), size = 2 ) idx += 2 X_priv_repeated = _genRepeatedFeatures ( n_repeated , np . hstack ([ X_priv_strel , X_priv_weakrel ]), random_state ) X_priv = np . hstack ([ X_priv_strel , X_priv_weakrel , X_priv_repeated ]) e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ]), scale = noise * np . std ( X_priv ) ) X = X_priv + e scores = np . dot ( X_informative , w ) if ( problemName == \"classification\" or problemName == ProblemName . LUPI_CLASSIFICATION ): y = scores > 0 elif problemName == \"regression\" or problemName == ProblemName . LUPI_REGRESSION : y = scores elif ( problemName == \"ordinalRegression\" or problemName == ProblemName . LUPI_ORDREGRESSION ): step = 1 / ( n_ordinal_bins ) quantiles = [ i * step for i in range ( 1 , n_ordinal_bins )] bs = np . quantile ( scores , quantiles ) bs = np . append ( bs , np . inf ) scores = scores [:, np . newaxis ] y = np . sum ( scores - bs >= 0 , - 1 ) if n_irrel > 0 : X = np . hstack ([ X , random_state . normal ( size = ( n_samples , n_irrel ))]) X_priv = np . hstack ([ X_priv , random_state . normal ( size = ( n_samples , n_irrel ))]) if label_noise > 0 : sample = random_state . choice ( len ( y ), int ( len ( y ) * label_noise )) y [ sample ] = random_state . permutation ( y [ sample ]) return ( X , X_priv , y . squeeze ()) ####################################################################################################################### # # # New Regression # # # ####################################################################################################################### def _checkParam2 ( n_samples , n_strel , n_weakrel , flip_y : float = 0 , partition = None ): if not 1 < n_samples : raise ValueError ( \"We need at least 2 samples.\" ) if not 0 <= flip_y < 1 : raise ValueError ( \"Flip percentage has to be between 0 and 1.\" ) if n_strel + n_weakrel < 1 : raise ValueError ( \"No informative features.\" ) if n_strel == 0 and n_weakrel < 2 : raise ValueError ( \"If we have no strong features, we need more than 1 weak feature.\" ) if partition is not None : if sum ( partition ) != n_weakrel : raise ValueError ( \"Sum of partition values should yield number of redundant features.\" ) if 0 in partition or 1 in partition : raise ValueError ( \"Subset defined in Partition needs at least 2 features. 0 and 1 is not allowed.\" ) def genRegressionData2 ( n_samples : int = 100 , random_state : object = None , noise : float = 0.0 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , partition = None , ) -> object : \"\"\"Generate synthetic regression data Parameters ---------- n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation noise : float, optional Noise of the created samples around ground truth n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features that are irrelevant partition: list of int Entries of the list define how many weak relevant features are based on the same strongly relevant feature The sum over all list entries must be equal to n_weakrel Returns ------- X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel] The generated samples. y : array of shape [n_samples] The output values (target). Raises ------ ValueError Wrong parameters for specified amonut of features/samples. \"\"\" _checkParam2 ( n_samples = n_samples , n_strel = n_strel , n_weakrel = n_weakrel , partition = partition ) random_state = check_random_state ( random_state ) if partition is None : n_informative = n_strel + n_weakrel else : n_informative = n_strel + len ( partition ) X_informative = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [: n_informative , :] = 0.3 bias = 0 y = np . dot ( X_informative , ground_truth ) + bias if noise > 0.0 : y += random_state . normal ( scale = noise , size = y . shape ) X_strel = X_informative [:, : n_strel ] X_weakrel = _genWeakFeatures ( n_weakrel , X_informative [:, n_strel :], random_state , partition ) X_repeated = _genRepeatedFeatures ( n_repeated , np . hstack ([ X_strel , X_weakrel ]), random_state ) X_irrel = random_state . normal ( size = ( n_samples , n_irrel )) X = np . hstack ([ X_strel , X_weakrel , X_repeated , X_irrel ]) y = np . squeeze ( y ) return X , y","title":"Module fri.toydata.gen_lupi"},{"location":"reference/fri/toydata/gen_lupi/#functions","text":"","title":"Functions"},{"location":"reference/fri/toydata/gen_lupi/#genlupidata","text":"def ( problemName : fri . ProblemName , n_samples : int = 100 , random_state : object = None , noise : float = 0.1 , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel_groups : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0.0 ) Generate Lupi Data for Classification, Regression and Ordinal Regression Problems","title":"genLupiData"},{"location":"reference/fri/toydata/gen_lupi/#parameters","text":"problemName : ProblemName Defines the type of y-values of the problem. Example ProblemName.CLASSIFICATION . n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation. noise : float, optional Noise of the created samples around ground truth. n_ordinal_bins : int, optional Number of bins in which the regressional target variable is split to form the ordinal classes, Only has an effect if problemType == 'ordinalRegression' n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel_groups : int, optional Number of 2 feature groups which are part of redundant subsets (weakly relevant) n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features which are irrelevant to the underlying model label_noise: float, optional Percentage of labels which get permutated.","title":"Parameters"},{"location":"reference/fri/toydata/gen_lupi/#returns","text":"X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel)] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [n_samples] The generated target values In case of problemType == 'classification' : values are in [0,1] In case of problemType == 'regression' : values are continious In case of problemType == 'ordinalRegression' : values are in [0, n_ordinal_bins] View Source def genLupiData ( problemName : ProblemName , n_samples : int = 100 , random_state : object = None , noise : float = 0 . 1 , n_ordinal_bins : int = 3 , n_strel : int = 1 , n_weakrel_groups : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , label_noise = 0 . 0 , ) : \"\"\" Generate Lupi Data for Classification , Regression and Ordinal Regression Problems Parameters ---------- problemName : ProblemName Defines the type of y - values of the problem . Example ` ProblemName . CLASSIFICATION `. n_samples : int , optional Number of samples random_state : object , optional Randomstate object used for generation . noise : float , optional Noise of the created samples around ground truth . n_ordinal_bins : int , optional Number of bins in which the regressional target variable is split to form the ordinal classes , Only has an effect if problemType == ' ordinalRegression ' n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_weakrel_groups : int , optional Number of 2 feature groups which are part of redundant subsets ( weakly relevant ) n_repeated : int , optional Number of features which are clones of existing ones . n_irrel : int , optional Number of features which are irrelevant to the underlying model label_noise : float , optional Percentage of labels which get permutated . Returns ------- X : array of shape [ n_samples , ( n_strel + n_weakrel + n_repeated + n_irrel ) ] The generated samples X_priv : array with same shape as X The generated privileged samples y : array of shape [ n_samples ] The generated target values In case of problemType == ' classification ' : values are in [ 0 , 1 ] In case of problemType == ' regression ' : values are continious In case of problemType == ' ordinalRegression ' : values are in [ 0 , n_ordinal_bins ] \"\"\" random_state = check_random_state ( random_state ) n_informative = n_strel + n_weakrel_groups w = random_state . normal ( size = n_informative ) X_informative = random_state . normal ( size = ( n_samples , n_informative )) X_priv_strel = X_informative [:, : n_strel ] X_priv_weakrel = np . zeros ( [ n_samples , n_weakrel_groups * 2 ] ) idx = 0 for i in range ( n_weakrel_groups ) : X_priv_weakrel [:, idx : idx + 2 ] = np . tile ( X_informative [:, n_strel + i ], ( 2 , 1 ) ) . T + random_state . normal ( loc = 0 , scale = np . std ( X_informative [:, n_strel + i ] ) , size = 2 ) idx += 2 X_priv_repeated = _genRepeatedFeatures ( n_repeated , np . hstack ( [ X_priv_strel , X_priv_weakrel ] ) , random_state ) X_priv = np . hstack ( [ X_priv_strel , X_priv_weakrel , X_priv_repeated ] ) e = random_state . normal ( size = ( n_samples , X_priv . shape [ 1 ] ) , scale = noise * np . std ( X_priv ) ) X = X_priv + e scores = np . dot ( X_informative , w ) if ( problemName == \" classification \" or problemName == ProblemName . LUPI_CLASSIFICATION ) : y = scores > 0 elif problemName == \" regression \" or problemName == ProblemName . LUPI_REGRESSION : y = scores elif ( problemName == \" ordinalRegression \" or problemName == ProblemName . LUPI_ORDREGRESSION ) : step = 1 / ( n_ordinal_bins ) quantiles = [ i * step for i in range ( 1 , n_ordinal_bins ) ] bs = np . quantile ( scores , quantiles ) bs = np . append ( bs , np . inf ) scores = scores [:, np . newaxis ] y = np . sum ( scores - bs >= 0 , - 1 ) if n_irrel > 0 : X = np . hstack ( [ X , random_state . normal ( size = ( n_samples , n_irrel )) ] ) X_priv = np . hstack ( [ X_priv , random_state . normal ( size = ( n_samples , n_irrel )) ] ) if label_noise > 0 : sample = random_state . choice ( len ( y ) , int ( len ( y ) * label_noise )) y [ sample ] = random_state . permutation ( y [ sample ] ) return ( X , X_priv , y . squeeze ())","title":"Returns"},{"location":"reference/fri/toydata/gen_lupi/#genregressiondata2","text":"def ( n_samples : int = 100 , random_state : object = None , noise : float = 0.0 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , partition = None ) -> object Generate synthetic regression data","title":"genRegressionData2"},{"location":"reference/fri/toydata/gen_lupi/#parameters_1","text":"n_samples : int, optional Number of samples random_state : object, optional Randomstate object used for generation noise : float, optional Noise of the created samples around ground truth n_strel : int, optional Number of features which are mandatory for the underlying model (strongly relevant) n_weakrel : int, optional Number of weakly relevant features n_repeated : int, optional Number of features which are clones of existing ones. n_irrel : int, optional Number of features that are irrelevant partition: list of int Entries of the list define how many weak relevant features are based on the same strongly relevant feature The sum over all list entries must be equal to n_weakrel","title":"Parameters"},{"location":"reference/fri/toydata/gen_lupi/#returns_1","text":"X : array of shape [n_samples, (n_strel + n_weakrel + n_repeated + n_irrel] The generated samples. y : array of shape [n_samples] The output values (target).","title":"Returns"},{"location":"reference/fri/toydata/gen_lupi/#raises","text":"ValueError Wrong parameters for specified amonut of features/samples. View Source def genRegressionData2 ( n_samples : int = 100 , random_state : object = None , noise : float = 0 . 0 , n_strel : int = 1 , n_weakrel : int = 0 , n_repeated : int = 0 , n_irrel : int = 0 , partition = None , ) -> object : \"\"\" Generate synthetic regression data Parameters ---------- n_samples : int , optional Number of samples random_state : object , optional Randomstate object used for generation noise : float , optional Noise of the created samples around ground truth n_strel : int , optional Number of features which are mandatory for the underlying model ( strongly relevant ) n_weakrel : int , optional Number of weakly relevant features n_repeated : int , optional Number of features which are clones of existing ones . n_irrel : int , optional Number of features that are irrelevant partition : list of int Entries of the list define how many weak relevant features are based on the same strongly relevant feature The sum over all list entries must be equal to n_weakrel Returns ------- X : array of shape [ n_samples , ( n_strel + n_weakrel + n_repeated + n_irrel ] The generated samples . y : array of shape [ n_samples ] The output values ( target ) . Raises ------ ValueError Wrong parameters for specified amonut of features / samples . \"\"\" _checkParam2 ( n_samples = n_samples , n_strel = n_strel , n_weakrel = n_weakrel , partition = partition ) random_state = check_random_state ( random_state ) if partition is None : n_informative = n_strel + n_weakrel else : n_informative = n_strel + len ( partition ) X_informative = random_state . randn ( n_samples , n_informative ) ground_truth = np . zeros (( n_informative , 1 )) ground_truth [: n_informative , :] = 0 . 3 bias = 0 y = np . dot ( X_informative , ground_truth ) + bias if noise > 0 . 0 : y += random_state . normal ( scale = noise , size = y . shape ) X_strel = X_informative [:, : n_strel ] X_weakrel = _genWeakFeatures ( n_weakrel , X_informative [:, n_strel :], random_state , partition ) X_repeated = _genRepeatedFeatures ( n_repeated , np . hstack ( [ X_strel , X_weakrel ] ) , random_state ) X_irrel = random_state . normal ( size = ( n_samples , n_irrel )) X = np . hstack ( [ X_strel , X_weakrel , X_repeated , X_irrel ] ) y = np . squeeze ( y ) return X , y","title":"Raises"}]}