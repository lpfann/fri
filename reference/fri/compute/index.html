



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../../docs/favicon.ico">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.4.2">
    
    
      
        <title>Compute - fri</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/application.30686662.css">
      
      
    
    
      <script src="../../../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../../assets/fonts/material-icons.css">
    
    
    
      
    
    
  </head>
  
    <body dir="ltr">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#module-fricompute" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../../.." title="fri" class="md-header-nav__button md-logo">
          
            <img src="../../../docs/logo.png" width="24" height="24">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              fri
            </span>
            <span class="md-header-nav__topic">
              
                Compute
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/lpfann/fri/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    lpfann/fri
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main" role="main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../../.." title="fri" class="md-nav__button md-logo">
      
        <img src="../../../docs/logo.png" width="48" height="48">
      
    </a>
    fri
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/lpfann/fri/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    lpfann/fri
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../../.." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../docs/Guide/" title="Guide" class="md-nav__link">
      Guide
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../docs/background/" title="Background" class="md-nav__link">
      Background
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../docs/citing_FRI/" title="Citing Fri" class="md-nav__link">
      Citing Fri
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5" checked>
    
    <label class="md-nav__link" for="nav-5">
      Reference
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Reference
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5-1" type="checkbox" id="nav-5-1" checked>
    
    <label class="md-nav__link" for="nav-5-1">
      Fri
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-5-1">
        Fri
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Compute
      </label>
    
    <a href="./" title="Compute" class="md-nav__link md-nav__link--active">
      Compute
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#variables" class="md-nav__link">
    Variables
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#functions" class="md-nav__link">
    Functions
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create_probe_statistic" class="md-nav__link">
    create_probe_statistic
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#feature_classification" class="md-nav__link">
    feature_classification
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#relevanceboundsintervals" class="md-nav__link">
    RelevanceBoundsIntervals
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#compute_multi_preset_relevance_bounds" class="md-nav__link">
    compute_multi_preset_relevance_bounds
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#parameters" class="md-nav__link">
    Parameters
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#compute_probe_values" class="md-nav__link">
    compute_probe_values
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_relevance_bounds" class="md-nav__link">
    compute_relevance_bounds
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_single_preset_relevance_bounds" class="md-nav__link">
    compute_single_preset_relevance_bounds
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_normalized_intervals" class="md-nav__link">
    get_normalized_intervals
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_normalized_lupi_intervals" class="md-nav__link">
    get_normalized_lupi_intervals
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../" title="Index" class="md-nav__link">
      Index
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../main/" title="Main" class="md-nav__link">
      Main
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../parameter_searcher/" title="Parameter Searcher" class="md-nav__link">
      Parameter Searcher
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../plot/" title="Plot" class="md-nav__link">
      Plot
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../utils/" title="Utils" class="md-nav__link">
      Utils
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5-1-7" type="checkbox" id="nav-5-1-7">
    
    <label class="md-nav__link" for="nav-5-1-7">
      Model
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-5-1-7">
        Model
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../model/base_cvxproblem/" title="Base Cvxproblem" class="md-nav__link">
      Base Cvxproblem
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../model/base_initmodel/" title="Base Initmodel" class="md-nav__link">
      Base Initmodel
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../model/base_lupi/" title="Base Lupi" class="md-nav__link">
      Base Lupi
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../model/base_type/" title="Base Type" class="md-nav__link">
      Base Type
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../model/classification/" title="Classification" class="md-nav__link">
      Classification
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../model/" title="Index" class="md-nav__link">
      Index
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../model/lupi_classification/" title="Lupi Classification" class="md-nav__link">
      Lupi Classification
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../model/lupi_ordinal_regression/" title="Lupi Ordinal Regression" class="md-nav__link">
      Lupi Ordinal Regression
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../model/lupi_regression/" title="Lupi Regression" class="md-nav__link">
      Lupi Regression
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../model/ordinal_regression/" title="Ordinal Regression" class="md-nav__link">
      Ordinal Regression
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../model/regression/" title="Regression" class="md-nav__link">
      Regression
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5-1-8" type="checkbox" id="nav-5-1-8">
    
    <label class="md-nav__link" for="nav-5-1-8">
      Toydata
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-5-1-8">
        Toydata
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../toydata/gen_data/" title="Gen Data" class="md-nav__link">
      Gen Data
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../toydata/gen_lupi/" title="Gen Lupi" class="md-nav__link">
      Gen Lupi
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../toydata/" title="Index" class="md-nav__link">
      Index
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#variables" class="md-nav__link">
    Variables
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#functions" class="md-nav__link">
    Functions
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create_probe_statistic" class="md-nav__link">
    create_probe_statistic
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#feature_classification" class="md-nav__link">
    feature_classification
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#relevanceboundsintervals" class="md-nav__link">
    RelevanceBoundsIntervals
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#compute_multi_preset_relevance_bounds" class="md-nav__link">
    compute_multi_preset_relevance_bounds
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#parameters" class="md-nav__link">
    Parameters
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#compute_probe_values" class="md-nav__link">
    compute_probe_values
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_relevance_bounds" class="md-nav__link">
    compute_relevance_bounds
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_single_preset_relevance_bounds" class="md-nav__link">
    compute_single_preset_relevance_bounds
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_normalized_intervals" class="md-nav__link">
    get_normalized_intervals
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_normalized_lupi_intervals" class="md-nav__link">
    get_normalized_lupi_intervals
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/lpfann/fri/edit/master/docs/reference/fri/compute.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="module-fricompute">Module fri.compute</h1>
<details class="example"><summary>View Source</summary><pre><code>import logging

from collections import defaultdict

import joblib

import numpy as np

from scipy import stats

from fri.model.base_cvxproblem import Relevance_CVXProblem

from fri.model.base_initmodel import InitModel

from fri.model.base_type import ProblemType

from fri.utils import permutate_feature_in_data

MIN_N_PROBE_FEATURES = 20  # Lower bound of probe features

def _start_solver_worker(bound: Relevance_CVXProblem):

    """

    Worker thread method for parallel computation

    """

    return bound.solve()

class RelevanceBoundsIntervals(object):

    def __init__(

        self,

        data,

        problem_type: ProblemType,

        best_init_model: InitModel,

        random_state,

        n_resampling,

        n_jobs,

        verbose,

        normalize=True,

    ):

        self.data = data

        self.problem_type = problem_type

        self.verbose = verbose

        self.n_jobs = n_jobs

        self.n_resampling = n_resampling

        self.random_state = random_state

        self.best_init_model = best_init_model

        self.best_hyperparameters = best_init_model.hyperparam

        self.normalize = normalize

        # Relax constraints to improve stability

        relaxed_constraints = problem_type.get_relaxed_constraints(

            best_init_model.constraints

        )

        self.init_constraints = relaxed_constraints

    def get_normalized_lupi_intervals(self, lupi_features, presetModel=None):

        # We define a list of all the features we want to compute relevance bounds for

        X, _ = self.data  # TODO: handle other data formats

        all_d = X.shape[1]

        normal_d = all_d - lupi_features

        # Compute relevance bounds and probes for normal features and LUPI

        with joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose) as parallel:

            d_n = _get_necessary_dimensions(normal_d, presetModel)

            rb = self.compute_relevance_bounds(d_n, parallel=parallel)

            probe_upper = self.compute_probe_values(d_n, True, parallel=parallel)

            probe_lower = self.compute_probe_values(d_n, False, parallel=parallel)

            d_l = _get_necessary_dimensions(all_d, presetModel, start=normal_d)

            rb_l = self.compute_relevance_bounds(d_l, parallel=parallel)

            probe_priv_upper = self.compute_probe_values(d_l, True, parallel=parallel)

            probe_priv_lower = self.compute_probe_values(d_l, False, parallel=parallel)

        probes = [probe_lower, probe_upper, probe_priv_lower, probe_priv_upper]

        #

        # Postprocess

        #

        # Get Scaling Parameters

        l1 = self.init_constraints["w_l1"]

        l1_priv = self.init_constraints["w_priv_l1"]

        l1 = l1 + l1_priv

        # Normalize Normal and Lupi features

        rb_norm = self._postprocessing(l1, rb)

        rb_l_norm = self._postprocessing(l1, rb_l)

        interval_ = np.concatenate([rb_norm, rb_l_norm])

        # Normalize Probes

        probe_lower = self._postprocessing(l1, probe_lower)

        probe_upper = self._postprocessing(l1, probe_upper)

        probe_priv_lower = self._postprocessing(l1, probe_priv_lower)

        probe_priv_upper = self._postprocessing(l1, probe_priv_upper)

        #

        #

        # Classify features

        fc = feature_classification(

            probe_lower, probe_upper, rb_norm, verbose=self.verbose

        )

        fc_l = feature_classification(

            probe_priv_lower, probe_priv_upper, rb_l_norm, verbose=self.verbose

        )

        fc_both = np.concatenate([fc, fc_l])

        return interval_, fc_both

    def get_normalized_intervals(self, presetModel=None):

        # We define a list of all the features we want to compute relevance bounds for

        X, _ = self.data  # TODO: handle other data formats

        d = X.shape[1]

        # Depending on the preset model, we dont need to compute all bounds

        # e.g. in the case of fixed features we skip those

        dims = _get_necessary_dimensions(d, presetModel)

        with joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose) as parallel:

            relevance_bounds = self.compute_relevance_bounds(

                dims, parallel=parallel, presetModel=presetModel

            )

            probe_values_upper = self.compute_probe_values(

                dims, isUpper=True, parallel=parallel, presetModel=presetModel

            )

            probe_values_lower = self.compute_probe_values(

                dims, isUpper=False, parallel=parallel, presetModel=presetModel

            )

        # Postprocess bounds

        norm_bounds = self._postprocessing(

            self.best_init_model.L1_factor, relevance_bounds

        )

        norm_probe_values_upper = self._postprocessing(

            self.best_init_model.L1_factor, probe_values_upper

        )

        norm_probe_values_lower = self._postprocessing(

            self.best_init_model.L1_factor, probe_values_lower

        )

        feature_classes = feature_classification(

            norm_probe_values_lower,

            norm_probe_values_upper,

            norm_bounds,

            verbose=self.verbose,

        )

        return norm_bounds, feature_classes

    def compute_relevance_bounds(

        self, dims, parallel=None, presetModel=None, solverargs=None

    ):

        init_model_state = self.best_init_model.model_state

        work_queue = self._generate_relevance_bounds_tasks(

            dims, self.data, presetModel, init_model_state

        )

        # Solve relevance bounds in parallel (when available)

        if parallel is None:

            parallel = joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose)

        bound_results = parallel(map(joblib.delayed(_start_solver_worker), work_queue))

        # Retrieve results and aggregate values in dict

        solved_bounds = defaultdict(list)

        for finished_bound in bound_results:

            # Only add bounds with feasible solutions

            if finished_bound.is_solved:

                solved_bounds[finished_bound.current_feature].append(finished_bound)

        # Initalize array for pair of bounds(= intervals)

        length = len(dims)

        intervals = np.zeros((length, 2))

        for abs_index, rel_index in zip(dims, range(length)):

            # Return interval for feature i (can be a fixed value when set beforehand)

            interval_i = self._create_interval(abs_index, solved_bounds, presetModel)

            intervals[rel_index] = interval_i

        return intervals  # TODO: add model model_state (omega, bias) to return value

    def compute_probe_values(self, dims, isUpper=True, parallel=None, presetModel=None):

        # Get model parameters

        init_model_state = self.best_init_model.model_state

        # Prepare parallel framework

        if parallel is None:

            parallel = joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose)

        # Generate

        probe_queue = self._generate_probe_value_tasks(

            self.data,

            dims,

            isUpper,

            self.n_resampling,

            self.random_state,

            presetModel,

            init_model_state,

        )

        # Compute solution

        probe_results = parallel(map(joblib.delayed(_start_solver_worker), probe_queue))

        # probe_values.extend([probe.objective.value for probe in probe_results if probe.is_solved])

        candidates = defaultdict(list)

        for candidate in probe_results:

            # Only add bounds with feasible solutions

            if candidate.is_solved:

                candidates[candidate.probeID].append(candidate)

        probe_values = []

        for probes_for_ID in candidates.values():

            if isUpper:

                probe_values.append(

                    self.problem_type.get_cvxproblem_template.aggregate_max_candidates(

                        probes_for_ID

                    )

                )

            else:

                probe_values.append(

                    self.problem_type.get_cvxproblem_template.aggregate_min_candidates(

                        probes_for_ID

                    )

                )

        return np.array(probe_values)

    def _generate_relevance_bounds_tasks(

        self, dims, data, preset_model=None, best_model_state=None

    ):

        # Do not compute bounds for fixed features

        if preset_model is not None:

            dims = [di for di in dims if di not in preset_model]

        # Instantiate objects for computation later

        for di in dims:

            # Add Lower Bound problem(s) to work list

            yield from self.problem_type.get_cvxproblem_template.generate_lower_bound_problem(

                self.best_hyperparameters,

                self.init_constraints,

                best_model_state,

                data,

                di,

                preset_model,

            )

            # Add problem(s) for Upper bound

            yield from self.problem_type.get_cvxproblem_template.generate_upper_bound_problem(

                self.best_hyperparameters,

                self.init_constraints,

                best_model_state,

                data,

                di,

                preset_model,

            )

    def _generate_probe_value_tasks(

        self,

        data,

        dims,

        isUpper,

        n_resampling,

        random_state,

        preset_model=None,

        best_model_state=None,

    ):

        if isUpper:

            factory = (

                self.problem_type.get_cvxproblem_template.generate_upper_bound_problem

            )

        else:

            factory = (

                self.problem_type.get_cvxproblem_template.generate_lower_bound_problem

            )

        # Random sample n_resampling shadow features by permuting real features and computing upper bound

        random_choice = random_state.choice(a=dims, size=n_resampling)

        # Instantiate objects

        for i, di in enumerate(random_choice):

            data_perm = permutate_feature_in_data(data, di, random_state)

            # We only use upper bounds as probe features

            yield from factory(

                self.best_hyperparameters,

                self.init_constraints,

                best_model_state,

                data_perm,

                di,

                preset_model,

                probeID=i,

            )

    def _create_interval(

        self, feature: int, solved_bounds: dict, presetModel: dict = None

    ):

        # Return preset values for fixed features

        if presetModel is not None:

            if feature in presetModel:

                return presetModel[feature].squeeze()

        all_bounds = solved_bounds[feature]

        min_problems_candidates = [p for p in all_bounds if p.isLowerBound]

        max_problems_candidates = [p for p in all_bounds if not p.isLowerBound]

        if len(all_bounds) &lt; 2:

            logging.error(

                f"(Some) relevance bounds for feature {feature} were not solved."

            )

            raise Exception("Infeasible bound(s).")

        lower_bound = self.problem_type.get_cvxproblem_template.aggregate_min_candidates(

            min_problems_candidates

        )

        upper_bound = self.problem_type.get_cvxproblem_template.aggregate_max_candidates(

            max_problems_candidates

        )

        return lower_bound, upper_bound

    def compute_single_preset_relevance_bounds(

        self, i: int, signed_preset_i: [float, float]

    ):

        """

        Method to run method once for one restricted feature

        Parameters

        ----------

        i:

            restricted feature

        signed_preset_i:

            restricted range of feature i (set before optimization = preset)

        """

        preset = {i: signed_preset_i}

        rangevector = self.compute_multi_preset_relevance_bounds(preset)

        return rangevector

    def compute_multi_preset_relevance_bounds(self, preset, lupi_features=0):

        """

        Method to run method with preset values

        Parameters

        ----------

        lupi_features

        """

        # The user is working with normalized values while we compute them unscaled

        if self.normalize:

            normalized = {}

            for k, v in preset.items():

                normalized[k] = np.asarray(v) * self.best_init_model.L1_factor

            preset = normalized

        # Add sign to presets

        preset = self._add_sign_to_preset(preset)

        # Calculate all bounds with feature i set to min_i

        if lupi_features &gt; 0:

            rangevector, f_classes = self.get_normalized_lupi_intervals(

                lupi_features, presetModel=preset

            )

        else:

            rangevector, f_classes = self.get_normalized_intervals(presetModel=preset)

        return rangevector

    def _add_sign_to_preset(self, unsigned_presets):

        """

        We need signed presets for our convex problem definition later.

        We reuse the coefficients of the optimal model for this

        Parameters

        ----------

        unsigned_presets : dict

        Returns

        -------

        dict

        """

        signed_presets = {}

        # Obtain optimal model parameters

        w = self.best_init_model.model_state["w"]

        sum = 0

        for i, preset in unsigned_presets.items():

            preset = np.array(preset)

            if preset.size == 1:

                preset = np.repeat(preset, 2)

            unsigned_preset_i = np.sign(w[i]) * preset

            # accumulate maximal feature  contribution

            sum += unsigned_preset_i[1]  # Take upper preset

            signed_presets[i] = unsigned_preset_i

        # Check if unsigned_presets makes sense

        l1 = self.init_constraints["w_l1"]

        if sum &gt; l1:

            print("maximum L1 norm of presets: ", sum)

            print("L1 allowed:", l1)

            print("Presets are not feasible. Try lowering values.")

            return

        return signed_presets

    def _postprocessing(self, L1, rangevector, round_to_zero=True):

        if self.normalize:

            assert L1 &gt; 0

            rangevector = rangevector.copy() / L1

        if round_to_zero:

            rangevector[rangevector &lt;= 1e-11] = 0

        return rangevector

def _get_necessary_dimensions(d: int, presetModel: dict = None, start=0):

    dims = np.arange(start, d)

    # if presetModel is not None:

    #    # Exclude fixed (preset) dimensions from being redundantly computed

    #    dims = [di for di in dims if di not in presetModel.keys()]

    # TODO: check the removal of this block

    return dims

def feature_classification(

    probes_low, probes_up, relevance_bounds, fpr=1e-4, verbose=0

):

    logging.debug("**** Feature Selection ****")

    logging.debug("Generating Lower Probe Statistic")

    lower_stat = create_probe_statistic(probes_low, fpr, verbose=verbose)

    logging.debug("Generating Upper Probe Statistic")

    upper_stat = create_probe_statistic(probes_up, fpr, verbose=verbose)

    weakly = relevance_bounds[:, 1] &gt; upper_stat[1]

    strongly = relevance_bounds[:, 0] &gt; lower_stat[1]

    both = np.logical_and(weakly, strongly)

    prediction = np.zeros(relevance_bounds.shape[0], dtype=np.int)

    prediction[weakly] = 1

    prediction[both] = 2

    return prediction

def create_probe_statistic(probe_values, fpr, verbose=0):

    # Create prediction interval statistics based on randomly permutated probe features (based on real features)

    n = len(probe_values)

    if n == 0:

        if verbose &gt; 0:

            logging.info(

                "All probes were infeasible. All features considered relevant."

            )

        #    # If all probes were infeasible we expect an empty list

        #    # If they are infeasible it also means that only strongly relevant features were in the data

        #    # As such we just set the prediction without considering the statistics

        mean = 0

    else:

        probe_values = np.asarray(probe_values)

        mean = probe_values.mean()

    if mean == 0:

        lower_threshold, upper_threshold = mean, mean

        s = 0

    else:

        s = probe_values.std()

        lower_threshold = mean + stats.t(df=n - 1).ppf(fpr) * s * np.sqrt(1 + (1 / n))

        upper_threshold = mean - stats.t(df=n - 1).ppf(fpr) * s * np.sqrt(1 + (1 / n))

    if verbose &gt; 0:

        print(

            f"FS threshold: {lower_threshold}-{upper_threshold}, Mean:{mean}, Std:{s}, n_probes {n}"

        )

    return lower_threshold, upper_threshold
</code></pre>
</details>
<h2 id="variables">Variables</h2>
<pre><code class="python3">MIN_N_PROBE_FEATURES
</code></pre>

<h2 id="functions">Functions</h2>
<h3 id="create_probe_statistic">create_probe_statistic</h3>
<pre><code class="python3">def create_probe_statistic(
    probe_values,
    fpr,
    verbose=0
)
</code></pre>

<details class="example"><summary>View Source</summary><pre><code>def create_probe_statistic(probe_values, fpr, verbose=0):

    # Create prediction interval statistics based on randomly permutated probe features (based on real features)

    n = len(probe_values)

    if n == 0:

        if verbose &gt; 0:

            logging.info(

                "All probes were infeasible. All features considered relevant."

            )

        #    # If all probes were infeasible we expect an empty list

        #    # If they are infeasible it also means that only strongly relevant features were in the data

        #    # As such we just set the prediction without considering the statistics

        mean = 0

    else:

        probe_values = np.asarray(probe_values)

        mean = probe_values.mean()

    if mean == 0:

        lower_threshold, upper_threshold = mean, mean

        s = 0

    else:

        s = probe_values.std()

        lower_threshold = mean + stats.t(df=n - 1).ppf(fpr) * s * np.sqrt(1 + (1 / n))

        upper_threshold = mean - stats.t(df=n - 1).ppf(fpr) * s * np.sqrt(1 + (1 / n))

    if verbose &gt; 0:

        print(

            f"FS threshold: {lower_threshold}-{upper_threshold}, Mean:{mean}, Std:{s}, n_probes {n}"

        )

    return lower_threshold, upper_threshold
</code></pre>
</details>
<h3 id="feature_classification">feature_classification</h3>
<pre><code class="python3">def feature_classification(
    probes_low,
    probes_up,
    relevance_bounds,
    fpr=0.0001,
    verbose=0
)
</code></pre>

<details class="example"><summary>View Source</summary><pre><code>def feature_classification(

    probes_low, probes_up, relevance_bounds, fpr=1e-4, verbose=0

):

    logging.debug("**** Feature Selection ****")

    logging.debug("Generating Lower Probe Statistic")

    lower_stat = create_probe_statistic(probes_low, fpr, verbose=verbose)

    logging.debug("Generating Upper Probe Statistic")

    upper_stat = create_probe_statistic(probes_up, fpr, verbose=verbose)

    weakly = relevance_bounds[:, 1] &gt; upper_stat[1]

    strongly = relevance_bounds[:, 0] &gt; lower_stat[1]

    both = np.logical_and(weakly, strongly)

    prediction = np.zeros(relevance_bounds.shape[0], dtype=np.int)

    prediction[weakly] = 1

    prediction[both] = 2

    return prediction
</code></pre>
</details>
<h2 id="classes">Classes</h2>
<h3 id="relevanceboundsintervals">RelevanceBoundsIntervals</h3>
<pre><code class="python3">class RelevanceBoundsIntervals(
    data,
    problem_type: fri.model.base_type.ProblemType,
    best_init_model: fri.model.base_initmodel.InitModel,
    random_state,
    n_resampling,
    n_jobs,
    verbose,
    normalize=True
)
</code></pre>

<details class="example"><summary>View Source</summary><pre><code>class RelevanceBoundsIntervals(object):

    def __init__(

        self,

        data,

        problem_type: ProblemType,

        best_init_model: InitModel,

        random_state,

        n_resampling,

        n_jobs,

        verbose,

        normalize=True,

    ):

        self.data = data

        self.problem_type = problem_type

        self.verbose = verbose

        self.n_jobs = n_jobs

        self.n_resampling = n_resampling

        self.random_state = random_state

        self.best_init_model = best_init_model

        self.best_hyperparameters = best_init_model.hyperparam

        self.normalize = normalize

        # Relax constraints to improve stability

        relaxed_constraints = problem_type.get_relaxed_constraints(

            best_init_model.constraints

        )

        self.init_constraints = relaxed_constraints

    def get_normalized_lupi_intervals(self, lupi_features, presetModel=None):

        # We define a list of all the features we want to compute relevance bounds for

        X, _ = self.data  # TODO: handle other data formats

        all_d = X.shape[1]

        normal_d = all_d - lupi_features

        # Compute relevance bounds and probes for normal features and LUPI

        with joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose) as parallel:

            d_n = _get_necessary_dimensions(normal_d, presetModel)

            rb = self.compute_relevance_bounds(d_n, parallel=parallel)

            probe_upper = self.compute_probe_values(d_n, True, parallel=parallel)

            probe_lower = self.compute_probe_values(d_n, False, parallel=parallel)

            d_l = _get_necessary_dimensions(all_d, presetModel, start=normal_d)

            rb_l = self.compute_relevance_bounds(d_l, parallel=parallel)

            probe_priv_upper = self.compute_probe_values(d_l, True, parallel=parallel)

            probe_priv_lower = self.compute_probe_values(d_l, False, parallel=parallel)

        probes = [probe_lower, probe_upper, probe_priv_lower, probe_priv_upper]

        #

        # Postprocess

        #

        # Get Scaling Parameters

        l1 = self.init_constraints["w_l1"]

        l1_priv = self.init_constraints["w_priv_l1"]

        l1 = l1 + l1_priv

        # Normalize Normal and Lupi features

        rb_norm = self._postprocessing(l1, rb)

        rb_l_norm = self._postprocessing(l1, rb_l)

        interval_ = np.concatenate([rb_norm, rb_l_norm])

        # Normalize Probes

        probe_lower = self._postprocessing(l1, probe_lower)

        probe_upper = self._postprocessing(l1, probe_upper)

        probe_priv_lower = self._postprocessing(l1, probe_priv_lower)

        probe_priv_upper = self._postprocessing(l1, probe_priv_upper)

        #

        #

        # Classify features

        fc = feature_classification(

            probe_lower, probe_upper, rb_norm, verbose=self.verbose

        )

        fc_l = feature_classification(

            probe_priv_lower, probe_priv_upper, rb_l_norm, verbose=self.verbose

        )

        fc_both = np.concatenate([fc, fc_l])

        return interval_, fc_both

    def get_normalized_intervals(self, presetModel=None):

        # We define a list of all the features we want to compute relevance bounds for

        X, _ = self.data  # TODO: handle other data formats

        d = X.shape[1]

        # Depending on the preset model, we dont need to compute all bounds

        # e.g. in the case of fixed features we skip those

        dims = _get_necessary_dimensions(d, presetModel)

        with joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose) as parallel:

            relevance_bounds = self.compute_relevance_bounds(

                dims, parallel=parallel, presetModel=presetModel

            )

            probe_values_upper = self.compute_probe_values(

                dims, isUpper=True, parallel=parallel, presetModel=presetModel

            )

            probe_values_lower = self.compute_probe_values(

                dims, isUpper=False, parallel=parallel, presetModel=presetModel

            )

        # Postprocess bounds

        norm_bounds = self._postprocessing(

            self.best_init_model.L1_factor, relevance_bounds

        )

        norm_probe_values_upper = self._postprocessing(

            self.best_init_model.L1_factor, probe_values_upper

        )

        norm_probe_values_lower = self._postprocessing(

            self.best_init_model.L1_factor, probe_values_lower

        )

        feature_classes = feature_classification(

            norm_probe_values_lower,

            norm_probe_values_upper,

            norm_bounds,

            verbose=self.verbose,

        )

        return norm_bounds, feature_classes

    def compute_relevance_bounds(

        self, dims, parallel=None, presetModel=None, solverargs=None

    ):

        init_model_state = self.best_init_model.model_state

        work_queue = self._generate_relevance_bounds_tasks(

            dims, self.data, presetModel, init_model_state

        )

        # Solve relevance bounds in parallel (when available)

        if parallel is None:

            parallel = joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose)

        bound_results = parallel(map(joblib.delayed(_start_solver_worker), work_queue))

        # Retrieve results and aggregate values in dict

        solved_bounds = defaultdict(list)

        for finished_bound in bound_results:

            # Only add bounds with feasible solutions

            if finished_bound.is_solved:

                solved_bounds[finished_bound.current_feature].append(finished_bound)

        # Initalize array for pair of bounds(= intervals)

        length = len(dims)

        intervals = np.zeros((length, 2))

        for abs_index, rel_index in zip(dims, range(length)):

            # Return interval for feature i (can be a fixed value when set beforehand)

            interval_i = self._create_interval(abs_index, solved_bounds, presetModel)

            intervals[rel_index] = interval_i

        return intervals  # TODO: add model model_state (omega, bias) to return value

    def compute_probe_values(self, dims, isUpper=True, parallel=None, presetModel=None):

        # Get model parameters

        init_model_state = self.best_init_model.model_state

        # Prepare parallel framework

        if parallel is None:

            parallel = joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose)

        # Generate

        probe_queue = self._generate_probe_value_tasks(

            self.data,

            dims,

            isUpper,

            self.n_resampling,

            self.random_state,

            presetModel,

            init_model_state,

        )

        # Compute solution

        probe_results = parallel(map(joblib.delayed(_start_solver_worker), probe_queue))

        # probe_values.extend([probe.objective.value for probe in probe_results if probe.is_solved])

        candidates = defaultdict(list)

        for candidate in probe_results:

            # Only add bounds with feasible solutions

            if candidate.is_solved:

                candidates[candidate.probeID].append(candidate)

        probe_values = []

        for probes_for_ID in candidates.values():

            if isUpper:

                probe_values.append(

                    self.problem_type.get_cvxproblem_template.aggregate_max_candidates(

                        probes_for_ID

                    )

                )

            else:

                probe_values.append(

                    self.problem_type.get_cvxproblem_template.aggregate_min_candidates(

                        probes_for_ID

                    )

                )

        return np.array(probe_values)

    def _generate_relevance_bounds_tasks(

        self, dims, data, preset_model=None, best_model_state=None

    ):

        # Do not compute bounds for fixed features

        if preset_model is not None:

            dims = [di for di in dims if di not in preset_model]

        # Instantiate objects for computation later

        for di in dims:

            # Add Lower Bound problem(s) to work list

            yield from self.problem_type.get_cvxproblem_template.generate_lower_bound_problem(

                self.best_hyperparameters,

                self.init_constraints,

                best_model_state,

                data,

                di,

                preset_model,

            )

            # Add problem(s) for Upper bound

            yield from self.problem_type.get_cvxproblem_template.generate_upper_bound_problem(

                self.best_hyperparameters,

                self.init_constraints,

                best_model_state,

                data,

                di,

                preset_model,

            )

    def _generate_probe_value_tasks(

        self,

        data,

        dims,

        isUpper,

        n_resampling,

        random_state,

        preset_model=None,

        best_model_state=None,

    ):

        if isUpper:

            factory = (

                self.problem_type.get_cvxproblem_template.generate_upper_bound_problem

            )

        else:

            factory = (

                self.problem_type.get_cvxproblem_template.generate_lower_bound_problem

            )

        # Random sample n_resampling shadow features by permuting real features and computing upper bound

        random_choice = random_state.choice(a=dims, size=n_resampling)

        # Instantiate objects

        for i, di in enumerate(random_choice):

            data_perm = permutate_feature_in_data(data, di, random_state)

            # We only use upper bounds as probe features

            yield from factory(

                self.best_hyperparameters,

                self.init_constraints,

                best_model_state,

                data_perm,

                di,

                preset_model,

                probeID=i,

            )

    def _create_interval(

        self, feature: int, solved_bounds: dict, presetModel: dict = None

    ):

        # Return preset values for fixed features

        if presetModel is not None:

            if feature in presetModel:

                return presetModel[feature].squeeze()

        all_bounds = solved_bounds[feature]

        min_problems_candidates = [p for p in all_bounds if p.isLowerBound]

        max_problems_candidates = [p for p in all_bounds if not p.isLowerBound]

        if len(all_bounds) &lt; 2:

            logging.error(

                f"(Some) relevance bounds for feature {feature} were not solved."

            )

            raise Exception("Infeasible bound(s).")

        lower_bound = self.problem_type.get_cvxproblem_template.aggregate_min_candidates(

            min_problems_candidates

        )

        upper_bound = self.problem_type.get_cvxproblem_template.aggregate_max_candidates(

            max_problems_candidates

        )

        return lower_bound, upper_bound

    def compute_single_preset_relevance_bounds(

        self, i: int, signed_preset_i: [float, float]

    ):

        """

        Method to run method once for one restricted feature

        Parameters

        ----------

        i:

            restricted feature

        signed_preset_i:

            restricted range of feature i (set before optimization = preset)

        """

        preset = {i: signed_preset_i}

        rangevector = self.compute_multi_preset_relevance_bounds(preset)

        return rangevector

    def compute_multi_preset_relevance_bounds(self, preset, lupi_features=0):

        """

        Method to run method with preset values

        Parameters

        ----------

        lupi_features

        """

        # The user is working with normalized values while we compute them unscaled

        if self.normalize:

            normalized = {}

            for k, v in preset.items():

                normalized[k] = np.asarray(v) * self.best_init_model.L1_factor

            preset = normalized

        # Add sign to presets

        preset = self._add_sign_to_preset(preset)

        # Calculate all bounds with feature i set to min_i

        if lupi_features &gt; 0:

            rangevector, f_classes = self.get_normalized_lupi_intervals(

                lupi_features, presetModel=preset

            )

        else:

            rangevector, f_classes = self.get_normalized_intervals(presetModel=preset)

        return rangevector

    def _add_sign_to_preset(self, unsigned_presets):

        """

        We need signed presets for our convex problem definition later.

        We reuse the coefficients of the optimal model for this

        Parameters

        ----------

        unsigned_presets : dict

        Returns

        -------

        dict

        """

        signed_presets = {}

        # Obtain optimal model parameters

        w = self.best_init_model.model_state["w"]

        sum = 0

        for i, preset in unsigned_presets.items():

            preset = np.array(preset)

            if preset.size == 1:

                preset = np.repeat(preset, 2)

            unsigned_preset_i = np.sign(w[i]) * preset

            # accumulate maximal feature  contribution

            sum += unsigned_preset_i[1]  # Take upper preset

            signed_presets[i] = unsigned_preset_i

        # Check if unsigned_presets makes sense

        l1 = self.init_constraints["w_l1"]

        if sum &gt; l1:

            print("maximum L1 norm of presets: ", sum)

            print("L1 allowed:", l1)

            print("Presets are not feasible. Try lowering values.")

            return

        return signed_presets

    def _postprocessing(self, L1, rangevector, round_to_zero=True):

        if self.normalize:

            assert L1 &gt; 0

            rangevector = rangevector.copy() / L1

        if round_to_zero:

            rangevector[rangevector &lt;= 1e-11] = 0

        return rangevector
</code></pre>
</details>
<hr />
<h4 id="methods">Methods</h4>
<h5 id="compute_multi_preset_relevance_bounds">compute_multi_preset_relevance_bounds</h5>
<pre><code class="python3">def compute_multi_preset_relevance_bounds(
    self,
    preset,
    lupi_features=0
)
</code></pre>

<p>Method to run method with preset values</p>
<h2 id="parameters">Parameters</h2>
<p>lupi_features</p>
<details class="example"><summary>View Source</summary><pre><code>    def compute_multi_preset_relevance_bounds(self, preset, lupi_features=0):

        """

        Method to run method with preset values

        Parameters

        ----------

        lupi_features

        """

        # The user is working with normalized values while we compute them unscaled

        if self.normalize:

            normalized = {}

            for k, v in preset.items():

                normalized[k] = np.asarray(v) * self.best_init_model.L1_factor

            preset = normalized

        # Add sign to presets

        preset = self._add_sign_to_preset(preset)

        # Calculate all bounds with feature i set to min_i

        if lupi_features &gt; 0:

            rangevector, f_classes = self.get_normalized_lupi_intervals(

                lupi_features, presetModel=preset

            )

        else:

            rangevector, f_classes = self.get_normalized_intervals(presetModel=preset)

        return rangevector
</code></pre>
</details>
<h5 id="compute_probe_values">compute_probe_values</h5>
<pre><code class="python3">def compute_probe_values(
    self,
    dims,
    isUpper=True,
    parallel=None,
    presetModel=None
)
</code></pre>

<details class="example"><summary>View Source</summary><pre><code>    def compute_probe_values(self, dims, isUpper=True, parallel=None, presetModel=None):

        # Get model parameters

        init_model_state = self.best_init_model.model_state

        # Prepare parallel framework

        if parallel is None:

            parallel = joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose)

        # Generate

        probe_queue = self._generate_probe_value_tasks(

            self.data,

            dims,

            isUpper,

            self.n_resampling,

            self.random_state,

            presetModel,

            init_model_state,

        )

        # Compute solution

        probe_results = parallel(map(joblib.delayed(_start_solver_worker), probe_queue))

        # probe_values.extend([probe.objective.value for probe in probe_results if probe.is_solved])

        candidates = defaultdict(list)

        for candidate in probe_results:

            # Only add bounds with feasible solutions

            if candidate.is_solved:

                candidates[candidate.probeID].append(candidate)

        probe_values = []

        for probes_for_ID in candidates.values():

            if isUpper:

                probe_values.append(

                    self.problem_type.get_cvxproblem_template.aggregate_max_candidates(

                        probes_for_ID

                    )

                )

            else:

                probe_values.append(

                    self.problem_type.get_cvxproblem_template.aggregate_min_candidates(

                        probes_for_ID

                    )

                )

        return np.array(probe_values)
</code></pre>
</details>
<h5 id="compute_relevance_bounds">compute_relevance_bounds</h5>
<pre><code class="python3">def compute_relevance_bounds(
    self,
    dims,
    parallel=None,
    presetModel=None,
    solverargs=None
)
</code></pre>

<details class="example"><summary>View Source</summary><pre><code>    def compute_relevance_bounds(

        self, dims, parallel=None, presetModel=None, solverargs=None

    ):

        init_model_state = self.best_init_model.model_state

        work_queue = self._generate_relevance_bounds_tasks(

            dims, self.data, presetModel, init_model_state

        )

        # Solve relevance bounds in parallel (when available)

        if parallel is None:

            parallel = joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose)

        bound_results = parallel(map(joblib.delayed(_start_solver_worker), work_queue))

        # Retrieve results and aggregate values in dict

        solved_bounds = defaultdict(list)

        for finished_bound in bound_results:

            # Only add bounds with feasible solutions

            if finished_bound.is_solved:

                solved_bounds[finished_bound.current_feature].append(finished_bound)

        # Initalize array for pair of bounds(= intervals)

        length = len(dims)

        intervals = np.zeros((length, 2))

        for abs_index, rel_index in zip(dims, range(length)):

            # Return interval for feature i (can be a fixed value when set beforehand)

            interval_i = self._create_interval(abs_index, solved_bounds, presetModel)

            intervals[rel_index] = interval_i

        return intervals  # TODO: add model model_state (omega, bias) to return value
</code></pre>
</details>
<h5 id="compute_single_preset_relevance_bounds">compute_single_preset_relevance_bounds</h5>
<pre><code class="python3">def compute_single_preset_relevance_bounds(
    self,
    i: int,
    signed_preset_i: [&lt;class 'float'&gt;, &lt;class 'float'&gt;]
)
</code></pre>

<p>Method to run method once for one restricted feature
Parameters</p>
<hr />
<p>i:
    restricted feature
signed_preset_i:
    restricted range of feature i (set before optimization = preset)</p>
<details class="example"><summary>View Source</summary><pre><code>    def compute_single_preset_relevance_bounds(

        self, i: int, signed_preset_i: [float, float]

    ):

        """

        Method to run method once for one restricted feature

        Parameters

        ----------

        i:

            restricted feature

        signed_preset_i:

            restricted range of feature i (set before optimization = preset)

        """

        preset = {i: signed_preset_i}

        rangevector = self.compute_multi_preset_relevance_bounds(preset)

        return rangevector
</code></pre>
</details>
<h5 id="get_normalized_intervals">get_normalized_intervals</h5>
<pre><code class="python3">def get_normalized_intervals(
    self,
    presetModel=None
)
</code></pre>

<details class="example"><summary>View Source</summary><pre><code>    def get_normalized_intervals(self, presetModel=None):

        # We define a list of all the features we want to compute relevance bounds for

        X, _ = self.data  # TODO: handle other data formats

        d = X.shape[1]

        # Depending on the preset model, we dont need to compute all bounds

        # e.g. in the case of fixed features we skip those

        dims = _get_necessary_dimensions(d, presetModel)

        with joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose) as parallel:

            relevance_bounds = self.compute_relevance_bounds(

                dims, parallel=parallel, presetModel=presetModel

            )

            probe_values_upper = self.compute_probe_values(

                dims, isUpper=True, parallel=parallel, presetModel=presetModel

            )

            probe_values_lower = self.compute_probe_values(

                dims, isUpper=False, parallel=parallel, presetModel=presetModel

            )

        # Postprocess bounds

        norm_bounds = self._postprocessing(

            self.best_init_model.L1_factor, relevance_bounds

        )

        norm_probe_values_upper = self._postprocessing(

            self.best_init_model.L1_factor, probe_values_upper

        )

        norm_probe_values_lower = self._postprocessing(

            self.best_init_model.L1_factor, probe_values_lower

        )

        feature_classes = feature_classification(

            norm_probe_values_lower,

            norm_probe_values_upper,

            norm_bounds,

            verbose=self.verbose,

        )

        return norm_bounds, feature_classes
</code></pre>
</details>
<h5 id="get_normalized_lupi_intervals">get_normalized_lupi_intervals</h5>
<pre><code class="python3">def get_normalized_lupi_intervals(
    self,
    lupi_features,
    presetModel=None
)
</code></pre>

<details class="example"><summary>View Source</summary><pre><code>    def get_normalized_lupi_intervals(self, lupi_features, presetModel=None):

        # We define a list of all the features we want to compute relevance bounds for

        X, _ = self.data  # TODO: handle other data formats

        all_d = X.shape[1]

        normal_d = all_d - lupi_features

        # Compute relevance bounds and probes for normal features and LUPI

        with joblib.Parallel(n_jobs=self.n_jobs, verbose=self.verbose) as parallel:

            d_n = _get_necessary_dimensions(normal_d, presetModel)

            rb = self.compute_relevance_bounds(d_n, parallel=parallel)

            probe_upper = self.compute_probe_values(d_n, True, parallel=parallel)

            probe_lower = self.compute_probe_values(d_n, False, parallel=parallel)

            d_l = _get_necessary_dimensions(all_d, presetModel, start=normal_d)

            rb_l = self.compute_relevance_bounds(d_l, parallel=parallel)

            probe_priv_upper = self.compute_probe_values(d_l, True, parallel=parallel)

            probe_priv_lower = self.compute_probe_values(d_l, False, parallel=parallel)

        probes = [probe_lower, probe_upper, probe_priv_lower, probe_priv_upper]

        #

        # Postprocess

        #

        # Get Scaling Parameters

        l1 = self.init_constraints["w_l1"]

        l1_priv = self.init_constraints["w_priv_l1"]

        l1 = l1 + l1_priv

        # Normalize Normal and Lupi features

        rb_norm = self._postprocessing(l1, rb)

        rb_l_norm = self._postprocessing(l1, rb_l)

        interval_ = np.concatenate([rb_norm, rb_l_norm])

        # Normalize Probes

        probe_lower = self._postprocessing(l1, probe_lower)

        probe_upper = self._postprocessing(l1, probe_upper)

        probe_priv_lower = self._postprocessing(l1, probe_priv_lower)

        probe_priv_upper = self._postprocessing(l1, probe_priv_upper)

        #

        #

        # Classify features

        fc = feature_classification(

            probe_lower, probe_upper, rb_norm, verbose=self.verbose

        )

        fc_l = feature_classification(

            probe_priv_lower, probe_priv_upper, rb_l_norm, verbose=self.verbose

        )

        fc_both = np.concatenate([fc, fc_l])

        return interval_, fc_both
</code></pre>
</details>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../../../docs/citing_FRI/" title="Citing Fri" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Citing Fri
              </span>
            </div>
          </a>
        
        
          <a href="../" title="Index" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Index
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Powered by
        <a href="http://timothycrosley.github.io/portray">portray.</a>
        You too can
        <a href="http://timothycrosley.github.io/portray">
          portray</a>
        your Python project well using automatic documentation.
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../assets/javascripts/application.c648116f.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"../../.."}})</script>
      
    
  </body>
</html>